
<!DOCTYPE html>
<html>
<head>
	<title>Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="http://hcie.csail.mit.edu/images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="https://hcie.csail.mit.edu/css/bootstrap.css" rel="stylesheet">
	<link href="https://hcie.csail.mit.edu/css/custom-style.css" rel="stylesheet">

	<!-- Lightbox -->
	<link href="../../css/lightbox.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="https://hcie.csail.mit.edu/js/analytics.js"></script>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>
<div class="container" style="padding-top: 100px;">
	<div class="row">
	<!-- Publication details -->
	<div class="col-md-4" style="text-align: left;">
		</br>
		</br>
		<span class="medium-headline">
		Publication
		</span>
		</br>
		</br>
		 Faraz Faruqi, Ahmed Katary, Tarik Hasic, Amira Abdel-Rahman, Nayeemur Rahman, Leandra Tejedor, Mackenzie Leake, Megan Hofmann, Stefanie Mueller

		</br>
		Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI.
		</br>
		Conditionally accepted to 
		<a href="https://uist.acm.org/2023/" target="_blank">ACM UIST '23.</a><br>
			</br>
			
				<!-- <a href="https://dl.acm.org/doi/10.1145/3491102.3501951" class="btn btn-doi" alt="doi" target="_blank">DOI</a>
				&nbsp; &nbsp;
				
				<a href="https://arxiv.org/abs/2202.06165" class="btn btn-doi" alt="doi" target="_blank">arXiv</a> -->
				<!-- &nbsp; &nbsp; -->

				
				<a href="https://groups.csail.mit.edu/hcie/files/research-projects/style2fab/style2fab.pdf" class="btn btn-pdf" alt="pdf" target="_blank">PDF</a>
				&nbsp; &nbsp;
		
				
				<a href="https://www.youtube.com/watch?v=wGp1vMNsM3Q" class="btn btn-vdo" alt="video" target="_blank">Video</a>
				&nbsp; &nbsp;
			 <!--
				<a href="https://groups.csail.mit.edu/hcie/files/research-projects/infraredtags/2022-CHI-InfraredTags-Presentation.key.zip" class="btn btn-talk" alt="slide" target="_blank">Slides</a>
				&nbsp; &nbsp;

				<a href="https://github.com/dogadogan/InfraredTags" class="btn btn-doi" alt="doi" target="_blank">Code</a> 
				&nbsp; &nbsp; -->


				<a href="" class="btn btn-doi" alt="doi" target="_blank">Code (will be released at UIST'23)</a> 
				&nbsp; &nbsp;


			</br>
			</br>




			<!-- <span class="medium-headline">
			Videos
			</span> -->


			</br>
			</br>
	 <!--  <iframe width="360" height="203" src="https://www.youtube.com/embed/Tjs1zH7IMxE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br>
			<br>
	  <iframe width="360" height="203" src="https://www.youtube.com/embed/-8NWIsyY-2E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
			</br>
			</br>

	<!-- <span class="medium-headline">
			UIST Talk
	  </span>
			</br>
			</br>
	  <iframe width="360" height="203" src="https://www.youtube.com/embed/61n4RrsKoBo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
			<br>
			<br>
			</br> 





 		



		</div>



	<!-- Project information -->
	<div class="col-md-8" style="text-align: left;">
		<br>
	  <h3 class="headline">
		Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI
		</h3><br>
  <div title="Page 2">
    <div>
      <div>
        <p>
        	<img src="images/Figure-1-full.png" width="750" alt=""/></p>
		      <p style="font-size: 12px"><strong>Figure 1</strong>. To stylize a 3D model with Generative AI without affecting its functionality a user: (a) selects a model to stylize, (b) segments the model, (c) automatically classifies the aesthetic and functional segments, (d) selectively styles only the aesthetic segments, and 
						<br>
						(e) fabricates their stylized model.</p>
		      <br>
					<br>

					<span class="medium-headline">
						ABSTRACT
						</span>
					<br>
					</p>
				<div title="">
					<div>
						<div>
							
					<p>With recent advances in Generative AI, it is becoming easier to automatically manipulate 3D models. However, current methods tend to apply edits to models globally, which risks compromising the intended functionality of the 3D model when fabricated in the physical world. For example, modifying functional segments in 3D models, such as the base of a vase, could break the original functionality of the model, thus causing the vase to fall over. We introduce a method for automatically segmenting 3D models into functional and aesthetic elements. This method allows users to selectively modify aesthetic segments of 3D models, without affecting the functional segments. To develop this method we first create a taxonomy of functionality in 3D models by qualitatively analyzing 1000 models sourced from a popular 3D printing repository, Thingiverse. With this taxonomy, we develop a semi-automatic classification method to decompose 3D models into functional and aesthetic elements. We propose a system called Style2Fab that allows users to selectively stylize 3D models without compromising their functionality. We evaluate the effectiveness of our classification method compared to human-annotated data, and demonstrate the utility of Style2Fab with a user study to show that functionalityaware segmentation helps preserve model functionality.</p>

							</div>
						</div>
				</div>
					<p>

					<span class="medium-headline">
						INTRODUCTION
						</span>
					<br>
					<br>
					</p>
				<div title="">
					<div>
						<div>
							<p>A key challenge for many makers is modifying or â€œstylizingâ€ open source designs shared in online repositories (e.g., Thingiverse). While these platforms provide numerous ready-to-print 3D models, customization is limited to changing predefined parameters. While recent advances in deep-learning methods enable aesthetic modifications in 3D models with styles, customizing existing models with these styles presents new challenges. Beyond aesthetics, 3D printed models often have designed functionality that is directly related to geometry. Manipulating an entire 3D model, which can change the whole geometry, may break this functionality. Styles can be selectively applied, but this requires the maker to identify which pieces of a 3D model affect the functionality and which are purely aesthetic â€” a daunting task for users remixing unfamiliar designs. In some cases, users can label functionality in CAD tools, however, most of the models shared in online repositories are 3D models that have lost this key meta-data.</p>

							<p>To help makers make use of emerging AI-based 3D manipulation tools, we present a method that automatically decomposes 3D meshes designed for 3D printing into components based on their functional and aesthetic parts. This method allows makers to selectively stylize 3D models while maintaining the desired original functionality. Derived from a formative study of 1000 designs on the Thingiverse repository, we contribute a taxonomy for classifying geometric components of a 3D mesh as (1) aesthetic, contributing only to model aesthetics; (2) internally-functional, related to assembly of component-based models; or (3) externally-functional, related to an interaction with the environment. Based on this taxonomy, we contribute a topology-based method that can automatically segment 3D meshes, and classify the functionality of those segments into these three categories. To demonstrate this method, we present an interactive tool, â€œStyle2Fabâ€, that enables makers to manipulate 3D meshes without modifying their functionality. Style2Fab uses differentiable rendering for stylization as proposed in Text2Mesh. Our work demonstrates how we extend these methods to enable complex manipulation of open-source 3D meshes for 3D printing without modifying their original functionality.</p>
							
							<p>Consider a leading scenario where an inexperienced maker, Alex, wants to stylize the outside of a 3D printable self-watering planter (Figure 1). Conceptually, Alex understands that the base needs to stay flat and the interlocking segments of the two components of the model should remain unchanged. But she does not know how to isolate these regions in the two 3D meshes. She processes the models in Style2Fab, where our functionality-aware segmentation method segments the two models, and labels the base and interlocking segments as functional. Our method has done the work of tediously editing the model for her, allowing her to verify that the modelâ€™s functionality was preserved on the segment level. She applies her style only to the outer edge of the planters and sends it off to the 3D printer. She uses the final design to grow herbs on her desk.</p>

							<p>In the remaining sections, we present a formative study on 3D model functionality in the context of 3D printed designs shared online. We then present our method for automatically segmenting and classifying the functional components of 3D models. Next, we present the Style2Fab system which uses this segmentation method to help makers stylize 3D printable designs. We use Style2Fab to evaluate if our classification and segmentation method can help makers modify existing designs without breaking their functionality.</p>
							</div>
						</div>
				</div>
					<p>

						<span class="medium-headline">
							FORMATIVE STUDY
							</span>
						<br>
						</p>
					<div title="">
						<div>
							<div>
								
						<p>One of the key challenges in modifying 3D printable models is ensuring that they remain functional. This requires a maker to carefully identify which parts of a design contribute to the functionality and which parts contribute only to the modelâ€™s aesthetics. The aim of this formative study is to identify functionality descriptors in a wide variety of 3D models. To do this we qualitatively coded 1000 designs sourced from Thingiverse using a similar approach to Hofmann et al. and Chen et al. From these codes, we developed a taxonomy of 3D model functionality.</p>

						<img src="images/figure-2.png" alt="Categories of 3D models based on functionality" width="700" style="padding-bottom:0px; margin-bottom:10px"/><br>
						<b>Figure 2: Categories of 3D models based on functionality: (a) We identify four categories of models defined by two dimensions: Artifact vs Task-Related and Single vs Multi-Component models. These dimensions align with differences between external and internal contexts, (b) shows an example of external and internal functionality on segments of a vase and a self-watering.
							planter.</b>

						<p>
							<span class="small-headline">
								<strong>Data Collection</strong>
								</span>
							
							<br>
 							<p>Thingiverse is a popular online resource for novice and expert makers to share 3D printing designs or things1. While some models are shared in editable formats, most are shared as difficult-to-modify 3D meshes in OBJ and STL file formats (OBJ/STL). We collected and analyzed the 1000 most popular things on Thingiverse as of January 23rd, 2023. Although large-scale datasets exist for segmentation tasks (COSEG and PartNet), we found that sampling models from Thingiverse provides a wider variety of 3D models suited for fabrication since existing data sets are not intended for 3D printing.</p>
							<p>We organized and standardized all 3D models included in these 1000 things. We first excluded any 3D models that were not in an STL, OBJ, or SCAD format, limiting our data to 3D meshes. We excluded all corrupted 3D models, and the ones shared without the three given formats. We converted all remaining models to the OBJ format. Next, we manually excluded duplicate meshes of varied sizes since this would not contribute to our classification of functional components. For highly-similar models shared as a collection (e.g., â€˜Fantasy Mini Collectionâ€™, Thing ID: 3054701), we kept only a single mesh. After this preprocessing, we had a total of 993 different Thingiverse things, comprising 10,945 unique 3D models (i.e., objects or parts of objects).</p>

							<p>
								<span class="small-headline">
									<strong>Inductive Taxonomy Development</strong>
									</span>
								
								<br>
								 <p>We used an iterative qualitative coding method to develop our tax- onomy of functionality. We first inductively coded 100 randomly selected models to develop an understanding of the functionality of these 3D-printed models. After negotiation across coders, we iden- tified two distinct categories of 3D models based on their function- ality: Artifacts and Task-Related Models. Artifacts are objects that serve primarily aesthetic purposes, such as statues. Task-Related models have been designed to help perform a specific task, such as a phone stand or battery dispenser. Both Artifacts and Task- Related models can be composed of single or multiple components assembled together (Figure 2).</p>
								<p>From this classification of types of models, we determined two axes that we can use to classify if a segment of a 3D mesh can be modified without changing the intended functionality of the design. The first axis is external context, which describes how the surface of the segment interfaces with the real world to affect the function- ality of the model (e.g., the flat base of a planter interfaces with a table surface). Most Artifacts have few segments with external contexts, while Task-Related models have many. The second axis is internal context; a segment has high internal context if it interfaces with other segments within the same thing to affect the designâ€™s functionality (e.g., linkages in an articulated lizard). Segments that do not have internal or external context are considered aesthetic since they do not affect functionality. Segments without internal and external context can readily be modified since they only serve an aesthetic purpose.
								</p>

								<p>
									<span class="small-headline">
										<strong>Deductive Functionality Classification</strong>
										</span>
									
									<br>
									 <p>Using this taxonomy, we then labeled our entire data set of 993 models based on the two types of designs and the axes of internal and external context. For each 3D model, two annotators examined the associated Thingiverse meta-data to understand the intended functionality of the model using shared images of the model being used in specific scenarios. Independently, each annotator labeled the model as Artifact or Task-Related based on its external functionality. This resulted in a Cohenâ€™s Kappa inter-rater reliability score of 0.94. They negotiated differences to finalize the labels for each model, reaching full agreement. Two examples of models that required negotiation are ThingID:3096598 (Chainmail) and ThingID:1015238 (Robotic Arm). The annotators resolved the former to be an Artifact because the Thingiverse page did not showcase any task-specific use case, and the latter to be Task-Related since its metadata con- tained videos describing a specific functionality. Following data classification, we removed all models that had no aesthetic seg- ments because these cannot be readily modified. This exclusion primarily removed models used for calibrating printers where any change would have changed the functionality. Our final, labeled data set contained 938 models and is summarized in Table 1.</p>
									

								</div>
							</div>
					</div>

					<span class="medium-headline">
						FUNCTIONALITY-AWARE SEGMENTATION
						</span>
					<br>
					</p>
				<div title="">
					<div>
						<div>
							
					<p>Our formative study helps us better understand the types of functional segments that affect the functionality of the model. Based on these results and our data set, we present a method for functionality-aware segmentation and classification of 3D meshes designed for 3D printing. In this section, we first define our segmentation problem and present our segmentation approach. Next, we present a method for classifying internal and external functionality on each segment of a model. We present our approach to tuning important hyper-parameters that affect the efficacy of our method and an evaluation of our classification approach compared to labels generated in our formative study. Finally, we present our Style2Fab user interface developed using this functionality-aware segmentation approach.</p>
					
					<p>We use an unsupervised segmentation method based on spectral segmentation that leverages the mesh geometry to predict a mesh-specific number of segments. This method allows us to generalize across 3D printable models with diverse functionality. Using our Thingiverse data set, we evaluated this method for its accuracy in predicting the number of functional segments and its ability to handle a wide range of mesh resolutions.</p>

					<p>
						<span class="small-headline">
							<strong>Segmentation Approach</strong>
							</span>
						
						<br>
						 <p>The process of segmenting a 3D mesh can be defined as finding a partition of the mesh such that the points in different clusters are dissimilar from each other, while points within the same cluster are similar to each other. We use a spectral segmentation process that leverages the spectral properties of a graph representation of the 3D mesh to identify meaningful segments. By examining the eigenvectors and eigenvalues of the graph norm Laplacian matrix, this method captures the underlying structure of the mesh and groups similar vertices together, resulting in a meaningful partition of the model.</p>

						 <p>Consider a 3D mesh as a graph where nodes represent a set of faces and edges represent connections between adjacent faces. The segmentation problem is to decompose the mesh into ğ‘˜ non- overlapping sub-graphs that represent a piece of the model with consistent features (e.g., the base, outer rim, and inside of a vase). The hyper-parameter ğ‘˜ can be any integer value between 1, one segment containing all faces in the mesh, and ğ‘›, one segment for every individual face in the mesh. If ğ‘˜ is too low, the segments will not be able to isolate components with unique functionality (e.g., the base of a vase is not separate from the outside). If ğ‘˜ is too high, functional components of the model may be split into multiple segments and may be modified in incompatible ways (e.g., half of the base is stylized with a surface texture and the other half has the original flat surface). A key challenge in functionality-aware segmentation is automatically selecting a value of ğ‘˜ for each design; we do not assume that makers will be able to easily identify a good value of ğ‘˜ when examining a design.</p>

						 <p>
							Predicting the Number of Segments: We use a heuristic-based approach for estimating a value of ğ‘˜ that partitions a mesh into segments that isolate functionality. Using a 3D meshâ€™s degree- and adjacency-matrix, we use spectral decomposition to extract an eigenbasis for the mesh. This allows us to use the resulting eigen-value distribution, representing the connectedness of a mesh, to identify a partition yielding the highest connectedness for individual segments. 
						 </p>

						<p>
							We first describe the spectral segmentation approach. Given a 3D mesh where ğ¹ represents the set of faces, we first construct a weighted adjacency matrix ğ‘Š. The element ğ‘Š<sub>ğ‘–ğ‘—</sub> represents the similarity between faces ğ‘“<sub>i</sub> and ğ‘“<sub>j</sub>, calculated using the shortest geodesic distance between the centers of faces ğ‘“<sub>i</sub> and ğ‘“<sub>j</sub> and the angular distance between them.
						</p>

						<p>
							We use the weight matrix, ğ‘Š , and the degree matrix, ğ· in order to compute the eigenvectors and values of the face graph. Formally defined as the norm Laplacian of a graph, ğ¿ = âˆšğ·<sup>T</sup>ğ‘Šâˆšğ·. From the eigenvalues of ğ¿, we are able to capture the connectedness of the mesh, where large gaps between eigenvalues imply weak connectedness.</p>

						<p>In the approach proposed by Liu and Zhang, the eigen-vectors corresponding to the smallest ğ‘˜ eigenvalues ğœ† are used to construct a k-dimensional feature space, where ğ‘˜ is the desired number of segments. Instead of using the smallest ğ‘˜ eigenvalues, we analyze the entire distribution of eigenvalues ğœ†. A high standard deviation in the eigenvalue distribution indicates that the eigenvalues are spread out over a wide range, which suggests a more complex graph structure with varying connectivity and potentially multiple distinct clusters or segments. In this case, the graph may benefit from a more refined segmentation process. On the other hand, a low standard deviation implies that the eigenvalues are more tightly clustered, which suggests a relatively uniform graph structure with fewer distinct clusters. In this case, it would be sufficient to partition the graph into a lower number of clusters. We leverage this distribution to automatically calculate a value of ğ‘˜. Specifically, we calculate the number of eigenvalues that have a higher dispersion than the distributionâ€™s standard deviation, using Equation 1.
					</p>

						<p>Once we have extracted the lowest ğ‘˜ eigenvectors and their corresponding eigenvalues we follow Liu and Zhangâ€™s segmentation method that uses k-means clustering to identify segments spanning from the ğ‘› faces captured by these high-variation eigenvectors. Based on the resulting clusters, we assign each face in the mesh graph to its corresponding segment, resulting in a segmented 3D model.</p>


						<p>Uniform Mesh Resolution for Segmentation: This segmentation approach is dependent on a uniform resolution of a mesh; non-uniform meshes will produce incoherent segmentation as some portions of the model are represented by too few faces and other portions have too many faces. Unlike other segmentation approaches, our data set of real-world models did not have consistent resolution and this would have affected the utility of our method. Thus, we re-mesh all models to give them a uniform 25k resolution using Pymeshlab. Note that this processâ€™ runtime increases with the resolution. Therefore, we want a low-resolution value that does not negatively affect our segmentation and classification method.</p>

						<p>We determined that resolution to be 25k (vertices) by segmenting and comparing 100 randomly selected 3D models from our data set. For each mesh, we segmented the remeshed models with 15K, 20K, 25K, 30K, and 35K vertices. We then looked for the lowest resolution that stabilized the predicted number of segments ğ‘˜. That is, for all higher resolutions, the number ğ‘˜ did not change. For 88% of models, a 25k resolution stabilized this value. The segmentation at 25K resolution took an average of 72 seconds, while segmentation at 30K resolution took an average of 102 seconds. Figure 3 shows the effect of mesh resolution on the number of models it stabilized and the time it took to complete segmentation.</p>


					</div>
				</div>
			</div>

						<p>
							<span class="small-headline">
								<strong>Analyzing Functionality in 3D Models</strong>
								</span>
							
							<br>
							 <p>After segmentation, the system must classify each segment as functional or aesthetic. To do this we use a heuristic that infers that if a segment ğ‘– is topologically similar to (i.e., shaped like) another segment ğ‘— , the functionality of ğ‘– will be the same as ğ‘— . Thus, to classify each segment, we must find some similar topology that has already been labeled as functional. Using the taxonomy of internal and external functionality from our formative study, we can break up the problem of finding a similar, labeled segment ğ‘— into two approaches: (1) we analyze external functionality by identifying topologically similar segments in models in our Thingiverse data set, and (2) we determine internal-functionality in multi-component models by identifying linkages between components based on topologically similar segments. However, it is critical to compare segments and their parent meshes because comparing only segments introduces noise. Segments without the context of their parent mesh are just geometrical features that, while topologically similar, are likely to be used in different ways.</p>

							 <p>Using Similarity as a Heuristic: We hypothesize that similar models will use similar geometries to enact a similar functionality. We use the approach of measuring topological similarity from Hilaga et al., which uses a Multiresolution Reeb Graph representation (MRGs) of meshes to analyze the similarity. This method is ideal for our domain due to its invariance to translation, robustness to connectivity variations, and computational efficiency. Note that we can use this method on both whole meshes and individual segments since a segment is, itself, a mesh of connected faces. Thus, given segment ğ‘ <sub>ğ‘–</sub> in a mesh ğ‘š <sub>ğ‘–</sub> and segment ğ‘ <sub>ğ‘—</sub> in mesh ğ‘š<sub>ğ‘—</sub> , the contextualsimilarity ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™_ğ‘†ğ‘–ğ‘š(ğ‘ <sub>ğ‘–</sub>,ğ‘  <sub>ğ‘—</sub>) is the product of the similarity between the segments and their parent meshes (Equation 2). This gives us a value between 0 (i.e., a complete topological mismatch) and 1 (i.e., identical topology).</p>

							 <p>Given a segment ğ‘  in mesh ğ‘š and a set of other meshes M, we can find the similarity between ğ‘  and all of the segments S in the other meshes. From these similarity values, we decide the label of ğ‘ â€™s functionality by comparing it to the labels on the subset of S that are most similar S<sub>ğ‘ ğ‘–ğ‘š</sub>. We take a uniformly weighted vote of the labels on each segment in S<sub>ğ‘ ğ‘–ğ‘š</sub> and classify ğ‘  as the majority label. Regardless of the similarity of these most similar segments, we weigh their label votes equally. We empirically found that the accuracy of functionality classification converges after comparing ğ‘  to five other similar segments (i.e., |S| = 5).</p>


							 <p>Now that we have a method of labeling a segment based on a related set of pre-labeled meshes M, we must find mesh sets that help us to identify, separately, internal and external functionality. The size of M will have a significant effect on the time it takes to compute segment similarity. For each mesh ğ‘š<sub>ğ‘–</sub> in the set, there will be ğ‘˜<sub>ğ‘–</sub> segments to compare to ğ‘ . Thus, as the size of the mesh set increases the number of similarity comparisons increases by a factor of ğ‘˜<sub>ğ‘–</sub> and quickly becomes too time-consuming to compute. To classify functionality, we need to find a small set M that provides the most information about the segment with the least amount of noise. Our insights into the differences between internal and external functionality help us to select good sets of meshes.</p>

							 <p>Classifying External Functionality: To identify external functionality, we compare segments to models that function similarly using similar geometric features. We built a labeled data set of segmented models from Thingiverse for identifying external functionality from the 46 Artifacts and 91 Task-Related single-component models in our data set. First, we segmented all of these models using our segmentation method and produced 1151 different segments. Then two annotators analyzed each segment with contextual information from the parent modelâ€™s Thingiverse page and independently labeled the segment as Aesthetic or Functional. We also asked the annotators to independently label a segment if it contained an Aesthetic and Functional component fused together (inefficient segmentation). After reviewing all segments, they had an inter-rater reliability of 0.97. They negotiated all disagreements to produce our ground truth classification of segment type. At the end of the study, 51% of the segments were annotated as Aesthetic, while 49% segments were annotated as Functional. From the functional segments, the annotators agreed that 24 segments (2%) from 17 different (12.4%) models were composed of Aesthetic and Functional elements fused together, leading them to annotate the entire segment as Functional.</p>

							 <p>Naively, we could classify the external functionality of a segment ğ‘  in a mesh ğ‘š by comparing it to all meshes in this data set. However, this would be computationally expensive and introduce noise since segment-level similarity may occur between segments that are used in different ways in models with different uses. Thus, we prune the set of labeled models by first comparing mesh-to-mesh similarity. Thus, we collect the set of most-similar meshes M<sub>ğ‘ ğ‘–ğ‘š</sub> from our data set. As shown in Section 4.2.1, we found that five meshes were sufficient. Considering External Functionality to be a binary label, we got a Precision score of 74% and a recall of 88%. Hence, this is a conservative approach towards functionality prediction, where the classifier has a higher false positive rate. It is more important that the system does not modify functional components than that it misses aesthetic components.</p>

							 <p>Classifying Internal-Functionality: Similar to the case of external context, we can identify internal functionality by comparing a segment to a set of segmented models with similar internal functionality. However, internal context is dependent on similarity within a group of meshes that make up a mechanism. Two segments have internal functionality if they interface with each other to form a linkage in a mechanism. Thus, our problem was to identify pairs of segments between components (multiple models that compose a single mechanism) and classify them as internally functional. We approach the problem in the following manner: We first create the set (ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ (M<sub>ğ‘š</sub>)) containing all possible pairs of segments not from the same mesh. For any model ğ‘š, we can now search over all possible pairs of segments between different meshes from the set ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ (M<sub>ğ‘š</sub>), and identify similar segments.</p>
								
							 <p>In this case, instead of assigning the label by taking a vote across the five most similar segments (i.e., Sğ‘ ğ‘–ğ‘š), we decide that there is a linkage between a segment ğ‘  and another segment ğ‘ <sub>ğ‘</sub> , in another component mesh, if they have a similarity value greater than the hyper-parameter ğ›¼. In the case that multiple segments have high enough similarity, ğ‘ <sub>ğ‘</sub> will be the segment with the highest similarity to ğ‘ . Once identified, we label both ğ‘  and ğ‘ <sub>ğ‘</sub> as having internal functionality.</p>
							
							 <p>To increase efficiency, we prune the number of segment comparisons in this process. First, we can exclude all segments in meshes that have been labeled as having external functionality. Ultimately, we are looking for a single binary label between aesthetic and functional, so it does not matter if a segment is both internally and externally functional. Second, as we identify linkages we can remove both ğ‘  and ğ‘ <sub>ğ‘</sub> from future comparisons within the mechanism because linkages are formed of only two segments.</p>
							
							 <p>To decide that two segments are sufficiently similar to be considered a linkage, we need a threshold similarity value of ğ›¼. To identify this threshold we gathered ground truth data from our data set. Two researchers annotated segments of the multi-component models in our data set to identify linked segments. We randomly selected 50 things with multiple components which contained a total of 157 component meshes. For each model, two annotators independently labeled all pairs of segments that formed a linkage in the mechanism resulting in an inter-rater reliability score of 0.99. They negotiated disagreements and produced a ground truth dataset. From this data set, we identified an effective threshold similarity score ğ›¼ = 0.86 by evaluating the precision and recall across multiple ğ›¼ values. We found that ğ›¼ = 0.86 maximized the identification of functional segments and then minimized the misidentification of aesthetic segments. By this analysis, we got a precision value of 64% and a recall value of 86%. Like classifying external context, we prioritize high recall over precision since the cost of missing a functional segment will break a model while missing an aesthetic segment will only affect aesthetics. Therefore, we opted to have a more conservative classifier for both the internal and external functionality.</p>


						<span class="small-headline">
							<strong>Stylization of Segments</strong>
							</span>
							<p>
								We use Text2Mesh [32] to stylize models based on text prompts. Text2Mesh uses a neural network architecture that leverages the CLIP [39] representation. The system considers a 3D model as a collection of vertices, where each vertex has a color channel (RGB) and a 3D position that can move along its vertex normal. Text2Mesh reduces the loss between the 3D model rendering (CLIP representation) from different angles and the CLIP representation of the textual prompt using gradient descent. Text2Mesh makes small manipulations in both the color channel and vertex displacement along the vertex normal for each of the vertices in order to make it look more similar to the text prompt. This allows the system to generate a stylized 3D model that reflects the userâ€™s desired style.</p>
								<p>This method will stylize the whole mesh and change the topology of the functional segments. In Figure 4a-c, we show that global stylization can render a functional object, in this case, an articulated cat, inoperable. We augment this system by adding an additional step of masking the gradients and setting functional vertices to zero. This allows manipulation of the color and displacement channels while preserving the functional segments of the model. As specified in Text2Mesh [32], we run this optimization for 1500 iterations.
							</p>

							
						<span class="small-headline">
							<strong>Style2Fab User Interface and Workflow</strong>
							</span>
							<p>
								Style2Fab is a plugin for the open-source 3D design software tool Blender. To stylize a model with Style2Fab, the user must: (1) pre- process their model for segmentation and stylization, (2) segment and classify the functionality of each segment, (3) selectively apply a style to segments based on functionality, and (4) review their stylized model. We break these tasks up into four menus in the user interface (Figure 5).</p>

								<p>Pre-Processing and Segmentation: Once the user has loaded an OBJ file of their 3D mesh into the plugin, they â€œProcessâ€ the model to standardize its resolution and automatically detect the number of segments (i.e., ğ‘˜) needed to classify functionality across the model (Figure 5b). By default, the resolution is set to 25k faces based on our evaluation. Next, the system will segment the model and give each segment a unique color to help the user visually identify the segments. If the user wants more or fewer segments they can modify the value of ğ‘˜ in the interface and re-segment the model. For multi-component models, the user can load multiple meshes representing each component and segment them in parallel.</p> 
								
								<p>Functionality-Verification of Segments. After segmentation, the plugin opens panels displaying the type classification of each segment. Users can then review each segment and determine if they agree with the classification. The userâ€™s goal is to identify the set of segments that should not be stylized to preserve the desired functionality of the design. To simplify this process, the user can select â€œHighlight all functional segmentsâ€ (Figure 5c) to have all segments classified as functional highlighted in the user interface. If they agree with this segmentation, they can move on to stylization. Otherwise, they can individually review all segments. Next, we describe the process for the user to review individual segments.</p>
								
								<p>First, in order to verify externally functional segments, the user can walk through the modelâ€™s segments and toggle the functionality class based on their interpretation of the model. When walking over a segmented model via the interface, the segments are highlighted on the model (Figure 5b).</p>
								
								<p>When working with multiple components, the user can review the segments that were classified as having internal context based on linkages between components of the model. The user can use the "Assembly" panel (Figure 5c) to see pairs of connected segments on distinct models, and click â€˜Separateâ€™ for incorrect assignments. If the user disagrees with this classification, they can adjust this similarity parameter ğ›¼ between 0 and 1.</p>
								
								<p>Selective Stylization of Aesthetic Elements: Post verification of functionality, users can stylize aesthetic segments of a 3D model by entering a natural language description of their desired style and clicking "Stylize Mesh". The completed model is then rendered alongside the original for review. Users can iterate on this process and apply new styles using new text prompts, or re-segment the model as needed.</p>




						<span class="medium-headline">
							USER STUDY
							</span>
						<br>
						</p>
					<div title="">
						<div>
							<div>
								
						<p>To evaluate if our functionality-aware segmentation method supports users in separating functional elements in 3D that they did not design, we had eight university students (Table 2) with varied 3D modeling and printing experience segment and stylize 3D models from our Thingiverse data set with and without automatic support from Style2Fab.</p>

						<p>
							We randomly selected eight 3D models representing each of our four categories of models based on internal and external context: two single-component Artifacts, two single-component TaskRelated models, two multi-component Artifacts, and two multicomponent Task-Related models. We segmented each model using our segmentation method and automatically classified the functionality of those segments. Each participant was presented with two models with segmentation and functionality classification (i.e., experimental group) and two models that were only segmented and that they needed to manually classify(i.e., control group). In each condition, the participant received an Artifact and Task-Related model. One of these was always a single component and the otherwas a multi-component model. We controlled for model-specific and learning effects by giving each participant a different combination of models and conditions. We asked participants to classify each segment in each model as functional or aesthetic. In the experimental condition, participants could accept or modify our functionality-aware classification. In the control condition, they had to make a manual classification. After classifying the segments, the users were asked open-ended questions about their experience. They were compensated with $20 for the hour-long study.
						</p>
	
								</div>
							</div>
					</div>
						<p>


							<span class="medium-headline">
								ABSTRACT
								</span>
							<br>
							</p>
						<div title="">
							<div>
								<div>
									
							<p>With recent advances in Generative AI, it is becoming easier to automatically manipulate 3D models. However, current methods tend to apply edits to models globally, which risks compromising the intended functionality of the 3D model when fabricated in the physical world. For example, modifying functional segments in 3D models, such as the base of a vase, could break the original functionality of the model, thus causing the vase to fall over. We introduce a method for automatically segmenting 3D models into functional and aesthetic elements. This method allows users to selectively modify aesthetic segments of 3D models, without affecting the functional segments. To develop this method we first create a taxonomy of functionality in 3D models by qualitatively analyzing 1000 models sourced from a popular 3D printing repository, Thingiverse. With this taxonomy, we develop a semi-automatic classification method to decompose 3D models into functional and aesthetic elements. We propose a system called Style2Fab that allows users to selectively stylize 3D models without compromising their functionality. We evaluate the effectiveness of our classification method compared to human-annotated data, and demonstrate the utility of Style2Fab with a user study to show that functionalityaware segmentation helps preserve model functionality.</p>
		
									</div>
								</div>
						</div>
							<p>

					


				
		
					
			 <br>
					<span class="medium-headline">
						BASIC WORKING PRINCIPLE
						</span>
					<br>
					<br>
					</p>
						 Photochromic dyes can transform from a transparent to a colored state through the absorption of UV light (â€˜activationâ€™), and transform back from colored to transparent through the absorption of visible light (â€˜deactivationâ€™). </p>
							<p>When cyan, magenta, and yellow photochromic colors are mixed together into a single solution and the solution is ac-tivated with UV light (i.e. all three color channels are fully saturated), the resulting color is black (Figure 2a). This is consistent with the CMY color chart shown in Figure 2b, i.e. the center of the chart, which shows the result of all three color channels in full saturation is black.  </p>
							<p><img src="images/photochameleon-CMY-color chart.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 2. (a) CMY inks mixed together achieve black, which matches (b) the CMY color model.<br>
						 </strong>
							</p>
			 
							 To achieve colors other than black, we need to deactivate one or more color channels. Deactivating the cyan color, for instance, would result in red since only yellow and magenta remain activated (Figure 2b). To deactivate each color channel individually, we can leverage the fact that the deactivation peak (i.e. absorption peak) of each photo-chromic color is at a different wavelength (Figure 3). </p>
							 <p><img src="images/photochameleon-absorption-peak.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 3. The absorption peak of each photochromic dye is at a different wavelength. To control each color chan-nel, we can use the RGB LEDs of a projector to supply the required deactivation wavelength.<br>
						 </strong>
		
						 <p>Since all of the deactivation wavelengths are within the spectrum of visible light (390 nm to 790 nm), we can use a regular office projectorâ€™s red, green, and blue LEDs to supply one deactivation wavelength each. As can be seen in Figure 3, shining blue light from the projector will deactivate yellow, green light will deactivate magenta and red light will deactivate cyan. Thus, to deactivate a specific photochromic color channel, the projector only needs to project R, G, or B pixels to reduce the saturation of the channel, as shown in Figure 4. </p>
		
							 <p><img src="images/effect of RGB on CMY (theoretical).png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 4. Theoretical result of shining different combinations of RGB on the mixed CMY photochromic ink.<br>
						 </strong>
		
							</p>
			 
							 Figure 5 shows the physical result of shining different combinations of RGB on the CMY ink mixture, which we ob-tained by following the deactivation chart in Figure 4. While Figure 5 demonstrates the results obtained by fully deactivating one or more channels, we can achieve intermediate colors of different saturation levels by only partially deactivating each channel. We will provide more information on this when we discuss the algorithm that times the projected deactivation wavelengths to accomplish this.  </p>
		
								<p><img src="images/Physical results of liquid.jpg" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 5. Physical result of shining different combina-tions of RGB on the mixed CMY dye. <br>
						 </strong>
						 </p>
			 
							 In the remainder of this paper, we will provide more detail on each of the steps, starting with (1) creating the photo-chromic CMY coating that can be applied to objects, (2) modifying the light output from the projector to ensure the wavelength of the light matches with the deactivation wavelength of the dyes, and (3) developing the algorithm to accomplish intermediate colors of different saturation levels. After this, we will (4) evaluate the color gamut that we can achieve with our method and determine how closely the physical color represents the virtual color. Finally, we will (5) discuss the implementation of our end-to-end system to transfer the texture onto an object.  </p>
		
				 <p><br>
			 <span class="medium-headline">
						1.DEVELOPING THE PHOTOCHROMIC COATING
						</span>
					<br>
					<br>
					</p>To make our method directly applicable to physical objects, we developed a re-programmable photochromic coating that can be airbrushed onto the surface of objects. To develop the coating, we dissolved cyan, magenta and yellow photochromic dyes directly into a transparent laquer.</p>
					
					<b>Creating the ink mixture</b><br>
						</p>
						Several factors were involved in selecting the dyes for the CMY solution: (1) The visual appearance of the dyes needs to be as close to cyan, magenta, and yellow as possible in order to achieve the largest color gamut; (2) to be able to control each color channel individually, the deactivation wavelengths for each photochromic dye need to have as little overlap as possible; (3) the color of the dyes needs to be stable (i.e. do not deactivate quickly under ambient light). </p>
						</p><b>#1 Visual appearance:</b> Figure 6 shows the available dyes from Yamada Chemical Co. which is the only compa-ny we found that reliably sells bi-stable (P-type) photo-chromic dyes. While we can see that a dye color exists for yellow (DAE-0068); for magenta, the nearest colors are red purple (DAE-0012), red (DAE-0004) and purple (DAE-0159), and for cyan: blue (DAE-0001) and blue purple (DAE-0018). </p>
		
						<p><img src="images/ink colors.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 6. Bi-stable photochromic inks available from Yamada Chemical Co.: 0.1wt% die mixed in ethyl acetate. <br>
						 </strong>
		
						 </p><b>#2 Absorption Spectra:</b> To be able to choose photochromic dyes that minimize overlap between different color channels, we determined the absorption spectrum for each photochromic dye. For this, we first mixed 0.1 wt% of each dye in ethyl acetate from Fisher Scientific using a magnetic stirrer for 1 hour at 500 RPM and then filled the solution into quartz cuvettes with a 1mL path length. We then radiated the solu-tion under UV light until the photochromic inks were fully activated and then placed the cuvettes in a spectropho-tometer (Varian Cary 5000 UV-Vis-NIR spectrophotometer) to determine their absorption spectra. The results are shown in Figure 7. We used yellow (DAE-0068) as a starting point for selection since it was the only available dye for this channel. For magenta, red purple (DAE-0012) and red (DAE-004) had the least overlap with yellow and cyan. For cyan, the least overlap with magenta was blue (DAE-0001) followed by blue purple (DAE-0018). </p>
		
						 <p><img src="images/absorption spectrum.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 7. Absorption spectra of the photochromic dyes in the visible wavelength range. <br>
						 </strong>
		
							</p><b>#3 Stable under ambient light:</b> Both red (DAE-0004) and blue purple (DAE-0018) were not very stable as they deac-tivated in ambient light after a few minutes. Thus, based on our criteria, we decided to use for cyan: blue (DAE-0001), for magenta: red purple (DAE-0012), and for yellow: yel-low (DAE-0068). This combination minimized overlap of deactivation wavelengths while also being closest to the desired color channels and the greatest stability under ambient light. For the purpose of easier explanation, we will refer to the blue dye as cyan dye and the red purple dye as magenta dye for the remainder of the paper.</p>
		
							<b>Applying the ink to materials & objects </b><br>
						</p>
						Before mixing the three photochromic dyes together, we first mixed each dye separately in laquer (Dupli-Color Paint Shop Finish Systems Matte-Finish Clear Coat (BSP307)). For the three separate mixtures, we used 0.05 wt% cyan, 0.05 wt% magenta and 0.3 wt% yellow respectively. These concentrations were chosen based on the deactivation times of each dye: Because yellow deac-tivates faster than the other two dyes, the concentration of the yellow dye was increased in order to extend the illumination time required for this dye to deactivate. We then mixed the resulting liquids by equal volumes (1:1:1) to achieve our multi-color coating. </p>
		
						</p>Before applying the coating to an object, we primed the surface of the object with spray paint to avoid subsurface scattering of the projected light. We first sprayed each object with black paint (drying time: 30 min), and then subsequently sprayed a white paint layer (drying time: 24 hours). </p>
		
						<p><img src="images/spraying.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 8. (a) Applying the photochromic coating and (b) the resulting coated object after UV activation. <br>
						 </strong>
		
						 </p>After this, we sprayed our photochromic coating onto the surface of the object using an airbrush system (Iwata HP-CS). The saturation of the sprayed ink can be increased by applying a second layer of ink after letting the first layer dry for 20 minutes. The ink is then fully dried after ca. 24 hours. </p>
						 </p>By using an airbrush to apply the coating, we can coat the surface of an object evenly. While we sprayed the layers manually and found the consistency to be good enough for our applications, a CNC airbrush system may further improve the results. The coating can also be painted, however, this leads to an appearance that is less uniform. </p>
		
						 </p>We used laquer as the carrier material for the photochromic dyes. The laquer is recommended for use on metal (see example of model car exterior in Figure 25) and plastic (3D printed chameleon in Figure 8).</p>
		
				 <p><br>
			 <span class="medium-headline">
						2. MODIFYING THE PROJECTOR 
						</span>
					<br>
					<br>
		
					</p>As mentioned previously, to deactivate each photochromic color channel individually, we need three different light sources: one light source with the corresponding deactiva-tion wavelength for each photochromic dye. Since existing DLP projectors already contain three light sources (3 LEDs for R,G,B), we decided to modify an existing office projec-tor (model: AAXA M6, 1200 Lumens). </p>
		
					</p>To determine the optical spectrum of each of the LEDs in the projector, we used a spectrometer (Thorlabs, CCS200) and found that the three LEDs overlap well with the select-ed photochromic dyes (Figure 9).</p>
		
					<p><img src="images/projector spectrum.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 9. Optical spectrum of the projector output and absorption spectra of cyan, magenta and yellow dyes. <br>
						 </strong>
		
						 </p>The blue LED of the projector creates light output over a small wavelength range and matches well with the yellow deactivation wavelength, therefore, by shining blue light from the projector, we can deactivate the yellow photo-chromic. Similarly, the red LED of the projector creates a light output over a small wavelength range and matches well with the cyan deactivation wavelength, thus, by shining red light, we can deactivate the cyan photochromic color. </p>
		
						 </p>Unfortunately, while the green LED of the projector matches well with the magenta deactivation wavelength, the default green LED creates a light output over a broader wavelength range. As a result, while shining green light would deactivate the magenta dye, it would also deactivate large parts of the yellow and cyan dyes.</p>
		
						 </p>To limit the wavelength range to only deactivate magenta, we opened up the projector and added a filter in front of the green LED (Semrock BrightlineÂ® FF02-529/24-25). To ensure the highest energy output, we placed the filter between the green LED and the collimator (Figure 10). With this modification, we were able to otherwise use the projector as is. </p>
		
						 <p><img src="images/projector modification.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 10. Modified projector with added optical filter between green LED and collimator. <br>
						 </strong>
		
				 <p><br>
			 <span class="medium-headline">
						3. ALGORITHM TO COMPUTE DEACTIVATION TIMES
						</span>
					<br>
					<br>
		
					</p>With the modified projector at hand, we can now create a projection image to control the photochromic color chan-nels of each pixel on the physical object.  </p>
		
					<b>Effect of Projector Distance on Color Gamut </b><br>
		
					</p>When starting our research, we had initially placed the photochromic mixture ink directly in front of the projector, i.e. only a few centimeters away from the lens, which produced the results shown in Figure 5. We found that once we placed the photochromics further away from the projector, i.e. at a projection distance of 30cm to allow the projected image to be in focus, the available color gamut reduced. Figure 11 shows the result acquired when projecting the same R,G,B sequence as in Figure 5 for comparison. Our assumption is that this difference is due to the change in light intensity, which we will explore in more detail in future work. By determining a relationship between projector distance and deactivation time, this could enable us to add â€˜projector distanceâ€™ as a variable into our system to accommodate different placements of the objects to be re-colored.</p>
		
					<b>NaÃ¯ve approach vs. Optimization algorithm </b><br>
		
					</p>In an ideal scenario each LED would deactivate only one color channel, however, as seen in Figure 9, shining light from one of the LEDs also partially deactivates the other two color channels. This has two implications: First, using the photochromic dyes commercially available today, we can achieve only part of the CMY color spectrum (Figure 14). For instance, it is not possible to achieve a fully saturated cyan as deactivating magenta also causes cyan to partially deactivate. Second, a naÃ¯ve approach that as-sumes that each projected deactivation wavelength (R, G, B), only affects a single photochromic color channel (Figure 4) will not lead to the correct color on the object due to additional effects of the other color channels on the dye. We therefore developed an optimization algorithm that takes into account the effect of overlapping absorption spectra. Figure 11 shows (a) the desired color texture and the results from both (b) the naÃ¯ve approach and (c) our approximation algorithm, the latter of which leads to a color representation closer to the desired.</p>
		
					<p><img src="images/optimization results.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 11. (a) Expected result. Results from (b) the naÃ¯ve approach and (c) our optimization.<br>
						 </strong>
		
						 <b>Deriving the Parameters of the Algorithm </b><br>
						
						 </p>Since our algorithm takes into account the effect each projector LED has on each photochromic color channel (i.e. its target photochromic color channel and the side effects on the other two channels), we first had to determine saturation decrease over time for each combination of LED and dye.</p>
				 
		
						 </p>Apparatus and Procedure: To capture the deactivation times of each dye when exposed to each of the wavelengths R, G, B from the projector, we airbrushed cyan, magenta, and yellow coatings onto seperate white cubes. We placed each cube in front of the projector and fully activated the photochromic dye by shining UV light on its surface for 30 s.</p>
		
						 </p>We then shone all three deactivation wavelengths red (R), green (G) and blue (B) onto each cube in order to quantify the effect of the projectorâ€™s LEDs on each dye. </p>
		
						 </p>The saturation level of each projected R, G, B color bar linearily increased from left to right over time until it reached the right edge of the cube (Figure 12). This proce-dure created a color gradient from fully saturated dye (left) to increasingly desaturated dye (right) and intermediate saturations in between. We then extracted the relative satu-ration level by capturing a photo of each cube under white light, converted the photo to the CMY color space, and plotted the saturation per color channel as the relative satu-ration decrease over time.</p> 
		
						 <p><img src="images/deactivation speed.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 12. Effect of exposure time of R, G, B projector channels on C, M, Y coatings (left = 0s, with time increasing to the right). <br>
						 </strong>
		
		
						 </p>Result: Figure 13 shows the relative saturation levels over time for each dye and each of the projectorâ€™s color channels. While each of the projectorâ€™s R, G, B LEDs deactivated its primary photochromic color channel, the deactiva-tion times varied significantly from 32 seconds for the yel-low dye under blue light to 620 seconds for the magenta dye under green light and 800 seconds for the cyan dye under red light. </p>
		
						 <p><img src="images/deactivation speed curves.png" alt="mosculpt-runner" width="700" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 13. Deactivation times of the photochromic dye per light channel (R, G, B). We consider a dye to be deactivated when it drops below 5% saturation. <br>
						 </strong>
		
						 </p>Computing Deactivation Times: We used the data on relative saturation level over time as input into our approximation algorithm. To minimize the global error rate across all three channels, our algorithm proceeds as follows: Let t=(t_r,t_g,t_b ) be the illumination time of the projectorâ€™s color channels, then the estimated color of a coated surface C(t) can be written as:</p>
		
						 <p><img src="images/image016.png" alt="mosculpt-runner" width="300" style="padding-bottom:0px; margin-bottom:10px"><br>
						
		
						 <p>Where X âˆˆC,M,Y are cyan, magenta, yellow at full saturation and  a_j,b_j,c_j  are linear factors on the saturation reduction in relation to the illumination time t per color channel. Let P be the target color, then we want to minimize the expression:<p>
		
						<p><img src="images/image021.png" alt="mosculpt-runner" width="150" style="padding-bottom:0px; margin-bottom:10px"><br>
		
						</p>We use a gradient descent scheme which can be written as:</p>
		
						<p><img src="images/image022.png" alt="mosculpt-runner" width="250" style="padding-bottom:0px; margin-bottom:10px"><br>
		
						</p>With Î³ being the step size. </p>
		
						</p>An example of the improvement of our optimization algorithm over the naÃ¯ve approach was shown in Figure 11. Our greatest enhancement is in the yellow color, a result of our optimization algorithm, which prioritizes the red LED over a combination of the red and green LED (as predicted in Figure 4) to deactivate the cyan and magenta dyes (since the green LED also deactivates the yellow dye).</p>
		
				<p><br>
			 <span class="medium-headline">
						4. EVALUATING OUR SYSTEM
						</span>
					<br>
					<br>
		
					</p>To evaluate our approach, we measured the available color gamut and the maximum resolution of the projected tex-tures.</p>
		
					<b>Available Color Gamut </b><br>
		
				</p>First, we were interested in the color gamut we could achieve with the photochromic dyes purchased from Yamada Chemicals. To determine this, we placed a white cube coated with the CMY solution and fully activated it with the UV light until the coating appeared black. To sample the available color gamut, we took 5 images at evenly spaced deactivation times across the maximum deactivation length, i.e. for red/green: 1800 s and blue: 45 s. This created 5x5x5 sample images. After each light exposure, we used a camera to capture an image of the resulting outcome. We then converted the image into the CMY color space and extracted the mean value of each color channel using an OpenCV script.</p>
		
				</p>Figure 14 shows the captured texture colors of this experiment in the CIExy chromaticity diagram. As can be seen, our color gamut has its greatest impact in the area between the three primary photochromic colors.</p>
		
				<img src="images/color gamut.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 14. Achievable color gamut of our photochromic coating in the CIE xy chromaticity diagram. <br>
						 </strong>
		
		
						 <b>Texture Resolution </b><br>
		
						 </p>Next, we quantified the texture resolution that can be achieved using the photochromic coating. We projected three black and white checkerboard patterns with varying checkerboard widths (5px, 2px, and 1px width), onto the surface of a coated cube and applied the texture on the cube for 12 minutes until the white checkerboard areas (white = all three R, G, B LEDs on) were fully deactivated. We measured the pixel size of the physical texture using an Olympus SZ61 microscope (Figure 15) and found the pixel measurements to be consistent at 129 um for a single pixel (5px = 647 um, 35px = 257 um, 1px = 129 um).</p>
		
						 <p><img src="images/resolution test.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 15. Resolution of checkerboard pattern. <br>
						 </strong>
		
							</p>While Figure 15a/b (5px and 2px width) both show a clear checkerboard pattern, the squares start to become indistin-guishable in Figure 15c (1px width). To clarify if the blur at the edges of the individual pixels in Figure 15 stems from the projector being out-of-focus or from subsurface scattering, we ran an additional test: we attached a 1x1 cm square mask on a UV activated cube and projected white light on it, after which we removed the square mask. Examining under a microscope, the edges and corners of the pixels remained sharp indicating that the projectorâ€™s lens and focus is the main reason for blur. Thus, while with our current projector system, the maximum resolution we can achieve is 257 um (2px), a better projector could achieve higher texture resolutions. </p>
		
		
							<b>Color Fade Over Time </b><br>
		
							</p>The time for a texture to fade depends on the saturation level of the texture being applied to the object. A stronger saturation will take longer to deactivate fully than a lighter saturation. To evaluate the durability in normal lighting conditions, we activated three samples, coated with C, M, and Y colors, with a UV light and placed them under 150 lux illumination (average light intensity of a living room is 100â€“300 lux). We recorded the time each color took to disappear: the cyan and magenta colors disappeared after 26h and 19h respectively, while the yellow color disap-peared after 5h (Figure 16). Since outside light is stronger than indoor lighting, the dyes deactivate quicker and are therefore more suitable for indoor use. These deactivation times are a limitation in the materials currently commercially available.</p>
		
							<p><img src="images/color fading curves.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 16. Color fade vs time for cyan, magenta and yel-low dyes under 150 lux illumination. <br>
						 </strong>
		
		
				<p><br>
			 <span class="medium-headline">
						5. END-TO-END SYSTEM TO TRANSFER A TEXTURE
						</span>
					<br>
					<br>
		
					
		
						 </p>In the last part of this paper, we describe the end-to-end system that allows users to transfer a texture onto a physical object. Figure 17 shows a summary of the system pipe-line.</p>
		
							<p><img src="images/end-to-end system.png" alt="mosculpt-runner" width="800" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 17. Overview of the end-to-end system. <br></p>
						 </strong>
		
		
						 <b>Hardware Setup</b><br>
		
		
						 </p>We use a similar hardware setup and component layout as ColorMod: For activation, we use a UV light (Luminus Devices Inc. CBM-40-UV, ~365 nm, 4W) that we automatically turn on/off using a digital controller (PhatLight LED Develop Kits). For deactivation, we use our modified projector (LED DLP projector AAXA M6, 1920x1080 pixels, 1200 Lumens, with added green filter). For 360Â° projection on the object, we use a rotating platform that is controlled with a stepper motor. The rotating platform has a positioning screw to ensure central placement of the object when rotating. The setup with the above components is shown in Figure 18.</p>
		
							<p><img src="images/hardware setup.jpg" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 18. Hardware setup.  <br>
						 </strong></p>
		
						 <b>#1 Applying a Virtual Texture to a 3D Model</b><br>
		
						 </p>After placing the physical object onto the rotating platform, users load the corresponding 3D model into the 3D editor Blender (Figure 19a). Next, users apply a virtual texture to the digital model using Blenderâ€™s texture mapping tools (Figure 19b). Clicking on the â€˜Previewâ€™ button converts the texture to the closest match realizable with the available photochromic color space (Figure 19c). To compute this preview texture, we run our optimisation algorithm on the virtual texture and compute the deactivation times t for each pixel. We estimate the resulting physical color by calculating C(t) and then load the newly computed texture onto the 3D model. The user can toggle between the virtual and the preview texture, adjusting colors as required.</p>
		
							<p><img src="images/user interface.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 19. (a) Loading the 3D model of the object, (b) applying a virtual texture onto the 3D model, (c) computing the preview texture. <br>
						 </strong></p>
		
						 <b>#2 Transferring the Texture onto the 3D Object</b><br>
		
						 </p>When users hit the â€˜transfer textureâ€™ button, our custom python plug-in for Blender handles the projection mapping. Our system first generates four ray traced projection images by virtually rotating the 3D model in 90Â° increments to reach all four sides of the object. The plug-in then sends the projection images to a Processing application (via the OSC protocol) that generates the deactivation animations. To create the deactivation animations, the Processing application converts the color values of each pixel in the projection images into deactivation times for each of the projectorâ€™s R, G, B color channels. It then computes the optimal deactiva-tion times for each pixel using our optimization algorithm. Our implemention generates this animation in 5-25 seconds per view depending on the size of the object.</p>
						 
						 </p>To begin the projection sequence, users hit the â€˜Activationâ€™ button to activate the photochromic coating on the object. The rotating platform turns 360Â°; in 10 steps (36Â°; increments, 1 min per step, 10 min total). Users then press the â€˜Projectionâ€™ button to apply the computed R, G, B anima-tions that deactivate the color channels on a per-pixel basis to create the desired color texture (Figure 20). Once the projection on one side of the object is complete, the platform rotates the object by 90Â° and repeats this process until all the sides have processed. The total time to apply color to the object depends on the objectâ€™s shape and the texture being applied (up to 45 min per side). </p>
		
							<p><img src="images/transferring texture.jpg" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 20. Projecting the deactivation colors onto the 3D object to achieve the desired texture. <br>
						 </strong></p>
		
						 </p>Compensating for Overlap: When projecting a texture onto the object from several sides, parts of the object may be projected onto multiple times. To compensate for this, we compare the intersection points of each ray on the object to the intersection points of all other views. If a hit point is closer than the projected pixel size (129 um), we set this pixel to black, removing any projection on this area.</p>
		
						 </p>Compensating for Slanted Edges: The more a surface is slanted, the less light from the projector will hit each target pixel on the surface. As a result, the same deactivating time will reduce saturation more on a perpendicular surface than on a slanted surface. To compensate for this, we calculate the incident angle of each ray on the virtual object and increase the illumination time in relation to this angle (e.g. a 45Â° angle to the projectorâ€™s position increases the illumination time by a factor of 2). We limit the maximal surface angle to 65Â° in order to limit the total illumination time.</p>
		
						 <b>#3 Final result</b><br>
		
						 </p>Figure 21 shows the final result of the texture after it has been applied to the photochromic object. The texture shown took 2 hours to apply. Since the texture contains pure yellow, which requires the longest deactivation time, most other textures can be transferred significantly faster. Note that since the texture transfer is done in a black box, no decay due to external light occurs during this process. The (de)activation times could also be significantly decreased by using a stronger projector with more lumen.</p>
		
							<p><img src="images/actual texture on photochameleon.jpg" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 21. Final result of the texture on the object. <br>
						 </strong></p>
		
							<b>Re-Programming (Repeat Step #1 - #3)</b><br>
		
							</p>Objects can be re-colored multiple times, simply by repeat-ing the procedure described above, i.e. the UV light will re-activate all color channels (resulting in a black object), enabling a new texture to be applied through the projector. The zebra texture show in Figure 22 took 1 hour to apply.</p>
		
							<p><img src="images/reprogramming new textures.png" alt="mosculpt-runner" width="416" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 22. Re-coloring the chameleon (zebra texture) by activating with UV light and applying a new texture. <br>
						 </strong></p>
		
				<p><br>
			 <span class="medium-headline">
						APPLICATION SCENARIOS
						</span>
					<br>
					<br>
		
					 </p>We demonstrate a range of applications that make use of our ability to reprogram textures on different materials. </p>
		
						<b>#1 Day-to-Night Clothing and Adaptable Fashion</b><br>
		
						</p>Photochromic shoes, textiles, and accessories could enable users to alter the appearance of their outfits from day to night without the need to carry multiple items. Applying the photochromic coating to a phone case would enable users to apply and change the design or pattern of the casing on a daily basis. Here, have applied the coating to a phone case of an iPhone XR (length: 15.3 cm, width: 7.8 cm) (Figure 23). The patterns on the phone case can be re-programmed. The textures took between 24-40 minutes to apply (top: 24 min, middle: 34 min, bottom: 40 min).</p>
		
						 <p><img src="images/photochameleon-phone case.png" alt="mosculpt-runner" width="700" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 23. The same phone case re-programmed three times with different textures. <br>
						 </strong></p>
							 
							 </p>To demonstrate the application of our coating onto textiles, Figure 24 shows a womenâ€™s size US 6 shoe (length: 24.5 cm) coated with different textures to match different outfits; both textures took 1.5 hours to apply.</p>
		
								<p><img src="images/photochameleon-shoes.png" alt="mosculpt-runner" width="700" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 24 The same shoe with two different textures to match a userâ€™s daily outfit. <br>
						 </strong></p>
		
		
						 <b>#2 Product Exploration</b><br>
		
						 </p>In a showroom or sales scenario, a re-programmable color-changing coating could enable potential buyers to explore different textures or patterns on a product before making a final choice. Figure 25 demonstrates this by exploring different textures for a car. To realise this vision, we sprayed the coating onto a 1/18 scale model of an Aston Martin Vanquish (length: 26 cm) and projected a range of different textures. A re-programmable coating could also be used within the context of car-sharing among multiple users or to display changing advertisements on the car exterior. The textures shown took 40 min (butterflies) and 60 min (flame) to apply.</p>
		
							 <p><img src="images/photochameleon-car model.png" alt="mosculpt-runner" width="700" style="padding-bottom:0px; margin-bottom:10px"><br>
						 <strong>Figure 25. The same car with two different color textures. <br>
						 </strong></p>
		
		
						 </p>Other scenarios we envision include large-scale applications, such as entire rooms in which walls or furniture can be re-colored (e.g. using a ceiling mounted UV/projector system); and dynamic physical visualizations (e.g. a 3D printed earth sphere could update during the day to show changes in weather).</p>
		
				<p><br>
			 <span class="medium-headline">
						LIMITATIONS AND FUTURE WORK
						</span>
					<br>
					<br>
		
						 </p>While the contributions we make in this paper extend the capabilities of prior work, there are several improvements that we will address as part of future work. </p>
		
						 <b>Creating a Larger Color Gamut</b><br>
						 </p>The photochromic dyes available on the market are cur-rently not optimal for our approach for multiple reasons: First, among the available photochromic dyes there is no good visual match for the magenta and cyan color channels, therefore we had to approximate these color channels with the closest available dye, which significantly reduced the color gamut. The development of photochromic dyes is an on-going research field in material science. Irie et al., for instance, demonstrated how to create a cyan dye that would potentially be a better candidate for our approach than Yamada Chemicalâ€™s blue dye. Second, the available photochromic dyes at present signifanctly overlap in their deactivation spectra, which further reduces the available color gamut since the color channels cannot be controlled independent of one another. Photochromic dyes with narrower absorption spectra would be better suited for our purposes. </p>
		
						 <b>Fabrication: Single Material 3D Printing (FDM, SLA): </b><br>
		
						 </p>Our approach can turn a single-material process into a multi-color process. While we have shown applications in coat-ing, we can also consider mixing the multi-color photochromic ink into a single roll of filament for fused deposition modelling (FDM) printing, or in resin used for stereo-lithography (SLA) 3D printing. This has the potential to enable multi-color prints at high resolution that are not limited by the number of extrusion heads or the single-resin.</p>
		
		
				<p><br>
			 <span class="medium-headline">
						CONCLUSION
						</span>
					<br>
					<br>
		
					</p>In this paper, we demonstrated how we can use photo-chromic dyes mixed into a single solution to create re-programmable multi-color textures that are made from a single material. We discussed the properties photochromic dyes need to have in order to achieve a large color gamut (appearance as close as possible to the color channel, minimized overlap in deactivation wavelengths), demonstrated how to create a mixture solution that provides even saturation in each color channel (mixing CMY 1:1:6) and explained how the projector needs to be modified to provide deactivation wavelengths that mostly deactivate a single channel (i.e. by adding filters). Finally, we explained our algorithm to create intermediate colors and provided an end-to-end walkthrough of our system that transfers a vir-tual color texture onto a physical object. For future work, we plan to further increase the capabilities of our system by collaborating with material science researchers to develop improved photochromic dyes and extending our fabrication methods to 3D printing with filaments and resins. </p>
		
		
			 <p><br>
			 <span class="medium-headline">
						ACKNOWLEDGEMENTS
						</span>
					<br>
					<br>
		
					</p>The authors would like to thank the Engineering and Physi-cal Sciences Research Council (EPSRC grant number EP/N509619/1) for partial funding of this work, Ford Motor Company for the financial support, Prof. Masahiro Irie (Rikkyo University) for sending us photochromic samples, Madeleine Laitz for help with analyzing the absorption spectrum of the photochromic dyes, Cattalyya Nuengsigkapian for help with the preview function, and Tom Buehler for support in video editing. Permission to publish was granted by Ford Motor Company. </p>
		
		
		
							
							
							<br><br><br><br>
						</p>
						</br>
	        


						
	    </div>
	  </div>
		
  </div>
    <p>
      <br>
      
      </br>
      </br>
      
      
    </div>
	</div>
</div>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/headerstrap-for-subpage.js"></script>
<!-- lightbox -->
<script type="text/javascript" src="../../js/lightbox.js"></script>

</body>
</html>
