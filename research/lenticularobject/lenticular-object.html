
<!DOCTYPE html>
<html>
<head>
	<title>Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="http://hcie.csail.mit.edu/images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="https://hcie.csail.mit.edu/css/bootstrap.css" rel="stylesheet">
	<link href="https://hcie.csail.mit.edu/css/custom-style.css" rel="stylesheet">

	<!-- Lightbox -->
	<link href="../../css/lightbox.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="https://hcie.csail.mit.edu/js/analytics.js"></script>


	<style>
      .center {
        text-align: center;
      }
    </style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>
<div class="container" style="padding-top: 100px;">
	<div class="row">
	<!-- Publication details -->
	<div class="col-md-4" style="text-align: left;">
		</br>
		</br>
		<span class="medium-headline">
		Publication
		</span>
		</br>
		</br>
        Jiani Zeng*, Honghao Deng*, Yunyi Zhu*, Michael Wessely, Axel Kilian, Stefanie Mueller.
		</br>
        Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint.
		</br>
		In Proceedings of
		<a href="https://uist.acm.org/uist2021/" target="_blank">UIST &#8217;21</a>.<br>
			</br>
			
				<!--
				<a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376202" class="btn btn-doi" alt="doi" target="_blank">DOI</a>
				&nbsp; &nbsp;
				-->

				<a href="https://groups.csail.mit.edu/hcie/files/research-projects/sensicut/2021-UIST-SensiCut-paper.pdf" class="btn btn-pdf" alt="pdf" target="_blank">PDF</a>
				&nbsp; &nbsp;
			<!-- 
				<a href="https://www.youtube.com/watch?v=BdvSAJaukI8" class="btn btn-vdo" alt="video" target="_blank">Video</a> -->
				&nbsp; &nbsp;
			<!-- 
				<a href="http://groups.csail.mit.edu/hcie/files/research-projects/G-ID/2020-CHI-G-ID-Talk.pptx" class="btn btn-talk" alt="slide" target="_blank">Slides</a>
				&nbsp; &nbsp;

			-->
				&nbsp; &nbsp;
			</br>
			</br>




			<span class="medium-headline">
			Videos
			</span>
			</br>
			</br>
	  <iframe width="360" height="203" src="https://www.youtube.com/embed/BdvSAJaukI8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<!-- <iframe width="325" height="190" src="https://www.youtube.com/embed/T-22KOGFLoQ" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe> -->
			<br>
			<br>
			<!-- <iframe width="325" height="190" src="https://www.youtube.com/embed/T-22KOGFLoQ" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe> -->
			</br>
			</br>
			</br>

			<span class="medium-headline">
			Press
			</span>
			</br>
			</br>
<!--			<ul>-->
<!--				<li><a href="https://news.mit.edu/2021/smart-laser-cutter-system-detects-different-materials-0819" target="_blank">MIT News</a></li>-->
<!--				<li><a href="https://www.csail.mit.edu/news/smart-laser-cutter-system-detects-different-materials" target="_blank">CSAIL News</a></li>-->
<!--				<li><a href="https://thenextweb.com/news/mit-laser-cutter-ai-tool" target="_blank">The Next Web</a></li>-->
<!--				<li><a href="https://www.photonics.com/Articles/Laser_Cutting_System_Differentiates_Between/a67285" target="_blank">Photonics.com</a></li>-->
<!--				<li><a href="https://www.lasersystemseurope.com/news/deep-learning-material-sensing-platform-augments-laser-cutting" target="_blank">Laser Systems Europe</a></li>-->
<!--				<li><a href="https://hackaday.com/2021/09/06/smart-laser-cutter-ad-on-detects-material-optically/" target="_blank">Hackaday</a></li>-->
<!--				<li><a href="https://materialdistrict.com/article/smart-laser-cutter-system-detects-different-materials/" target="_blank">Material District</a></li>-->
<!--				<li><a href="https://newatlas.com/science/mit-sensicut-laser-cutting-materials/" target="_blank">New Atlas</a></li>-->
<!--				<li><a href="https://techxplore.com/news/2021-08-smart-material-sensing-platform-laser-cutters.html" target="_blank">Tech Xplore</a></li>-->
<!--				<li><a href="https://www.hackster.io/news/cutting-risk-ed87ed027bf4" target="_blank">Hackster.io</a></li>-->
<!--				<li><a href="https://blog.adafruit.com/2021/09/10/a-smart-laser-cutter-than-automatically-identifies-what-its-cutting-piday-raspberrypi-raspberry_pi/" target="_blank">Adafruit</a></li>-->
<!--				<li><a href="https://scienceblog.com/524864/smart-laser-cutter-system-detects-different-materials/" target="_blank">ScienceBlog</a></li>-->
<!--				<li><a href="https://worldindustrialreporter.com/sensicut-adding-flair-to-laser-cutters/" target="_blank">World Industrial Reporter</a></li>-->
<!--				<li><a href="https://www.3d-grenzenlos.de/magazin/zubehoer-zusatzgeraete/mit-tool-sensicut-optimiert-laserschneiden-27742303/" target="_blank">3D-Grenzenlos Magazin (German)</a></li>-->
<!--				<li><a href="https://www.objetconnecte.com/sensicut-decoupe-laser-detection-materiau/" target="_blank">Objectconnecte (French)</a></li>-->
<!--				<li><a href="https://www.popmech.ru/science/news-737443-novyy-lazernyy-rezak-sam-opredelyaet-kakoy-material-on-rezhet/" target="_blank">Popmech (Russian)</a></li>-->
<!--				<li><a href="https://www.industrysourcing.cn/client/article/details.html?id=406962" target="_blank">Industry Sourcing (Chinese)</a></li>-->

<!--			</ul>-->
			</br>


<!--

			<span class="medium-headline">
			CHI 2020 Talk
			</span>
			<br><br>
			<iframe width="325" height="190" src="https://www.youtube.com/embed/gKueHVbQfWo" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe> 
			</br>
			</br>


			</br>
			
-->


		</div>





	<!-- Project information -->
	<div class="col-md-8" style="text-align: left;">
		<br>
	  <h3 class="headline">
          Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint
		</h3><br>
  <div title="Page 2">
    <div>
      <div>
        <p>
        	<img src="images/Fig1.png" width="720" alt=""/>
        </p>
		      <p style="font-size: 12px; font-weight: 600">
		      	Figure 1: 3D printed objects with lenticular lens surfaces enable viewers to see different appearances from different viewpoints(a: looking down; b: looking horizontally; c: looking up). Our user interface supports designers in setting up different viewpoints and assigning the corresponding textures. On export, it automatically generates the files for fabrication. Designers canthen 3D print the object geometry, lenses, and underlying color patterns in a single pass with a multi-material 3D printer.
		      </p>
		      <br>
	          
	        <p>
	        	In this paper, we present a method that makes 3D objects appear differently under different viewpoints. We accomplish this by 3D printing lenticular lenses across the curved surface of objects. By calculating the lens distribution and the corresponding surface colorpatterns, we can determine which appearance is shown to the userat each viewpoint.
	        </p>
	        <p>
	        	We built a 3D editor that takes as input the 3D model, and thevisual appearances, i.e. images, to show at different viewpoints. Our3D editor then calculates the corresponding lens placements andunderlying color pattern. On export, the user can use ray tracing to live preview the resulting appearance from each angle. The 3D model, color pattern, and lenses are then 3D printed in one pass ona multi-material 3D printer to create the final 3D object.
	        </p>
	        <p>
	        	To determine the best fabrication parameters for 3D printinglenses, we printed lenses of different sizes and tested various post-processing techniques. To support a large number of different appearances, we compute the lens geometry that has the best trade-offbetween the number of viewpoints and the protrusion from theobject geometry. Finally, we demonstrate our system in practicewith a range of use cases for which we show the simulated andphysical results side by side.
	        </p>

	      <div>
            <h1 class="medium-headline">INTRODUCTION</h1>

            <p>
            	Lenticular printing refers to the method of using lenticular lensesto show different images from different viewpoints. It is commonly applied in advertisement and arts to achieve special purposeeffects, such as showing various images on one commercial mo-tion card or creating artistic effects with the illusion of depth.
            </p>
            <p>
            	Traditionally, lenticular prints are limited to 2D. They are fabri-cated by placing flat lenticular sheets onto 2D color patterns, whichare composed of multiple images. Depending on the viewpoint,light rays enter the lens at different angles and thus reflect off different parts of the underlying color pattern. Therefore, underdifferent viewpoints, a different part of the color pattern is visibleto the viewer’s eye and as a result the viewer sees only a particular image that is contained in the color pattern.
            </p>
            <p>
            	One reason why lenticular prints do not yet exist in the form of 3D objects is that up until recently, no fabrication process existed that was able to manufacture the lenticular lenses and high resolution color patterns on doubly curved surfaces. However, over thelast years, multi-material 3D printers have been developed that can print with a multitude of materials and in high-resolution color.For example, the 3D printer Stratasys J55 can print with optical clear materials, which can be used for the lenses, as well as CMYKmaterials, which can be used to print the color pattern.
            </p>
            <p>
            	In this paper, we explore how to leverage the recent advancesin multi-material 3D printing to create curved 3D objects withlenticular surfaces. To facilitate the creation of objects that lookdifferent from different viewpoints, we provide designers with a3D editor that takes as input the 3D model, the desired viewpoints,and the corresponding images and then computes the lens place-ment and color pattern across the object’s surface (Figure 1). Before fabrication, designers can preview the resulting object from eachviewpoint via ray tracing, and then send it to the 3D printer.
            </p>
            <p>
            	To determine the best fabrication parameters for 3D printinglenses, we printed different lens sizes (2mm-5mm) and tested different post-processing techniques (different types of varnishes andoil). To achieve a large number of different appearances on one3D object, we computed the lens geometry that has the best trade-off between a large number of viewpoints and avoiding protrusion of the lenses from the object geometry. We show that although oursystem can support up to 19 different appearances per object as shown by our digital ray tracing simulation, inaccuracies in current fabrication techniques do not yet allow for these results in practice. We demonstrate the usefulness of lenticular objects with 4 applica-tion examples in product design and HCI showing the simulatedand the physical results side by side.
            </p>

           
          <p>In summary, we contribute:            
          <ul>
            <li>
            	an end-to-end fabrication pipeline for printing lenticular ob-jects in a single pass on a multi-material 3D printer by print-ing lenticular lenses and color patterns on doubly-curvedobject geometries;
            </li>
            <li>
            	an interactive 3D editor plug-in that allows designers todefine multiple viewpoints and assign the correspondingtextures and that generates the resulting fabrication files;
            </li>
            <li>
            	four application scenarios that demonstrate lenticular objectsmade with our fabrication pipeline.
            </li>
          </ul>
          <br>
  		</p>
      </div>
            
      <div>
      	<h1 class="medium-headline">BACKGROUND: LENTICULAR DISPLAYS </h1>
      	<p>
			Figure 2 shows a simplified illustration of a lenticular lens thatdisplays a different appearance under each viewpoint. To createthis effect, the lens has a color pattern consisting of multiple col-ored image spots underneath. Because of the magnifying effect ofthe lens, it displays the color from only one of the colored imagespots, which is only a small portion of the entire area under thelens (Figure 2a). Because each lens only shows one image spot fromeach viewpoint, the colored image spot is the equivalent to onepixel in a display. Which image spot the viewer sees and thus whichcolor the ’pixel’ has, depends on the viewpoint of the viewer due tothe different incident angles of the light hitting the lens (Figure 2b).Since each lens represents one pixel in the display, multiple lensestogether form a lenticular display and collectively show an imagethat varies dependent on the viewpoint.
      	</p>

  		<img src="images/Fig2.png" width="720" alt=""/>
  		<p style="font-size: 12px; font-weight: 600">
	      	Figure 2: (a) Magnifying effect of a lens: from each view-point, the user sees only a small fraction of the underlyingcolor pattern, i.e. one image spot. (b) From different view-points, users see different image spots under the lens, thusfrom each viewpoint the lens shows a different color.
        </p>
  		<br>

      	<p>
      		The quality of a lenticular display is determined by the size of thelenses and the number of image spots that fit under a lens. Thesmaller the lens, the smaller each pixel is, and thus the more pixelscan fit in a given area, resulting in a higher resolution of the image.The number of image spots that fit under a lens determines howmany different colors a pixel can take on, and thereby determineshow many images the viewer can see from the overall display.We show later in our paper how we compute the lens geometrythat has the best trade-off between the number of viewpoints (i.e.image spots underneath the lens) and the protrusion from the objectgeometry. In addition, we will report the results from experiments,in which we investigated the trade-off between the size and theresulting quality of a lens. Before reporting on these experiments,we will illustrate in the next section how to create 3D printed objectswith lenticular surfaces with our custom design tool.
      	</p>
      </div>  

      <div>
      	<h1 class="medium-headline">DESIGN TOOL FOR 3D LENTICULAR DISPLAYS</h1>
	    <p>
	    	To support designers in creating 3D lenticular displays, we developed a design tool that is integrated into an existing 3D modeling environment, Rhino3D (open source code available here: <a href="https://github.com/yunyi-zhu/lenticular-object" target="_blank">link</a>). Designers start by loading the 3D modelof the object and then defining a set of viewpoints and correspond-ing visual appearances. Our editor then automatically places thelenses on the 3D geometry and assigns the corresponding colorpattern to each lens. Before fabrication, designers can preview theresulting object via ray tracing. On export, our tool provides a setof fabrication files ready for 3D printing.
      	</p>
      	<p>
      		In the next section, we demonstrate the functionality of our edi-tor at the example of a kettlebell that guides the user into the correctexercise pose, i.e. shows a checkmark when the user holds the ket-tlebell at the correct height, and an upward and downward arrowwhen the kettlebell is being held too low or too high (Figure 1).
      	</p>

      	<h3 class="medium-headline">Defining Viewpoints by Placing Virtual Cameras</h3>
      	<p>
      		After loading the 3D model of the object, designers define theviewpoints by placing virtual cameras in the viewport at the desired3D positions (Figure 3a). To do this, designers first click the ’newviewpoint’ button, which creates a virtual camera in the viewport.It also adds the viewpoint to the list of all viewpoints in the panel.Designers can then either move the camera in the viewport oralternatively enter a 3D coordinate to position it. While the designeris moving the camera, our editor automatically orients the camerato always face towards the object. Designers can verify that theviewpoint is correctly positioned by clicking ’go to viewpoint’ inthe panel, which shows the view from the selected virtual camerato the object.
      	</p>

      	<div class="center">
	  		<img src="images/Fig3-UI.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 3: Defining Viewpoints: (a) add viewpoints by placing virtual cameras, (b) assigning images to each viewpoint, (c) verifying the texture appears from the assigned viewpoint.
	        </p>
	        <br>
    	</div>

    	<h3 class="medium-headline">Specifying Appearances for EachViewpoint by Loading 3D Textures</h3>
      	<p>
      		After specifying a viewpoint, designers can define which appear-ance that the object should have from the viewpoint. In our system,since we work with 3D models, an appearance is represented by a3D model texture. These textures need to be prepared upfront, i.e.either created by the designer or retrieved from a 3D model texturelibrary. After preparing the texture, designers can assign a textureto a viewpoint by clicking the ’assign image’ button (Figure 3b).Once a texture is added to a viewpoint in the panel, the correspond-ing camera is assigned to the texture (Figure 3c). Designers canverify the texture from a specific viewpoint by looking through itfrom the viewport of the camera.
      	</p>

      	<h3 class="medium-headline">Exporting the Fabrication Files: Generating Lenses and Color Patterns</h3>
      	<div class="center">
	  		<img src="images/Fig4-preview.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 4: Previewing the resulting object: (a) Generating thelens geometry and underlying color pattern. (b) Simulatingthe optical result using ray tracing to provide an accuratepreview of the object before fabrication.
	        </p>
	        <br>
    	</div>
      	<p>
      		Clicking the ’export fabrication files’ button generates the fab-rication files for 3D printing, i.e. exports the object geometry andthe lenses (.vrml) and an image file for the color pattern (.png).Designer can then upload the fabrication files to the slicer andafter slicing finished, send them to the 3D printer. In the next sec-tion, we describe the slicing and fabrication process as well as thepost-processing techniques we used to fabricate the objects withlenticular surfaces shown in this paper.
      	</p>
      </div>


      <div>
      	<h1 class="medium-headline">FABRICATION AND POSTPROCESSING</h1>

      	<p>
      		We first explain which 3D printer and printing materials we use,and then detail our fabrication process, which includes slicing the3D model, 3D printing the object, and then post-processing the surface of the object. We then report on experiments investigatingthe print quality of different lens sizes as well as the color patternresolution of our 3D printer.
      	</p>

      	<h3 class="medium-headline">Fabrication Process</h3>

      	<p>
      	<i>3D Printer and Printing Materials:</i> We fabricate our objects usinga multi-material 3D printer that can print the object geometry,color patterns, and lenses in one pass. We fabricated all the objectsin this paper using the <a href="https://www.stratasys.com/3d-printers/j55-prime" target="_blank">Stratasys J55 3D printer</a>, which usespolyjet technology. To print the lenses, we use the Stratasys polyjetmaterial VeroUltraClear, a type of clear 3D printable acrylic. Toprint the color patterns, we use materials from the <a href="https://www.stratasys.com/materials/search/verovivid" target="_blank">VeroVivid family</a> (VeroCyan-V, VeroMagenta-V, and VeroYellow-V) in combinationwith VeroPureWhite as the base material, which can createdifferent colors on a per-voxel basis.
      	</p>

      	<p>
      		<i>Slicing:</i> Before printing, we slice the fabrication file that containsthe object geometry and lenses (.vrml), which was exported fromthe 3D editor. To do this, we load the fabrication file into the slicerGrabCAD that can be used with the Stratasys 3D printers. Afterloading the .vrml file, we associate each part with the correspondingprint materials in the ’print settings’. To further improve the qualityof the printed lenses, we use the ’glossy’ finish instead of the ’matte’finish in the ’print settings’. Clicking the ’save’ button generates afile in .print file format, which we subsequently load into the 3Dprinter for fabrication.
      	</p>
      	<p>
      		<i>3D Printing and Removing Support Material: </i>After 3D printing, lensesprinted at an angle are either fully or partially covered in supportmaterial. To remove the support material that the Stratasys 3Dprinters use (SUP710 [43]), we used an Objet Powerblast WaterJetmachine, which washes off the support in less than 15 minutes.
      	</p>
      	<p>
      		<i>Polishing the Lenses:</i> Although we chose the ’glossy’ 3D printingsetting for the lenses, the lenses that are in contact with supportmaterial will still be matte since the 3D printer cannot use theglossy finish in those areas. To address this, we post-process thelenses by spraying 3 layers of varnish on the surface of the lenses.
      	</p>
      	<p>
      		Since the 3D printing process leads to imperfections in the lensesas well as in the color patterns, we ran a set of experiments todetermine the print quality of different lens sizes and color patternresolutions, which we report on in the next section.
      	</p>

      	<h3 class="medium-headline">Fabrication Quality of Different Lens Sizes</h3>
      	<p>
      		Our preliminary experiments showed that the quality of the fab-ricated lenses depends on several different parameters, such asthe lens size, the orientation in which the lens is printed, and thepost-processing technique applied to the lens after fabrication. Wetherefore fabricated lenses using different parameters and com-pared the physical result with the rendered view of the lenses toevaluate their quality.
      	</p>
      	<p>
      		<i> Procedure: </i> We generated four different lens sizes, ranging from adiameter of 2mm - 5mm in 1mm increment, using the lens geometryas determined in Section 6. Since the print orientation also impactsthe optical quality, we 3D printed each lens size in five orientations: (1) facing upwards, (2) facing45◦upwards, (3) facing sideways,(4) facing45◦downwards, (5) facing downwards.
      	</p>
      	<p>
      		After printing the lenses, we aligned the backplane of each lensonto an ink-jet printed checkerboard with a checker size of 500microns (printer: Xerox Workcentre 7970, paper: HP Premium PlusPhoto Paper | Soft Gloss). We took pictures from the top of thelenses with a camera (model: Canon T3i, focal length: 18mm, dis-tance from lens top: 40cm). We then compared the photos withthe rendered image of the same lens with the same checkerboardpattern, which shows the resulting image without any print imper-fections (rendering engine: LuxCoreRender [31]).
      	</p>
      	<p>
      		To evaluate different post-processing methods, we comparedfour techniques: (1) no post-processing, (2) lenses coated with threelayers of painting varnish spray (Liquitex), (3) lenses coated withthree layers of clay gloss varnish spray (Krylon) and (4) lensesrubbed with baby oil ( Johnson). For each technique, we fabricated anew copy of the lenses. To make sure that the post-processing effectis stable, we put the post-processed lenses in a room for14daysbefore taking photos.
      	</p>

      	<div class="center">
	  		<img src="images/Fig5-checkerboard.png" width="500" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 5: Fabricated lenses compared with rendered lenses.(a) Larger lenses have better fabrication accuracy, (b) lensquality varies among different printing directions, (c) clayvarnish and painting varnish both improve the lens quality,(d) lens quality of our chosen parameters (3mm lenses withpainting varnish)
	        </p>
	        <br>
    	</div>

      	<p>
      		<i>Result: </i>Figure 5 shows the photos of the lenses compared with therendered image from the same viewpoint. For the lens sizes, largerlenses show an image that more closely resembles the renderedimage than smaller lenses. Concerning the print orientation, lensesprinted facing upwards result in the best lens surface quality be-cause no support material is needed and thus all parts of the lens areprinted with glossy finish. All other angles need support materialin varying amounts and thus the lens quality decreases. From thoselenses printed with support material the best print quality was45◦upwards, followed by lenses facing downwards and45◦down-wards, and the worst print quality resulted from printing lenses sideways. For post-processing, both types of varnish improved theglossiness of the lens surface, with the painting varnish providingthe best surface quality. In contrast, the baby oil made the lensglossy when rubbed, but the effect faded away after 14 days.
      	</p>
      	<p>
      		Which lens size to use is a trade-off between image quality andoverall image resolution. While smaller lenses are of less print qual-ity, they provide a higher resolution, which more closely representsthe details of the input image. We found that for our applicationscenarios, 3mm lenses with post-processing preserved the impor-tant visual features of our input textures while providing sufficientvisual quality. However, for one of our example applications (ear-pod case), we chose to use 2mm lenses to fit more lenses onto theotherwise small geometry.
      	</p>


      	<h3 class="medium-headline">Resolution of 3D Printable Color Pattern</h3>
      	<p>
      		We also investigated the 3D printing resolution of the color pattern,which forms the image spots underneath the lenses. If the printablecolor pattern resolution is too low, the image spots assigned to thedifferent viewpoints merge together on the backplane of the lenscausing the viewer to see the wrong image from the viewpoint. Thecolor pattern resolution required depends on the size of the lens,i.e. larger lenses have larger image spots (see next section: Lens Geometry Design).
      	</p>

      	<div class="center">
	  		<img src="images/Fig6-resolution.png" width="500" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 6: 3D printed dots of different sizes from differentprint directions. Dots that have a 600 micron diameter arethe smallest that have good print quality.
	        </p>
	        <br>
    	</div>

      	<p>
      		<i>Procedure: </i>To determine whether the 3D printer’s resolution canaccurately fabricate the image spots for different lens sizes, we ranan experiment in which we 3D printed circle patterns represent-ing the image spots of different lens sizes (2mm lens: 400 microns,3mm lens: 600 microns, 4mm lens: 800 microns, 5mm lens: 1000microns) and then analyzed their quality under a microscope. Toaccount for different print orientations, we printed the circle pat-terns on the top, side and bottom surfaces of a cube with a sidelength equal to 10 times the circle’s diameter. We then used a mi-croscope (model: inskam-316) to take photos.
      	</p>
      	<p>
      		<i>Result: </i> As shown in Figure 6, the larger the printed circle pattern,i.e. image spot, the better the print quality. We found that the color pattern of600microns, which corresponds to the 3mm lenses, wasthe smallest that had a good print quality with sharp edges. Thus,with our color print resolution, lenses that are 3mm or larger canmaintain the computed number of viewpoints with different images.
      	</p>
      </div>





      <div>
      	<h1 class="medium-headline">LENS GEOMETRY DESIGN</h1>

      	<p>
      		We next describe how we computed the lens geometry that repre-sents the best trade-off between a large number of viewpoints andavoiding protrusion of the lenses from the object geometry). Wethen render an example lens with the resulting lens geometry toshow that it supports the calculated number of viewpoints.
      	</p>
      	<p>
      		<i>Optimal Lens Geometry: </i>Figure 7a shows the three parameters thatdetermine the geometry of a lens: (1) the diameter 𝑑𝑏𝑎𝑠𝑒 of the lens,(2) the radius 𝑟 of the top spherical surface of the lens that shapes itscurvature, and (3) the height ℎ𝑏𝑎𝑠𝑒 of the substrate (the cylindrical base) of the lens. The overall lens geometry is determined by thesethree parameters, i.e. the ratio of𝑑𝑏𝑎𝑠𝑒:𝑟:ℎ𝑏𝑎𝑠𝑒. We further definethe width of the image spot underneath the lens as𝐿as shown in Figure 7b.
      	</p>

      	<div class="center">
	  		<img src="images/Fig7-parameters.png" width="720" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 7: (a) Basic lens parameters: lens diameter𝑑𝑏𝑎𝑠𝑒, ra-dius𝑟and substrate heightℎ𝑏𝑎𝑠𝑒(b) image spot𝐿: the area underneath the lens that can be seen from one specific view-ing angle, (c) to achieve the same image spot size, a smallerradius (i.e., more curvy lens), requires a smaller height.
	        </p>
	        <br>
    	</div>

      	<p>
      		The lens geometry determines how many viewpoints each lenscan support. The number of viewpoints is determined by how manyimage spots fit underneath each lens. There are several factors thatdetermine the size of the image spots. First, for a given lens cur-vature, the lens substrate has a specific height at which the imagespot is minimized. The more curvy the lens surface is, the smallerthe substrate height when it achieves its local minimum (Figure 7c).Since the lenses are added to the object’s surface, we want to mini-mize the height so that the lenses do not protrude too far out fromthe object geometry. We thus use the most curvy lens since it resultsin the smallest height and thus least protrusion from the surface. The most curvy lens is created by using the smallest radiusr, whichis𝑑𝑏𝑎𝑠𝑒2.
      	</p>
      	<p>
      		With the radius𝑟determined, we next find the lens substrateheightℎ𝑏𝑎𝑠𝑒that minimizes the width of the image spot𝐿so that thelens can hold the largest possible number of image spots. For this,we use different lens substrate heightsℎ𝑏𝑎𝑠𝑒as input and plot theresulting image spot widths. For a given lens substrate heightℎ𝑏𝑎𝑠𝑒,the image spot width𝐿also depends on the viewing angle, i.e. if theviewer looks straight down onto the lens, the magnification is largerand the image spot smaller than when the viewer looks at it fromthe side. We therefore plot the upper bound of the image spot widthfor each given lens substrate height. The detailed computation isincluded in the Appendix and the result is shown in Figure 8a.We can see that the image spot width𝐿is minimized when thesubstrate heightℎ𝑏𝑎𝑠𝑒=0.5𝑑𝑏𝑎𝑠𝑒with the image spot width being𝐿=0.2𝑑𝑏𝑎𝑠𝑒.
      	</p>
      	<p>
      		In conclusion, the lens geometry that supports the highest num-ber of viewpoints while protruding least from the object surfacehas a lens curvature of𝑟=0.5𝑑𝑏𝑎𝑠𝑒and a substrate height ofℎ𝑏𝑎𝑠𝑒=0.5𝑑𝑏𝑎𝑠𝑒. Therefore, the lens geometry we use has a ratioof𝑑𝑏𝑎𝑠𝑒:𝑟:ℎ𝑏𝑎𝑠𝑒=2 : 1 : 1(Figure 8b).
      	</p>


      	<div class="center">
	  		<img src="images/Fig8-plot.png" width="480" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 8: (a) Finding the base height that minimizes the im-age spot for𝑟=0.5𝑑𝑏𝑎𝑠𝑒; (b) the resulting lens geometry is𝑑𝑏𝑎𝑠𝑒:𝑟:ℎ𝑏𝑎𝑠𝑒=2 : 1 : 1, which results in𝐿=0.2𝑑𝑏𝑎𝑠𝑒.
	        </p>
	        <br>
    	</div>

      	<p>
      		<i>Viewing Range: </i>The lens geometry results in a viewing angle rangeof83.6◦, i.e. the viewer can see the correct image spot when theyare no more than46.8◦=83.6◦2away from the upwards direction ofthe lens. We determined this value by tracing20parallel viewingrays from each viewing angle and determined the viewing anglevalid when all viewing rays from a viewing angle were reflectedonto the backplane where the image spot is supposed to be located(see Appendix).
      	</p>
      	<p>
      		<i>Number of Viewpoints: </i>As mentioned above, the width of an imagespot from any viewing angle is at most𝐿=0.2𝑑𝑏𝑎𝑠𝑒. To determinethe resulting number of viewpoints, we calculated the maximumnumber of image spots that can fit into the lens backplane. Althoughthe exact image spots can have non-circular shapes,𝐿represents theupper bound of the image spot width across all viewing angles. Weused the Wolfram Alpha calculator’s circle packing function [52]and found that19image spots fit into the lens backplane. To verify that we can support19different viewpoints, we created a lens with19different image spots (Figure 9a) and simulated the appearancesthis lens creates from each of the19viewpoints using ray tracing.Figure 9b shows the result, i.e. that the lens is indeed capable ofcreating different appearances from19different viewpoints.
      	</p>

      	<div class="center">
	  		<img src="images/Fig9-19-viewpoints.png" width="480" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 9: (a) Our lens geometry allows packing 19 imagespots onto the backplane of a lens and thus can support19 viewpoints with different images; (b) ray traced resultsof the lens showing 19 different colors from different view-points.
	        </p>
	        <br>
    	</div>


      	<p>
      		To evaluate how many different viewpoints can be achievedconsidering the limitations of current fabrication technology, wefabricated lenses at different angles (facing up,45◦up, sideways,45◦down, down). We printed19lenses per angle with each lenscontaining a different number of viewpoints, ranging from1−19viewpoints. After fabrication, we took pictures from all visibleviewpoints. Our results show that lenses printed facing upwardsand downwards have the highest number of visible viewpoints,i.e. the upward printed lens can show up to19viewpoints andthe downward printed lens can show up to14viewpoints. Otherprint orientations had smaller number of view points (i.e. 45◦up: 12 viewpoints,45◦down: 9 viewpoints, sideways: 7 viewpoints).
      	</p>

      </div>


      <div>
      	<h1 class="medium-headline">IMPLEMENTATION</h1>

      	<p>
      		Our 3D editor is implemented in the 3D modeling softwareRhino3Das aGrasshopperplugin. Our software pipeline first distributes thelenses across the surface of the 3D model. It then maps the pixelsof the different input 3D color textures to the lens positions on the3D model. Next, it computes for each lens how the color pixels ofthe different 3D textures should be arranged on the backplane ofthe lens to show the correct appearance for each viewpoint. Finally,it generates the fabrication files and exports them.
      	</p>

      	<h3 class="medium-headline">Distributing Lenses Across the 3D Model</h3>

      	<p>
      		Our goal when distributing the lenses across the 3D object surfaceis to pack them as closely together as possible while not collidingwith each other. Two lenses are not colliding when the distancebetween the two lens centers is equal or greater than the diameterof the lenses. Since lenses are uniform circles, an efficient wayto pack them is hexagonal packing [54]. Hexagonal packing canbe achieved by first dividing the surface into equilateral trianglesand then placing a lens at each corner of the triangle. Since lenses should not collide, we set the edge length of the equilateral triangleto the lens diameter.
      	</p>
      	<p>
      		To implement this, our system first converts the 3D model into atriangular mesh using the instant meshes open source library [21](Figure 10a/b). Since instant meshes requires an .obj file as input,our system temporarily exports the 3D model from our editor anduses the exported file for the instant meshes conversion. In additionto the 3D model geometry in .obj format, instant meshes requiresseveral parameters: first, instant meshes requires the target facecount, i.e. the number of faces into which the mesh should be con-verted. Our system determines the number of faces by dividing thesurface area of the 3D model by the area of the corresponding equi-lateral triangle. Next, instant meshes requires as input the rotationaland position symmetry. Setting them to a value of ’6’ results in ahexagonal-directional field suitable for triangular packing. Onceinstant meshes finished the conversion, our system imports theconverted triangular mesh back into our design tool. Our systemthen uses the corners of the triangles, which represent all the ver-tices of the mesh, as the centers for the lenses to place the lensesacross the 3D surface (Figure 10c/d).
      	</p>


      	<div class="center">
	  		<img src="images/Fig10-bunny" width="720" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 10: Placing the lenses: (a) original 3D model, (b) trian-gular mesh with equilateral triangles, (c) each triangle cor-ner (vertex) is a location for a lens center, (d) lenses placed.
	        </p>
	        <br>
    	</div>

    	<h3 class="medium-headline">Mapping Color Pixels to Lens Locations</h3>
      	<p>
      		Once our system positioned the lenses across the 3D model’s sur-face, it next maps the color pixels of each texture image to the lenspositions on the 3D model (Figure 11a). To know which texturepixels belongs to which lens, our system uses UV mapping, whichmaps a vertex on a mesh to a 2D coordinate on the texture image.When a 3D object with texture mapping is imported, a list of vertex-to-UV coordinate mappings is already included in the 3D model.For points on the surface of the 3D model that are not vertices, oursystem can further compute their UV-coordinates in the textureimage using their barycentric coordinates, i.e. by finding which facethey are on and then interpolating them as the weighted averageof the vertices on the face.
      	</p>
      	<p>
      		After computing all UV coordinates, our system maps each colorpixel in the 3D texture to a lens. To do this, our system first finds thelens center’s pixel location in the color texture using the UV map-ping. Assuming that the input geometry has an even UV-mapping,i.e. the distance between two points on the texture is proportionalto the geodesic distance between the source points on the object,our system then finds the pixel-radius of each lens on the textureby sampling a point on the circumference of the lens and thenfinding the distance between its corresponding pixel and the lenscenter’s corresponding pixel. Therefore, every pixel that is within the pixel-radius distance to the lens center’s corresponding pixelare mapped to the lens.
      	</p>

      	<div class="center">
	  		<img src="images/Fig11-color.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 11: Mapping color pixels onto lenses for each 3D tex-ture: (a) Find which pixels are underneath which lens foreach texture. (b) For each texture, average the color under-neath each lens. (c) Divide the space underneath each lensusing ray-tracing and assign the averaged color of the corre-sponding texture for each lens.
	        </p>
	        <br>
    	</div>

      	<p>
      		Our system requires that all 3D textures have the same UV map-ping. Thus, once the UV mapping is computed, we can reuse theassignment of color pixels on a 3D texture to a specific lens posi-tion on the object geometry for all viewpoints and their 3D colortextures.
      	</p>


      	<h3 class="medium-headline">Averaging Color Pixels in each Image Spot</h3>

      	<p>
      		Next, our system averages the colors of all pixels in an image spotthat belong to the same viewpoint (Figure 11b). We do not keepthe individual color pixels in an image spot because when theviewer shifts the head slightly, they would see a different colorpixel each time, resulting in unrelated patterns even with slighthead movement. In addition, the image spot is very small due tothe magnifying effect of the lens. Thus, printing all color pixelsfrom the input texture into the small image spot would requirea print resolution beyond what our 3D printer is capable of. Thesmallest pixel size that our 3D printer supports is 200 microns,which translates to only 7 pixels for the image area underneatha 3mm lens. Because of this, our system averages the color of allpixels in an image spot that belong to the same viewpoint.
      	</p>

      	<h3 class="medium-headline">Arranging Image Spots Underneath a LensAccording to the Viewpoint</h3>

      	<p>
      		The previous processing step determined the average color assignedto each lens for each of the input textures. Next, our system arrangesthe average colors from the different input textures underneatheach lens to show the correct color at each viewpoint (Figure 11c).
      	</p>
      	<p>
      		Before our system can distribute the image spots underneaththe lens, it first has to determine which area underneath the lens isvisible from which viewpoint. To accomplish this, our system castsrays from different viewpoints to the top surface of the lens. Whenthe various rays hit different points on the top surface of the lens, they enter the lens at different angles and thus reflect off differentpositions at the bottom of the lens (our system approximates theback of the lens with a flat plane because of the small lens size).Since the color pattern is placed at the bottom of the lens, we knowthat the positions were the rays reflect off the backplane are visiblefrom the specific viewpoint from which the ray was cast. We thusonly keep the color of the texture that should be visible from thisviewpoint and delete all other textures in this area.
      	</p>


      	<h3 class="medium-headline">Exporting the Fabrication Files</h3>
      	<p>
      		Finally, once the lenses are generated and the color pattern is cor-rectly distributed across all lenses, our system exports the fabrica-tion files, i.e. the geometry file (.vrml) with the lenses and the objectgeometry, and the color pattern image (.png) that is referenced inthe .vrml file. To prepare for rendering, our system also generatesa viewpoint file (.txt) that records the 3D coordinates of the view-points defined in the system, which the render environment usesas the locations for the render cameras. Our system renders theexported object using the raytracing plugin LuxCoreRenderer [31]in Blender [5].
      	</p>


      </div>

      <div>
      	<h1 class="medium-headline">APPLICATION SCENARIOS</h1>
      	<p>
      		Being able to design objects that look different from different view-points enables a wide range of novel applications in product designand HCI. In the following section, we highlight several applicationscenarios that focus on using 3D lenticular objects. In the scenarios,the objects demonstrate that our system can produce objects with:(1) different geometric complexities (e.g. single and doubly curvedsurfaces), (2) image complexities (e.g. patterns, symbols and text),and (3) number of distinct images (ranging from 2 to 5).
      	</p>

      	<h3 class="medium-headline">Guiding User’s Body Poses</h3>
      	<p>
      		Our work enables the development of tangible objects that canguide users into placing or holding the object at a specific bodypose. Figure 12 illustrates this at the example of a piece of exerciseequipment, i.e. a kettlebell. This lenticular kettlebell guides the userinto the correct exercise position, i.e. a front raise, by displaying ifit is held at the correct height. The kettlebell displays a downwardsarrow if it is held too high (Figure 12a), an upwards arrow if it isheld too low (Figure 12c), and a check mark when the user holds itat the correct height (Figure 12b). This can prevent the user fromexercising at the incorrect body pose, which has been shown to leadto injuries [12]. The kettlebell is an example of a lenticular objectthat displays symbols (up/down arrow, checkmark) printed with3mm lenses on a doubly curved surface with three different viewingangles (low angle, eye level, high angle). In this example, ratherthan printing the entire kettlebell, we bought a 10lb kettlebell andaugmented it with a 3D printed lenticular shell.
      	</p>

      	<div class="center">
	  		<img src="images/Fig12-kettlebell.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 12: Pose Guidance: this kettlebell guides the user to acorrect front raise pose by displaying arrows when the ket-tlebell is too high or too low and a check mark when thekettlebell is at the right height.
	        </p>
	        <br>
    	</div>


      	<h3 class="medium-headline">Dynamic Message According to User’sPosition</h3>
      	<p>
      		Our work also enables objects that display dynamic message orsurface pattern according to the user’s position. Figure 13 showsthis at the example of a lampshade made for a user’s bedroom thatdisplays "Good Night" when the users lays in bed (i.e. looks at thelamp from the same height), and displays "Good Day" when the user looks at it while sitting up in bed (i.e. looks at the lamp fromabove). This application is an example of how lenticular objectscreated with our system can display text messages (’Good Night’,’Good Day’). The lamp was printed with 3mm lenses on a doublycurved surface with two different viewing angles (same height asthe lamp, above the lamp).
      	</p>

      	<div class="center">
	  		<img src="images/Fig13-lampshade.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 13: This lampshade displays different greeting mes-sages when the users looks at it from different heights (lay-ing down in bed vs. sitting up in bed).
	        </p>
	        <br>
    	</div>


      	<h3 class="medium-headline">Dynamic Visual Effects for Product Design</h3>

      	<p>
      		When a user plays with, passes by or moves around an object, theychange their viewpoint towards the object. We leverage this tocreate dynamic visual effects on products as part of the object’sproduct design. Figure 14 shows this at the example of an earpodcase that transitions between different colors when it changes itsviewing angle with respect to the viewer. This application is anexample that displays a pattern (colorful stripes) printed with 2mmlenses on a surface that is flat on the front and doubly curved in thecorners with five different viewpoints (one for the yellow, green,cyan, purple, magenta stripe patterns). The discrepancy betweenthe fabricated and rendered earpod case in the right-most view-point results from fabrication limitations, i.e. since the earpod casewas printed with 2mm lenses the printing resolution for the colorpattern is not high enough to support the magenta sections.
      	</p>

      	<div class="center">
	  		<img src="images/Fig14-shoe.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 14: This earpod case flickers between five alternat-ing bright-dark stripe patterns of different colors as the userhandles it, creating unique aesthetics effects as part of theobject’s product design.
	        </p>
	        <br>
    	</div>


      	<h3 class="medium-headline">Images Only Seen by Specific Users</h3>
      	<p>
      		Different viewers tend to have different viewing angles on thesame object. We can use this to create appearances that are onlyaccessible by specific users. Figure 15 shows this at the example ofa visual design that is only visible from the user’s viewing angle.When the user looks down on their pair of shoes, the pair of shoes display a ’Cheer Up’ message, however, other people cannot seethe message and instead see a plain shoe design. This applicationis an example of how our system enables displaying a text message.The pair of shoes was printed with 3mm lenses on a doubly curvedsurface with two different viewpoints (regular shoe design [47]from the side and ’cheer up’ message from the top). The entire shoeincluding the lenticular lenses was printed in one piece.
      	</p>

      	<div class="center">
	  		<img src="images/Fig15-shoe.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 15: Visual designs only visible from the user’s view-ing angle: (a) A pair of shoes that shows a supportivemessage to the wearer looking down on the shoes, which(b) other people looking from the side at the shoes cannotsee.
	        </p>
	        <br>
    	</div>

      	<p>
      		In summary, our four application scenarios demonstrate that oursystem can create lenticular objects with different geometric andimage complexities as well as different number of viewpoints. Inthe next section, we will discuss limitations and future work of oursystem.
      	</p>
      </div>

      <div>
      	<h1 class="medium-headline">DISCUSSION AND LIMITATIONS</h1>

      	<p>
      		Our current implementation is subject to several limitations, whichcan be addressed in future work:
      	</p>
      	<p>
      		<i>Impact of Lenses on Object Geometry and Surface Haptics: </i> In ourcurrent work, we place the lenses on top of the surface of the3D model, which extends the geometry by a lens layer of 3mm.While a layer of such lenses may not be an issue for larger objects,smaller objects with thin geometries will substantially increase inthickness. For future work, we plan to integrate the lens substrateinto the object geometry to minimize protrusion from the surface. Inaddition, covering objects with lenses changes the tactile qualities ofthe object. We hope that with future developments in 3D printing, itwill become possible to print lenses that are small enough that theyno longer change the surface haptics. This may also address anotherchallenge, which is that lenses that protrude from the surface cancollide when geometries are highly curved, which can happen inour system but would no longer occur when lenses are smaller.
      	</p>
      	<p>
      		<i>Impact of Non-Uniform UV and UV Seams on Color Pattern: </i>As we ex-plained in the implementation, we assume that the input geometryhas a uniform UV texture mapping. If the UV texture mapping isuniform, the circular lens geometry refers to a circular area on theinput texture, thus the mapping in our system creates the correct result. If the UV mapping is non-uniform, the circular backplanesof the lenses are mapped to non-circular areas on the input tex-ture and thus assuming a circular-mapping may result in lensesbeing assigned to the wrong color pixels. This can be resolved bysampling a large number of points on the circumference of thelens and mapping them to the input texture to accurately representthe non-uniform mapping. Another problem that can occur whenperforming the UV mapping is that the lens center and the sampledpoint on the lens circumference may be on opposite sides of theUV texture when the lens is located on a seam. This causes themapped radius to be much larger than the actual radius. We detectsuch cases by setting an upper bound for the mapped radius andsubsequently do not assign a color. For future work, we plan toperform UV mapping for all points inside the lens’s backplane andnot just the circumference so that all of the points are mapped tothe correct location on the input texture.
      	</p>
      	<p>
      		<i>Showing Optical Limitations in User Interface: </i> Our current user inter-face supports the designer in specifying viewing angles, assigningcolor textures, and generating fabrication files for the lenticularobject. However, our current system relies on the designer to checkin the rendering environment whether the generated result matchestheir specified design. In cases where the defined viewpoints exceedoptical limitations, e.g. a viewpoint is outside the viewing range ofa lens or two viewpoints are too close together, there can be crosstalk between different images. In future iterations, we will extendour user interface to only support those viewing angles that arevalid.
      	</p>
      	<p>
      		<i>Fabricating Lenticular Objects on Consumer Level 3D Printers: </i>While we fabricated our lenticular objects on a high-end Stratasys J55 3Dprinter, it may also be possible to fabricate lenticular objects onlow-cost 3D printers, albeit at the expense of manual assembly. Forinstance, Formlabs 3D printers [10] have been shown to be capableof 3D printing lenses, while the Da Vinci 3D Printer [53]) can printthe color patterns in full color. Thus, by printing the lenses as aseparate shell, it may be possible to fabricate lenticular objects onconsumer level 3D printers.
      	</p>
      	<p>
      		<i>Dynamic Lens Sizes and Other Types of Lenses: </i> In our current system, all lenses on the object’s surface have the same size. Whilethis is sufficient for scenarios that requires an even distribution ofviewpoints, some scenarios might benefit from having lenses ofdifferent sizes located on different parts of the object’s surface. Forfuture work, we plan to improve our algorithm to support dynamiclens sizes based on the required image resolution and number ofviewpoints. In addition, while in this paper, we have focused onspherical lenses, other lens types, such as cylindrical lenses, exist. Abenefit of cylindrical lenses is that they have higher spatial resolu-tion while having the drawback that different viewpoints can onlychange along one direction. Figure 16 shows an application of thisthat we created manually: a minimalist product that only showstext instructions when it is held at eyesight and is otherwise clear.Compared to the text in the shoe application, the text resolution inthis minimalist design example is higher. Since the application onlyneeds to support one direction of movement, cylindrical lenses aresufficient to support this transition.
      	</p>

      	<div class="center">
	  		<img src="images/Fig16-future-work.png" width="540" alt=""/>
	  		<br>
	  		<p style="font-size: 12px; font-weight: 600">
		      	Figure 16: Cylindrical Lenses: A container that shows infor-mation only when needed: (a) the container does not showany text when looking from an upper angle, (b) the con-tainer shows informative texts when held at eye level.
	        </p>
	        <br>
    	</div>
      </div>

      <div>
      	<h1 class="medium-headline">CONLUSION</h1>
      	<p>
      		In this paper, we presented a method to extend lenticular displaysto 3D object geometries, enabling 3D objects to look different fromdifferent viewing angles. We showed how our design tool supportsdesigners in creating 3D lenticular lens displays by enabling themto define viewpoints and assign corresponding color textures. Wealso provided information on the fabrication process and evaluatedthe 3D printing quality of different lens sizes, the print resolution ofthe color pattern, as well as the effects of different post-processingtechniques. We discussed our implementation pipeline that auto-matically computes the lens distribution across the object geometryand assigns the correct color pixels to each lens to achieve the de-sired appearance from each viewing angle. We then demonstratedvarious example applications that highlight different geometricand image complexities as well as different numbers of viewpoints.For future work, we plan to further improve our implementationby integrating the lens substrate into the geometry of the object,improving the UV-mapping algorithm to cover non-uniform UVmappings and seams of the UV textures, and to extend our 3D edi-tor to consider optimal limitations. In addition, we plan to explorethe use of dynamic lens sizes and other types of lenses, such ascylindrical lenses.
      	</p>


      </div>

      <div>
      	<h1 class="medium-headline">ACKNOWLEDGEMENTS</h1>
      	<p>
      		We thank Neil Gershenfeld for granting us access to the StratasysJ55 printer in his lab and Tom Lutz for 3D printing our models onthe 3D printer. This paper is based upon work supported by theNational Science Foundation under Grant No. 1716413.
      	</p>

      </div>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/headerstrap-for-subpage.js"></script>
<!-- lightbox -->
<script type="text/javascript" src="../../js/lightbox.js"></script>

</body>
</html>
