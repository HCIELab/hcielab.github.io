<!DOCTYPE html>
<html>
<head>
	<title>FabO: Integrating Fabrication with a Player's Gameplay in Existing Digital Games</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="http://hcie.csail.mit.edu/images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="https://hcie.csail.mit.edu/css/bootstrap.css" rel="stylesheet">
	<link href="https://hcie.csail.mit.edu/css/custom-style.css" rel="stylesheet">
	<!-- Lightbox -->
	<link href="../../css/lightbox.css" rel="stylesheet">


	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="https://hcie.csail.mit.edu/js/analytics.js"></script>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>
<div class="container" style="padding-top: 100px;">
	<div class="row">
	<!-- Publication details -->
	<div class="col-md-4" style="text-align: left;">
		</br>
		</br>
		<span class="medium-headline">
		Publication
		</span>
		</br>
		</br>
		 <a href="https://dishitaturakhia.com">Dishita Turakhia</a>, Harrison Mitchell Allen, Kayla DesPortes, Stefanie Mueller.
		</br>
		 FabO: Integrating Fabrication with a Player's Gameplay in Existing Digital Games
		</br>
		In Proceedings of
		<a href="https://dis.acm.org/2021/" target="_blank">Designing Interactive Systems &#8217;21</a>.<br>
			</br>
				<a href="https://doi.org/10.1145/3461778.3462128" class="btn btn-doi" alt="doi" target="_blank">DOI</a>
				&nbsp; &nbsp;
				<!-- TODO -->
				<a href="https://www.dropbox.com/s/oqltsfdvqb7pyoa/Adapt2Learn-Turakhia%20et%20al.pdf?dl=0 " class="btn btn-pdf" alt="pdf" target="_blank">PDF</a>
				&nbsp; &nbsp;
				<a href="https://youtu.be/iiMgV_dD2jo" class="btn btn-vdo" alt="video" target="_blank">Video</a>
				&nbsp; &nbsp;
				<!-- <a href="https://www.youtube.com/watch?v=YHeSYS2eB2M" class="btn btn-vdo" alt="video" target="_blank">Talk</a>
				&nbsp; &nbsp;
				<a href="https://hcie.csail.mit.edu/research/adaptivelearning/adaptivelearning.html" class="btn btn-talk" alt="slide" target="_blank">Slides</a>
				&nbsp; &nbsp; -->
				<!-- TODO -->	
			</br>
			</br>

			<span class="medium-headline">
			Video
			</span>
			</br>
			</br>
			<!-- <iframe width="360" height="203" src="https://www.youtube.com/embed/iiMgV_dD2jo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
			
			<!-- TODO -->
			<!-- <iframe width="360" height="203" src="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<iframe width="325" height="190" src="https://www.youtube.com/embed/T-22KOGFLoQ" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
			</br> -->
			</br>

			<!-- TODO -->
			<!-- <span class="medium-headline">
			Press
			</span>	
			</br>
			</br>
			<ul>
				<li><a href="https://news.mit.edu/2020/better-learning-shape-shifting-objects-1207">MIT News</a></li>
				<li><a href="https://www.digitaltrends.com/news/mit-robot-basketball-hoop/">Digital Trends</a></li>
				<li><a href="https://technews.acm.org/">ACM News</a></li>
				<li><a href="https://www.innovationtoronto.com/2020/12/shape-shifting-adaptive-training-tools-could-transform-skills-training-and-sports-training/">Innovation Toronto</a></li>
				<li><a href="https://www.sciencewiki.com/articles/better-learning-with-shape-shifting-objects-mit-researchers-have">Science Wiki</a></li>
				<li><a href="https://techxplore.com/news/2020-12-shape-shifting.html">TechXplore</a></li>
				<li><a href="https://interestingengineering.com/mit-develops-shape-shifting-basketball-hoop-for-better-training">Interesting Engineering</a></li>
				<li><a href="https://newatlas.com/good-thinking/adaptive-basketball-hoop-smaller-higher/">News Atlas</a></li>
				<li><a href="https://northernterritoryonlinenews.com.au/better-learning-with-shape-shifting-objects-tech-xplore/">Northern Territory Technology News (Australia)</a></li>
				<li><a href="https://news8plus.com/better-learning-with-shape-shifting-objects/">News8Plus</a></li>

			</ul>
			</br> -->
			<!-- TODO -->

			<!-- TODO -->
			<!-- <span class="medium-headline">
			DIS Talk Video
			</span>
			</br>
			</br>
			<iframe width="360" height="203" src="https://www.youtube.com/embed/YHeSYS2eB2M" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</br>
			</br> -->
			<!-- TODO -->


			<!-- TODO -->
			<!-- <span class="medium-headline">
			Slides
			</span>
			</br>
			</br> -->
			<!-- TODO -->


			<!-- TODO -->
			<!-- <img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide01.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(1)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide02.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(2)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide03.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(3)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide04.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(4)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide05.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(5)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide06.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(6)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide07.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(7)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide08.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(8)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide09.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(9)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide10.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(10)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide11.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(11)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide12.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(12)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide13.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(13)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide14.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(14)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide15.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(15)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide16.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(16)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide17.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(17)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide18.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(18)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide19.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(19)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide20.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(20)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide21.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(21)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide22.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(22)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide23.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(23)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide24.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(24)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide25.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(25)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide26.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(26)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide27.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(27)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide28.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(28)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide29.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(29)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide30.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(30)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide31.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(31)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide32.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(32)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide33.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(33)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide34.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(34)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide35.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(35)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide36.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(36)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide37.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(37)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide38.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(38)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide39.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(39)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide40.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(40)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide41.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(41)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide42.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(42)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide43.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(43)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide44.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(44)" class="hover-shadow"/> -->
			
			</div>


		<!-- For slide show -->
			<!-- close button on tne right corner -->

			<div id="lightbox-modal" class="lb-modal">
				<div class="modal-content">
				
			<div class="lb-slides">
			<div class="numbertext">1 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide01.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">2 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide02.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">3 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide03.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">4 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide04.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">5 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide05.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">6 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide06.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">7 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide07.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">8 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide08.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">9 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide09.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">10 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide10.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>


			<div class="lb-slides">
			<div class="numbertext">11 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide11.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">12 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide12.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">13 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide13.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">14 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide14.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">15 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide15.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">16 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide16.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">17 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide17.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">18 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide18.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">19 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide19.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">20 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide20.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>


			<div class="lb-slides">
			<div class="numbertext">21 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide21.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">22 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide22.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">23 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide23.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">24 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide24.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">25 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide25.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">26 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide26.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">27 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide27.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">28 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide28.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">29 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide29.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">30 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide30.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>


			<div class="lb-slides">
			<div class="numbertext">31 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide31.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">32 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide32.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">33 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide33.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">34 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide34.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">35 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide35.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">36 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide36.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">37 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide37.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">38 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide38.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">39 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide39.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">40 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide40.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			
			<div class="lb-slides">
			<div class="numbertext">41 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide41.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">42 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide42.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">43 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide43.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">44 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide44.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			

					
			<!-- Next/previous controls -->

			<a class="close-button" onclick="closeModal()">&times;</a>
			<a class="prev" onclick="plusSlides(-1)">&#10094;</a>
			<a class="next" onclick="plusSlides(1)">&#10095;</a>
		</div>
	</div>





	<!-- Project information -->
	<div class="col-md-8" style="text-align: left;">
		<br>
		<h3 class="headline">
		<b>FabO:</b> Integrating Fabrication with a Player's Gameplay in Existing Digital Games
		</h3>
		<br>
		<!-- <img src="images/Mini_Vid-Intro.gif" alt="mosculpt-runner" width="240" style="padding-bottom:0px; margin-bottom:10px"/> -->
		<img src="images/fig1-intro.png" alt="mosculpt-runner" width="750" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 1. FabO allows designers to integrate fabrication with existing digital games. When players play these integrated games, FabO generates fabrication files of objects from their gameplay that can be used as (a) collectibles, such as a Pokemon from the game Pokemon Lets Go, or (b) custom game controllers, such as a sword-shaped controller from the Legend of Zelda game. (c) Examples of fabricated objects from our user study, wherein the participants integrated fabrication with existing games of their choice.
		</b>
		<br>
		<br>
		Fabricating objects from a player's gameplay, for example, collectibles of valuable game items, or custom game controllers shaped from game objects, expands ways to engage with digital games. Researchers currently create such integrated fabrication games from scratch, which is time-consuming and misses the potential of integrating fabrication with the myriad existing games. Integrating fabrication with the real-time gameplay of existing games, however, is challenging without access to the source files.
		<br>
		<br>
		To address this challenge, we present a framework that uses on-screen visual content to integrate fabrication with existing digital games. To implement this framework, we built the FabO toolkit, in which (1) designers use the FabO designer interface to choose the gameplay moments for fabrication and tag the associated on-screen visual cues; (2) players then use the FabO player interface which monitors their gameplay, identifies these cues and auto-generates the fabrication files for the game objects. Results from our two user studies show that FabO supported in integrating fabrication with diverse games while augmenting players' experience. We discuss insights from our studies on choosing suitable on-screen visual content and gameplay moments for seamless integration of fabrication. 
		<br>
		<br>
		<span class="medium-headline">
		Introduction
		</span>
		<br>
		<br>
		Fabricating physical objects from a player's digital gameplay expands a player's engagement with the game. Creative examples of integrating fabrication with gameplay include fabricating collectibles of valuable game items to augment the game's digital assets, custom game controllers shaped as game objects to improve the player's interactive experience, and fabricating game objects at chosen gameplay moments to learn fabrication skills.
		<br>
		<br>
		One way to currently integrate fabrication with gameplay is by creating such games from scratch. Because creating games from scratch can be time-consuming, researchers have proposed modifying existing games with added functionalities. However, modifying existing games requires access to the games' source files. Thus, while these approaches allow for tight integration of fabrication with games, they do not generalize across the large pool of existing games. 
		<br>
		<br>
		To tap the potential of myriad existing games for fabrication, we present a framework that uses on-screen content instead of accessing source files to integrate fabrication with the games. In this framework, designers choose significant gameplay moments and tag the on-screen visual cues using our system. When players play this game, our system monitors their gameplay and scans for the tagged cues. Once identified, our system extracts objects from on-screen visual content and generates fabrication files. In this way, our framework allows designers to integrate fabrication with existing games without accessing the source files that contain information on the player's gameplay, and the asset repository that contains information for generating the fabrication files. 
		<br>
		<br>
		To implement our framework with the above workflow, we developed the FabO toolkit, which consists of a designer interface and a player interface. Consider the example of integrating fabrication with the game Pokemon Lets Go in which every time players capture a Pokemon,  they can fabricate a collectible of that Pokemon. To embed such a fabrication event, designers use the FabO designer interface to tag the on-screen text `You have encountered a' because the same text appears on-screen every time players capture a Pokemon. When players play the game, the FabO player interface monitors their screen, identifies the tagged text cue, and auto-generates the fabrication files for the captured Pokemons using object extraction. Players can fabricate their collectible, for example, a Pikachu memento shown in Figure 1a. 
		<br>
		<br>
		We ran two user studies to evaluate (1) the performance and usability of the FabO toolkit for integrating fabrication with various existing games, and (2) the experience of fabricating objects from gameplay. In the first study, 12 participants used the FabO designer interface to integrate fabrication events within the games of their choice. In the second study, 12 participants played a collectible game with integrated fabrication events and used the FabO player interface to fabricate objects from their gameplay. Our user studies' results and the participants' feedback from the post-study interviews show that FabO can successfully generalize across various games. Based on our studies' findings, we discuss insights on how to choose suitable on-screen visual content and gameplay moments for integrating fabrication with existing games. 
		<br>
		<br>
		
		In summary, we contribute:

		<ul>
			<li>A framework to augment existing games by allowing fabrication of objects from the gameplay using on-screen visual content.  </li>
			
			<li>A toolkit that implements our framework through a designer interface for tagging on-screen visual content and a player interface for extracting fabrication files from on-screen visual content. </li>

			<li>Insights from our two user studies, on choosing the suitable on-screen visual content and gameplay moments for successful integration of fabrication with existing games.  </li>
			
		</ul>
		Below, we discuss the FabO toolkit and the user studies.
		<br>
		<br>


		<span class="medium-headline">
		FABO
		</span>
		<br>
		<br>
		In this section, we first explain our framework and how it uses on-screen visual content to support integrating fabrication with existing games. We then demonstrate the implementation of this framework via the FabO toolkit.
		<br>
		<br>
		<b>Framework</b><br>
		We developed a framework that allows designers to use on-screen visual content to integrate fabrication with games, and the players to fabricate objects from their gameplay. As shown in Figure ~\ref{fig:framework}, our framework takes existing games as input and outputs fabrication files of game-objects. To achieve this, designers use our system to choose gameplay moments as fabrication events by tagging visual content as cues, such as text or images. Designers can also tag on-screen regions to extract game objects for fabrication. Using our system, the designers then export all the fabrication events in a single file. Players load this events file while playing the game. Our system then monitors their gameplay, searches for cues of the tagged events and outputs the fabrication files of the game objects.
		<br>
		<br>
		<img src="images/fig2-framework.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 2. FabO's framework uses on-screen visual content to (1) allow designers to integrate fabrication with existing games and (2) auto-generate the fabrication files to allow players to fabricate objects from their gameplay.
		</b>
		<br>
		<br>
		<i><b>Extracting information on player's gameplay without access to the source code:</b></i><br>
		Information on a player's gameplay, such as when they encounter significant moments in the game, can be extracted from the on-screen visual content via computer vision. Such moments are typically accompanied by visual content, such as a congratulatory message or image. For example, in the game \textit{Pokemon Lets Go}, when players capture a Pikachu, the text message \textit{`You have caught a Pikachu'} appears on the screen. Using computer vision to monitor and match such cues allows us to extract information that the player acquired a game object in their gameplay. Furthermore, visual content of such gameplay moments can be easily sourced from online recorded gameplay videos or by recording their own gameplay.
		<br>
		<br>
		<i><b>Generating fabrication files from the gameplay without access to the game's assets:</b></i><br>
		Fabrication files can be generated during the gameplay by using object extraction on the on-screen visual content via computer vision. For example, by extracting the outline of the image of Pikachu that appears on-screen in the game \textit{Pokemon Lets Go}, we can generate its SVG file during a player's gameplay. The players can use this fabrication file to laser cut a Pikachu collectible. Alternatively, the players can mark on-screen objects for generating their fabrication files. If the object intended for fabrication is unavailable on-screen, our system allows linking a custom fabrication file that is released when players encounter that fabrication event.
		<br>
		<br>	
		<b>FabO Toolkit</b><br>
		We developed a framework that allows designers to use on-screen visual content to integrate fabrication with games, and the players to fabricate objects from their gameplay. As shown in Figure ~\ref{fig:framework}, our framework takes existing games as input and outputs fabrication files of game-objects. To achieve this, designers use our system to choose gameplay moments as fabrication events by tagging visual content as cues, such as text or images. Designers can also tag on-screen regions to extract game objects for fabrication. Using our system, the designers then export all the fabrication events in a single file. Players load this events file while playing the game. Our system then monitors their gameplay, searches for cues of the tagged events and outputs the fabrication files of the game objects.
		<br>
		<br>
		<i><b>Designer Interface:</b></i><br>
		The first step for designers is to select an existing game, for example, we selected \textit{Pokemon Lets Go}. The next step is to choose the gameplay moments to integrate with fabrication. We choose the moments when the players capture Pokemons as the fabrication events so they can fabricate a collectible of their Pokemons.
		<br>
		<br>
		<img src="images/fig3-designer-ui.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<br>
		<b>
		Figure 3. To integrate fabrication events, designers use FabO's designer interface to (a) capture the screenshot of the gameplay moment (b)~tag the on-screen visual content as cues, such as text or images, (c) set event properties, and (d) choose option for object fabrication.
		</b>
		<br>
		<br>
		To locate such gameplay moments, designers can either use videos of recorded gameplay from video platforms, such as Youtube, or play the game themselves. In our case, we located the Pokemon capturing moment from a gameplay video sourced from Youtube. Designers then use the `take a new screenshot' feature to import the chosen gameplay moments into \textit{FabO's designer interface}. For instance, Figure \ref{designer-ui} shows the screenshot of our chosen gameplay moment of a player capturing a Pikachu imported into \textit{FabO's designer interface}.
		<br>
		<br>
		The next step is for designers to tag on-screen visual cues, such as text or images associated with that gameplay moment. For instance, Figure \ref{designer-ui}b shows our tagged text cue of `You have encountered a '. We tag this cue because it allows us to create fabrication events for any Pokemon and not just a Pikachu. Alternatively, to limit our fabrication to only Pikachu, we can tag the image of Pikachu's character as a cue. Based on their preference, designers can set if the fabrication event should trigger just once or repeat every time the player encounters it. In our case, we choose to repeat the event to allow for the fabrication of Pokemons every time a player catches one (Figure \ref{designer-ui}c). 
		<br>
		<br>
		The next step for designers is to define the object for fabrication by choosing one of the three options from the designer interface: (1) specifying the on-screen area where the object appears, (2) providing a custom object's fabrication files, or (3) letting the players determine the object from the visual scene. We specify the on-screen area where Pikachu appears as it allows us to generalize for all the Pokemons that appear in that area (Figure \ref{designer-ui}d). We then save this fabrication event with its properties. The event can be modified later if needed.
		<br>
		<br>
		After creating all the events, designers finally export the `Fabrication Events' file which references all the fabrication events with their screenshots, the tagged events, marked regions, and choice of object's fabrication.
		<br>
		<br>
		<i><b>Player Interface:</b></i><br>
		Before playing the respective game, players load the exported `Fabrication Events' file into the player interface. While playing, the player interface monitors a player's screen, scans for tagged cues, and identifies fabrication events. Once identified, the player interface notifies players with a prompt message. At this point, players can either continue playing or pause the game to fabricate the object from the fabrication event using the player interface (Figure 4a). Based on the setting for the object's fabrication, the player interface either (1) automatically extracts the object's outline from the on-screen region, or (2) loads the pre-linked external fabrication file, or (3) allows players to choose an object for fabrication by marking an area on-screen.  
		<br>
		<br>
		<img src="images/fig4-player-ui.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<br>
		<b>
		Figure 4. FabO's player interface monitors players' screens during the gameplay, and when they encounter a fabrication event, (a) it extracts the tagged object for fabrication from the screenshot of the event (b) allows players to refine the extracted outline and generate fabrication files that (c) players can use with laser cutters and paper cutters to fabricate the object, for example a collectible of Pikachu from the game Pokemon Lets Go.
		</b>
		<br>
		<br>	
		<!-- <i><b>Additional Features:</b></i><br>
		In addition to the above workflow, the \textit{FabO} toolkit also supports functionalities, such as sequencing the fabrication events, previewing the frequency of events, and referencing game-controller outlines within fabrication files.
		<br>
		<br>
		Designers can decide the sequence of fabrication events, such that only when a certain fabrication event has occurred, the subsequent events are unlocked. This feature allows designers to impose linearity in the fabrication of objects during the gameplay (Figure \ref{features}a). To support designers with estimating the frequency of their embedded events, the preview feature allows them to check when and how often the embedded events occur in a gameplay video by scanning the source video for tagged cues and highlighting them on the video timeline (Figure \ref{features}b). To expand the use of objects extracted from the games that use game-controllers, the player interface has a library of outlines of standard game controllers that players can overlay on their fabrication files. For example, an outline of the game controller Nintendo Switch can be combined with the outline of a sword extracted from an on-screen game object to make a personalized sword-shaped game controller for the game \textit{The Legend of Zelda} (Figure \ref{features}c).
		<br>
		<br>
		<img src="images/fig5-additional-features.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<br> -->

		<span class="medium-headline">
		USER STUDY 1 - Evaluating FabO for Integrating Fabrication with Existing Games
		</span>
		<br>
		<br>
		In the first user study, we examined \textit{FabO's} workflow and user's experience for integrating fabrication events within various existing digital games. Insights from the study allowed us to determine how designers can choose  (1) suitable visual content for \textit{FabO's} workflow and (2) suitable gameplay moments for seamless integration of fabrication within the gameplay. 
		<br>
		<br>
		<b><i>Study Design</i></b><br>
		We recruited 12 participants from our institution (6f, 5m, 1n/b) aged between 20-29 years (M=24, STD.=2.82) and with varied experience of playing digital games (10+ yrs to never playing games). We conducted the 60min study remotely over a video call (Zoom). The participants used the \textit{FabO} toolkit on our computer via Zoom's remote control.
		<br>
		<br>
		Before the study, we asked the participants to choose up to 3 existing digital games, gameplay moments within those games to embed fabrication events, and associated game objects for fabrication. They could source these gameplay moments either from their own gameplay or from online videos. During the study, we first demonstrated the \textit{FabO} workflow using the game \textit{Pokemon Lets Go}. The participants then used the \textit{FabO designer interface} and their sourced videos to tag as many gameplay moments and associated objects for fabrication as they preferred using text and image cues. They then tested if the \textit{FabO player interface} successfully detected their embedded fabrication events. Finally, we gathered their feedback through semi-structured interviews and a post-study feedback form.
		<br>
		<br>
		<b><i>Study Results</i></b><br>
		Altogether, the 12 participants attempted to integrate fabrication with 35 existing digital games (2-3 games per participant) across 9 genres by tagging 47 events (1-2 events per game). We tested the success of the fabrication events across three conditions: (1)~were the participants able to tag on-screen visual content of their chosen gameplay moment, (2)~did \textit{FabO} identify those moments by scanning for the visual cues, and (3)~did \textit{FabO} generate a fabrication file of a game object for laser cutting. If all three conditions were met, we counted an event as a successful fabrication event. 
		<br>
		<br>
		<img src="images/fig7-us1-results.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 5. (a) User study participants tested 35 games (24 successful) across 9 genres. (b) Fabricated game-objects using \textit{FabO}.
		</b>
		<br>
		<br>
		From the 35 games attempted shown in Figure~\ref{fig:us1-results}a: (1) the participants were able to tag on-screen content for 33 games (94.29\%). In 2 games, they struggled to identify a discrete moment to tag a text or image cue for integration. (2) Within the 33 tagged games, participants tagged 47 events - 15 using text cues and 32 using image cues. Of these 47 tagged events, \textit{FabO} detected 35 events (74.47\%) - 12/15 text cues (80\%) and 23/32 image cues (71.19\%). In total 24 games had successfully detected events. (3) For the 35 detected events, \textit{FabO} successfully auto-extracted the fabrication files for all game objects as marked by the participants. Thus, in total, the participants successfully integrated fabrication in 24 out of 35 games (68.57\%) across all three conditions. Figure~\ref{fig:us1-results}b shows the objects that we fabricated from the generated files. These objects ranged from commemorative trophies and collectibles to supportive gameplay tools, such as maps.
		<br>
		<br>
		<b><strong>Study Insights</strong></b><br>
		We studied the successful and failed examples of fabrication events from the study to gain the following insights on choosing suitable visual content and gameplay moments:
		<br>
		<br>
		<b><i>#1 Choosing suitable visual content:</i></b><br>
		When tagging an event, the designer has to find a text or image cue on screen that indicates that the event occurred. While most games offer such \textit{discrete} cues through text messages or images, some games are \textit{continuous} and do not contain such cues. An example of a game with a discrete cue is the game \textit{Prof. Layton} [p2] (Figure \ref{fig:insight-vis}a), in which a text message appears on-screen when players acquire a coin, thus indicating that the event occurred. However, in the 2 games for which participants failed to integrate fabrication events, i.e., \textit{Unrailed} [p4] (Figure \ref{fig:insight-vis}a) and \textit{Parkitect} [p5], the gameplay was continuous with no discrete cues to indicate event occurrence, i.e., the players built a track and a park continuously, thereby making it difficult to select a discrete moment.
		<br>
		<br>
		Another important consideration in choosing the visual content is to select cues detectable using computer vision, i.e., extractable font and images with a high contrast background (Figure \ref{fig:insight-vis}b, c). If the font was too thick, artistic or low-res (Figure \ref{fig:insight-vis}b-bottom), the text extraction was faulty. Similarly, if the background was too noisy (Figure \ref{fig:insight-vis}c-bottom), the image cue detection was slow and faulty. To increase the detection speed, some participants used a smaller area with less background noise for monitoring and \textit{FabO} successfully detected the events. 
		<br>
		<br>
		Finally, to fabricate objects from visual content, it is essential to have them present on-screen at the moment selected for fabrication. When the objects were not visually present on-screen, participants linked external files with the event. However, one participant [p6] addressed this constraint by using \textit{FabO's} sequencing feature for the game \textit{Final Fantasy}, by using one fabrication event to trigger another event, wherein the fabrication object was on-screen.
		<br>
		<br>
		<!-- <img src="images/Mini_Vid-Sensor-switch.gif" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/Mini_Vid-Sensor-piezo.gif" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/> -->
		<img src="images/fig3-basketball-ui.png" alt="mosculpt-runner" width="750" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 4. Configuring the learning algorithm using Adapt2Learn's user interface by registering sensors and mapping sensor values onto success/failure states and corresponding scores.
		</b>
		<br>
		<br>
		<b><i>Step 1: Register Sensors:</i></b><br>
		We start by hitting the <i>'create new adaptive tool'</i> button and proceed to register the sensors. In the user interface, we can select among a range of different sensors, such as piezo, switch, ultrasonic, flex, accelerometer, force resistive, PIR motion, and hall sensors. For our purposes, we select the <i>'piezo'</i> sensor, label it <i>'board_sensor'</i>, and assign it pin 13 on our microcontroller (Figure 4a). We use the <i>'add another sensor'</i> button and repeat the procedure. We select the <i>'switch'</i> from the available sensors and label it as <i>'basket_sensor'</i> and assign it pin 10 on our microcontroller (Figure 4b).
		<br>
		<br>
		<b><i>Step 2: Map Sensor Values onto Success/Failure States:</i></b><br>
		Next, we configure the sensor states and the respective threshold values that define successful and unsuccessful performance. Depending on the sensors chosen, the user interface provides the respective range of sensor values. For example, for the piezo sensor, the user interface provides a range of values (0-255) whereas for the switch sensor, it provides only boolean values.
		<br>
		<br>
		<b><i>Step 3: Register Actuators:</i></b><br>
		Next, we register the actuators. In the user interface, we can select among a range of different actuators, such as a servo motor, stepper motor, pneumatic pump, and relay. Depending on the actuators chosen, the user interface provides the respective range of actuator values. For our purposes of raising and lowering the hoop height, we select <i>'stepper motor'</i> from the list of actuators and label it <i>'stand_motor'</i>. We assign it pin 1 on our microcontroller, and the user interface automatically assigns the remaining pins 2, 3, and 4 required for the stepper motor (Figure 4c).	We then add the motor that widens and tightens the hoop by clicking <i>'add another actuator'</i> button. We then select <i>'servo motor'</i> and label it <i>'hoop_motor'</i>, and then assign it pin 9 on the microcontroller (Figure 4d). 
		<br>
		<!-- <br>
		
		<img src="images/Mini_Vid-Actuator_stepper.gif" alt="mosculpt-runner" width="150" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/Mini_Vid-Actuator_servo.gif" alt="mosculpt-runner" width="150" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/fig3-basketball-ui.png" alt="mosculpt-runner" width="750" style="padding-bottom:0px; margin-bottom:10px"/>

		<br> -->
		<!-- <b>
		Figure 5. Configuring the learning algorithm using Adapt2Learn's user interface by registering actuators and mapping actuator values onto success/failure states.
		</b> -->
		<!-- <br> -->
		<br>
		<b><i>Step 4: Map Actuation Values onto Success/Failure States:</i></b><br>
		Next, we map the motors' actuation values onto adaptation states. We define states for the <i>'stand_motor'</i> (stepper motor): <i>'stand\_raise'</i> as <i>'success'</i> and with the motor turning 16 revolutions to increase the hoop height, and <i>'stand_lower'</i> as <i>'failure'</i> and with the motor turning -16 revolutions to decrease the hoop height (Figure 4c). We repeat the process for the hoop motor by defining <i>'hoop_motor'</i> (servo motor): <i>'hoop_tighten'</i> as <i>'success'</i> with the motor turning 8 revolutions, and <i>'hoop_widen'</i> as <i>'failure'</i> with the motor turning -8 revolutions (Figure 4d).
		<br>
		<br>
		<b><i>Step 5: Define Performance Evaluation Unit and Running Average Period:</i></b><br>
		Finally, we set up the performance evaluation unit (Figure 5a). The <i>'evaluation unit'</i> can be either <i>'attempts'</i> or <i>'time'</i>. For our adaptive basketball prototype, we select <i>'attempts'</i> representing attempted throws at the basket. Next, we define the <i>'running average period'</i>, i.e. the period over which the algorithm will evaluate the learner's performance. Depending on which evaluation unit was selected, the running average period is either a number of attempts (after 4 throws in our basketball example) or a time period (after 10 minutes in balancing a bike). We also set the time limit after which, if no sensor value is detected, the attempt is considered as a failure, for example as 10 seconds. 
		<br>
		<br>
		
		<b><strong>Generating the Microcontroller Script According to the Configuration:</strong></b><br>
		After configuring the learning algorithm using the steps described above, designers can hit the <i>'export'</i> button, which automatically generates the microcontroller code (Arduino script in .ino file format). After exporting, designers can then deploy the script onto the microcontroller integrated with their adaptive training tools.
		<br>
		<br>
		<b><strong>Visualization Tool: Displaying Performance and Adaptation:</strong></b><br>
		To provide tool designers with a way to assess the learner's performance and when the tool adapts, we added a visualization tool. The visualization tool plots the learner's attempt scores, the corresponding running average, and the computed derivative of the running average at that attempt. This performance data is plotted in real-time along with when the tool adapts to an easier or more difficult setting. 
		<br>
		<br>
		The visualization helps monitor how the configured learning algorithm takes a learner from a low difficulty setting to a high difficulty setting while maintaining their performance score at the optimal challenge point. 
		<br>
		<br>
		<img src="images/fig5-viztool.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 5. Visualization of the scoring of a learner and the adaptation frequency over a number of attempts. The visualization thus helps monitor how the configured learning algorithm takes a learner from a low difficulty setting to a high difficulty setting while maintaining their performance score at the optimal challenge point.
		</b>
		<br>
		<br>
		

		<span class="medium-headline">
		APPLICATION EXAMPLES OF ADAPTIVE LEARNING TOOLS USING ADAPT2LEARN
		</span>
		<br>
		<br>
		We built and configured an adaptive armband that supports learners in keeping their elbow straight during the golf-swing. The armband has a flex sensor to detect if the learner's elbow is straight or bent, and a pneumatic pump to deflate and inflate the arm band to restrict bending of the elbow 
		<br>
		<br>
		<b>Adaptive Armband for Golf: Single Sensor-Actuator Combination</b>
		<br>
		We built and configured an adaptive armband that supports learners in keeping their elbow straight during the golf-swing. The armband has a flex sensor to detect if the learner's elbow is straight or bent, and a pneumatic pump to deflate and inflate the arm band to restrict bending of the elbow (Figure~\ref{fig:golf}). 
		<br>
		<br>
		<img src="images/fig6-golfa.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/fig6-golfb.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 6. An adaptive armband that supports learners in keeping their elbow straight during the golf-swing, integrated with one flex sensor and  one pneumatic pump. Configuring the learning algorithm for the armband using \textit{Adapt2Learn}'s user interface.
		</b>
		<br>
		<br>
		The configured algorithm then senses the bending and inflates or deflates the armband to provide more or less support to the learner and thus varies the task difficulty. For example, if the learner bends the elbow too often during training, the algorithm makes the task difficulty easier by inflating the armband thereby restricting the bending, and thus providing more support to the learner by keeping the elbow straight.   
		<br>
		<br>

		<b>Adaptive Wobbleboard: Synchronizing Sensors:</b>
		<br>
		We built and configured an adaptive wobbleboard with inflatable cushion that supports learners in learning to balance the board. The wobbleboard has two ultrasonic sensors mounted on diametrically opposite sides of the board to detect if it is stable or wobbling, and a pneumatic pump to deflate and inflate the support cushion that restricts wobbling (Figure~\ref{fig:adapt2learnwobble}). 
		<br>
		<br>
		<img src="images/fig7a-wobbleboard.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/fig7b-wobbleboard.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 7. An adaptive wobbleboard with inflatable cushion that supports learners in learning to balance the board. Configuring the learning algorithm for the wobbleboard using \textit{Adapt2Learn}'s user interface to set synchronized two ultrasonic sensors and one pneumatic pump.
		</b>
		<br>
		<br>
		If the configured algorithm detects a failure state for either of the sensors, it implies that the corresponding edge of the wobbleboard is too high, and the other edge is too low (<5cm), meaning that the wobble board is imbalanced. Thus, two sensors can be used in tandem to detect balancing. To support the learner in keeping the wobbleboard balanced, the pneumatic pump can then inflate the cushion. Alternatively, if the learner balances the wobbleboard well, the pump deflates the cushion, thereby reducing the support and making the task of balancing harder. 
		<br>
		<br>

		<b>Adaptive Bike: Synchronizing Actuators</b>
		<br>
		We built and configured a bike with adaptive training wheels that supports learners in learning to balance the bike. The bike has one hall-effect sensor mounted on each of the training wheels to detect if the training wheel is being used, and one stepper motor on each of the training wheels to lower or raise them to provide more or less support in balancing the bike (Figure~\ref{fig:bike}). 
		<br>
		<br>
		<img src="images/fig8a-bike.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/fig8b-bike.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 8. A bike with adaptive training wheels that supports learners in learning to balance the bike. Configuring the learning algorithm for the adaptive bike using \textit{Adapt2Learn}'s user interface to set two synchronized hall-effect sensors and two synchronized stepper motors.
		</b>
		<br>
		<br>
		If the configured algorithm detects a failure state too often for either of the sensors, it implies that the learner is unable to balance without the use of the training wheels. The stepper motors then lower the wheels further to provide more support to the learner. Since both the actuators are mapped to the same failure state, they both turn at the same time and by the same amount. In this way two sensor values can be mapped to two actuator values in combination.  
		<br>
		<br>

		<b>Adaptive Heels: Synchronized Sensors and Actuators</b>
		<br>
		In addition to the above examples, we also configured the learning algorithm for the studio-built adaptive heels that support learners in training to walk in high heels (Figure \ref{fig:heels}). The participant team mounted two ultrasonic distance sensors per shoe, one on each side of the heel to measure the balance of the learner while walking in the heels. One servo motor was mounted on each of the shoe to raise and lower the heel height while walking. Thus, the adaptive heels had a combination of 4 synchronized sensors and 2 synchronized actuators. This ensures that both the heels were synchronized in their adaptation. Thus only when all the four sensor values detected success states, the servo motors actuated to raise the heels and increased the difficulty of walking. Note that the 3D printed spindle that increased and decreased its height by the servo motor at the base of the heel supported the weight of the learner while walking. 
		<br>
		<br>
		<img src="images/fig9a-heels.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/fig9b-heels.png" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 9. Adaptive heels that support learners in training to walk in high heels. Configuring the learning algorithm for the adaptive heels using \textit{Adapt2Learn}'s user interface to set four synchronized ultrasonic distance sensors (two per shoe), and two synchronized servo motors.
		</b>
		<br>
		<br>
		In the same way that we used our user interface to configure the examples above, the user interface can be used to configure other examples from the studio, such as the adaptive skateboard, dartboard, fencing, jumprope, and cornhole prototypes {(see Figure \ref{fig:classprojects})} that use similar sensor-actuator combinations. 
		<br>
		<br>
		However, we also encountered two challenges for which we could not yet configure the learning algorithm using our user interface. The first challenge occurs when the success state is coupled with a specific timing, such as when hitting a note on time for playing piano. For instance, the adaptive piano used a switch sensor to sense if a key was pressed at the right time and then actuated the servo motor under the key to provide feedback to the learner on which key to press next. Since our user interface does not support time-based sensing, we were not able to configure the learning algorithm for this adaptation. The second challenge occurs when additional processing on the sensor data is needed. For example, both the adaptive pitching machine and the adaptive juggling used a camera to detect the learner's position, which requires computer vision techniques that go beyond the sensor value thresholding that our user interface currently supports. 
		<br>
		<br>
		In summary, we demonstrated the use of \textit{Adapt2Learn} for configuring the learning algorithm for a variety of applications that ranged from single sensor-actuator combinations (e.g., golf-arm band) to multiple synchronized sensor-actuator combinations (e.g., adaptive heels). 

		



		<br>
		<br>
		<span class="medium-headline">
		
		DISCUSSION
		</span>
		<br>
		<br>
		We illustrated how \textit{Adapt2Learn} supports designers in configuring the learning algorithm for their custom adaptive training tools. \textit{Adapt2Learn's} built-in visualization tool then supports designers in assessing the learner's performance and the tool's adaptation. The interface also allows designers to update the learning algorithm without re-programming the microcontroller code. We next discuss the limitations of our toolkit and provide directions for future work:
		<br>
		<br>
		<b><i>Extending the Range of Supported Components:</i></b> Adapt2Learn} currently supports 8 sensors and 4 actuators, which can be used in multiple sensor-actuator combinations, as seen in our examples. However, as discussed earlier, providing more components would further extend the range of adaptive tools for configuring the learning algorithm. For the future, we plan to integrate components that require more processing, such as depth sensors and cameras. Additionally, adding time-based sensing and custom components to the user interface could be a direction for future work.
		<br>
		<br>
		<b><i>Configuring the Algorithm in Real-time:</i></b> While currently, our system provides real-time visualization of the learner's performance and tool's adaptation, it does not allow for real-time reconfiguration of the learning algorithm. The designers currently have to reconfigure the values, re-export the microcontroller script, and then deploy it again onto the adaptive tool. In future work, we plan to support designers to update the configuration of a learning algorithm in real-time in the learning situation.
		<br>
		<br>
		<b><i>Evaluating the toolkit through user studies:</i></b> While we demonstrated that \textit{Adapt2Learn} can be used for configuring various adaptive training prototypes, evaluating the use of toolkit through user studies with designers and testing it in different phases of the design process is a part of our future work. 
		<br>
		<br>
		<b><i>Visualizing the Learning Trajectory:</i></b> While not the focus of our work, the visualization tool may also help assess how long the learner takes to transition from a low difficulty level to a high difficulty level, and predict the time needed to reach the highest skill level. Additionally, the visualization tool may also allow comparing the learning trajectory of multiple learners and gain more insights into that motor skill's learning.
		<br>
		<br>
		<b><i>Comparing Different Tool Designs:</i></b> When building an adaptive training tool, designers have different options for sensing the learner's performance and adapting the task difficulty. For instance, when designing the adaptive basketball, instead of only detecting board and basket hits with a piezo sensor and switch, a camera can be used to sense the ball's trajectory, which provides more information. However, it is unclear which sensing-adapting method leads to the best results. Providing a way to compare the adaptation of different designs for the same training tool could allow designers to choose their designs appropriately.
		<br>
		<br>
		<b><i>Supporting Multiple Learners:</i></b> Many skills involve learning as a group where individuals may have varying skill levels. While currently the exported microcontroller script from our user interface and our visualization tool monitor a single learner's performance, a future direction for research could be to extend both the user interface and the visualization tool to support multiple learners at the same time.
		<br>
		<br>

		<span class="medium-headline">
		CONCLUSION
		</span>
		<br>
		<br>
		We developed a toolkit that supports designers in creating adaptive training tools that maintain the task difficulty at the optimal challenge point. Our formative study showed that designers needed support, particularly in configuring the learning algorithm and assessing the tool's adaptation. We showed that \textit{Adapt2Learn} addressed these two challenges through its user interface and its visualization tool. We showed that \textit{Adapt2Learn's} user interface supports configuring the learning algorithm by first registering the sensors and actuators of the adaptive tools, then mapping their values to success/failure states, and finally exporting the auto-generated micro-controller script, which can be deployed onto the micro-controller integrated with the tools. Furthermore, we showed how \textit{Adapt2Learn's} built-in visualization tool supports designers in assessing if the learning algorithm maintains the task difficulty at the optimal challenge point during training by visualizing the learner's performance and the tool's adaptation. We demonstrated \textit{Adapt2Learn's} use to configure the learning algorithm for five different adaptive tools with various sensor/actuator combinations, such as an adaptive basketball, armband for golf, wobbleboard, bike, and adaptive heels.  
		<br>
		<br>
		

		<span class="medium-headline">
		ACKNOWLEDGMENTS
		</span>
		<br>
		<br>
		We thank the 32 students at MIT, who took the 6.810 course and participated in the study. We also thank Christian De Weck and Or Oppenheimer for their contributon to this project. This work is supported by <a href="https://mitili.mit.edu/">MIT Learning Initiative</a> and the National Science Foundation (Grant No. 2008116).
		<br>
		<br>


		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>


	</div>
	</div>
</div>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/headerstrap-for-subpage.js"></script>
<!-- lightbox -->
<script type="text/javascript" src="../../js/lightbox.js"></script>

</body>
</html>
