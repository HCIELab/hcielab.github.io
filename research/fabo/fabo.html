<!DOCTYPE html>
<html>
<head>
	<title>FabO: Integrating Fabrication with a Player's Gameplay in Existing Digital Games</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="http://hcie.csail.mit.edu/images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="https://hcie.csail.mit.edu/css/bootstrap.css" rel="stylesheet">
	<link href="https://hcie.csail.mit.edu/css/custom-style.css" rel="stylesheet">
	<!-- Lightbox -->
	<link href="../../css/lightbox.css" rel="stylesheet">


	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="https://hcie.csail.mit.edu/js/analytics.js"></script>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>
<div class="container" style="padding-top: 100px;">
	<div class="row">
	<!-- Publication details -->
	<div class="col-md-4" style="text-align: left;">
		</br>
		</br>
		<span class="medium-headline">
		Publication
		</span>
		</br>
		</br>
		 <a href="https://dishitaturakhia.com">Dishita Turakhia</a>, Harrison Mitchell Allen, Kayla DesPortes, Stefanie Mueller.
		</br>
		 FabO: Integrating Fabrication with a Player's Gameplay in Existing Digital Games
		</br>
		In Proceedings of
		<a href="https://cc.acm.org/2021/" target="_blank">Creativity and Cognition &#8217;21</a>.<br>
			</br>
				<a href="https://doi.org/10.1145/3461778.3462128" class="btn btn-doi" alt="doi" target="_blank">DOI</a>
				&nbsp; &nbsp;
				<!-- TODO -->
				<a href="https://www.dropbox.com/s/bhadnl8nckaqthw/C%26C-FabO.pdf?dl=0 " class="btn btn-pdf" alt="pdf" target="_blank">PDF</a>
				&nbsp; &nbsp;
				<a href="https://www.dropbox.com/s/692ur2fzihsdie4/FabO-video-final.mp4?dl=0" class="btn btn-vdo" alt="video" target="_blank">Video</a>
				&nbsp; &nbsp;
				<!-- <a href="https://www.youtube.com/watch?v=YHeSYS2eB2M" class="btn btn-vdo" alt="video" target="_blank">Talk</a>
				&nbsp; &nbsp;
				<a href="https://hcie.csail.mit.edu/research/adaptivelearning/adaptivelearning.html" class="btn btn-talk" alt="slide" target="_blank">Slides</a>
				&nbsp; &nbsp; -->
				<!-- TODO -->	
			</br>
			</br>
			<!-- TODO -->
			<!-- <span class="medium-headline">
			Video
			</span>
			</br>
			</br> -->
			<!-- <iframe width="360" height="203" src="https://www.youtube.com/embed/iiMgV_dD2jo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
			
			
			<!-- <iframe width="360" height="203" src="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<iframe width="325" height="190" src="https://www.youtube.com/embed/T-22KOGFLoQ" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
			</br> -->
			</br>

			<!-- TODO -->
			<!-- <span class="medium-headline">
			Press
			</span>	
			</br>
			</br>
			<ul>
				<li><a href="https://news.mit.edu/2020/better-learning-shape-shifting-objects-1207">MIT News</a></li>
				<li><a href="https://www.digitaltrends.com/news/mit-robot-basketball-hoop/">Digital Trends</a></li>
				<li><a href="https://technews.acm.org/">ACM News</a></li>
				<li><a href="https://www.innovationtoronto.com/2020/12/shape-shifting-adaptive-training-tools-could-transform-skills-training-and-sports-training/">Innovation Toronto</a></li>
				<li><a href="https://www.sciencewiki.com/articles/better-learning-with-shape-shifting-objects-mit-researchers-have">Science Wiki</a></li>
				<li><a href="https://techxplore.com/news/2020-12-shape-shifting.html">TechXplore</a></li>
				<li><a href="https://interestingengineering.com/mit-develops-shape-shifting-basketball-hoop-for-better-training">Interesting Engineering</a></li>
				<li><a href="https://newatlas.com/good-thinking/adaptive-basketball-hoop-smaller-higher/">News Atlas</a></li>
				<li><a href="https://northernterritoryonlinenews.com.au/better-learning-with-shape-shifting-objects-tech-xplore/">Northern Territory Technology News (Australia)</a></li>
				<li><a href="https://news8plus.com/better-learning-with-shape-shifting-objects/">News8Plus</a></li>

			</ul>
			</br> -->
			<!-- TODO -->

			<!-- TODO -->
			<!-- <span class="medium-headline">
			DIS Talk Video
			</span>
			</br>
			</br>
			<iframe width="360" height="203" src="https://www.youtube.com/embed/YHeSYS2eB2M" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</br>
			</br> -->
			<!-- TODO -->


			<!-- TODO -->
			<!-- <span class="medium-headline">
			Slides
			</span>
			</br>
			</br> -->
			<!-- TODO -->


			<!-- TODO -->
			<!-- <img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide01.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(1)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide02.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(2)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide03.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(3)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide04.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(4)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide05.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(5)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide06.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(6)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide07.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(7)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide08.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(8)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide09.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(9)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide10.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(10)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide11.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(11)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide12.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(12)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide13.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(13)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide14.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(14)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide15.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(15)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide16.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(16)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide17.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(17)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide18.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(18)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide19.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(19)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide20.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(20)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide21.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(21)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide22.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(22)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide23.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(23)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide24.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(24)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide25.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(25)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide26.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(26)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide27.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(27)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide28.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(28)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide29.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(29)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide30.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(30)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide31.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(31)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide32.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(32)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide33.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(33)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide34.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(34)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide35.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(35)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide36.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(36)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide37.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(37)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide38.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(38)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide39.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(39)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide40.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(40)" class="hover-shadow"/>

			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide41.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(41)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide42.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(42)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide43.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(43)" class="hover-shadow"/>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide44.jpg" width="100px" style="padding-bottom:0px; margin-bottom:10px" onclick="openModal();currentSlide(44)" class="hover-shadow"/> -->
			
			</div>


		<!-- For slide show -->
			<!-- close button on tne right corner -->

			<div id="lightbox-modal" class="lb-modal">
				<div class="modal-content">
				
			<div class="lb-slides">
			<div class="numbertext">1 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide01.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">2 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide02.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">3 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide03.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">4 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide04.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">5 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide05.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">6 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide06.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">7 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide07.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">8 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide08.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">9 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide09.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">10 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide10.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>


			<div class="lb-slides">
			<div class="numbertext">11 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide11.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">12 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide12.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">13 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide13.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">14 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide14.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">15 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide15.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">16 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide16.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">17 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide17.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">18 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide18.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">19 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide19.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">20 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide20.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>


			<div class="lb-slides">
			<div class="numbertext">21 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide21.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">22 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide22.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">23 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide23.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">24 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide24.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">25 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide25.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">26 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide26.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">27 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide27.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">28 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide28.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">29 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide29.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">30 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide30.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>


			<div class="lb-slides">
			<div class="numbertext">31 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide31.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">32 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide32.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">33 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide33.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">34 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide34.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">35 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide35.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">36 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide36.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">37 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide37.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">38 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide38.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">39 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide39.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			<div class="lb-slides">
			<div class="numbertext">40 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide40.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			
			<div class="lb-slides">
			<div class="numbertext">41 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide41.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">42 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide42.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">43 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide43.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>

			<div class="lb-slides">
			<div class="numbertext">44 / 44</div>
			<img src="http://groups.csail.mit.edu/hcie/files/research-projects/adaptive-physical-tools/TEI-Talk-Slides/Slide44.jpg" style="padding-bottom:0px; margin-bottom:10px; width:100%">
			</div>
			
			

					
			<!-- Next/previous controls -->

			<a class="close-button" onclick="closeModal()">&times;</a>
			<a class="prev" onclick="plusSlides(-1)">&#10094;</a>
			<a class="next" onclick="plusSlides(1)">&#10095;</a>
		</div>
	</div>





	<!-- Project information -->
	<div class="col-md-8" style="text-align: left;">
		<br>
		<h3 class="headline">
		<b>FabO:</b> Integrating Fabrication with a Player's Gameplay in Existing Digital Games
		</h3>
		<br>
		<!-- <img src="images/Mini_Vid-Intro.gif" alt="mosculpt-runner" width="240" style="padding-bottom:0px; margin-bottom:10px"/> -->
		<img src="images/fig1-intro.png" alt="mosculpt-runner" width="750" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 1. FabO allows designers to integrate fabrication with existing digital games. When players play these integrated games, FabO generates fabrication files of objects from their gameplay that can be used as (a) collectibles, such as a Pokemon from the game Pokemon Lets Go, or (b) custom game controllers, such as a sword-shaped controller from the Legend of Zelda game. (c) Examples of fabricated objects from our user study, wherein the participants integrated fabrication with existing games of their choice.
		</b>
		<br>
		<br>
		Fabricating objects from a player's gameplay, for example, collectibles of valuable game items, or custom game controllers shaped from game objects, expands ways to engage with digital games. Researchers currently create such integrated fabrication games from scratch, which is time-consuming and misses the potential of integrating fabrication with the myriad existing games. Integrating fabrication with the real-time gameplay of existing games, however, is challenging without access to the source files.
		<br>
		<br>
		To address this challenge, we present a framework that uses on-screen visual content to integrate fabrication with existing digital games. To implement this framework, we built the FabO toolkit, in which (1) designers use the FabO designer interface to choose the gameplay moments for fabrication and tag the associated on-screen visual cues; (2) players then use the FabO player interface which monitors their gameplay, identifies these cues and auto-generates the fabrication files for the game objects. Results from our two user studies show that FabO supported in integrating fabrication with diverse games while augmenting players' experience. We discuss insights from our studies on choosing suitable on-screen visual content and gameplay moments for seamless integration of fabrication. 
		<br>
		<br>
		<span class="medium-headline">
		Introduction
		</span>
		<br>
		<br>
		Fabricating physical objects from a player's digital gameplay expands a player's engagement with the game. Creative examples of integrating fabrication with gameplay include fabricating collectibles of valuable game items to augment the game's digital assets, custom game controllers shaped as game objects to improve the player's interactive experience, and fabricating game objects at chosen gameplay moments to learn fabrication skills.
		<br>
		<br>
		One way to currently integrate fabrication with gameplay is by creating such games from scratch. Because creating games from scratch can be time-consuming, researchers have proposed modifying existing games with added functionalities. However, modifying existing games requires access to the games' source files. Thus, while these approaches allow for tight integration of fabrication with games, they do not generalize across the large pool of existing games. 
		<br>
		<br>
		To tap the potential of myriad existing games for fabrication, we present a framework that uses on-screen content instead of accessing source files to integrate fabrication with the games. In this framework, designers choose significant gameplay moments and tag the on-screen visual cues using our system. When players play this game, our system monitors their gameplay and scans for the tagged cues. Once identified, our system extracts objects from on-screen visual content and generates fabrication files. In this way, our framework allows designers to integrate fabrication with existing games without accessing the source files that contain information on the player's gameplay, and the asset repository that contains information for generating the fabrication files. 
		<br>
		<br>
		To implement our framework with the above workflow, we developed the FabO toolkit, which consists of a designer interface and a player interface. Consider the example of integrating fabrication with the game Pokemon Lets Go in which every time players capture a Pokemon,  they can fabricate a collectible of that Pokemon. To embed such a fabrication event, designers use the FabO designer interface to tag the on-screen text `You have encountered a' because the same text appears on-screen every time players capture a Pokemon. When players play the game, the FabO player interface monitors their screen, identifies the tagged text cue, and auto-generates the fabrication files for the captured Pokemons using object extraction. Players can fabricate their collectible, for example, a Pikachu memento shown in Figure 1a. 
		<br>
		<br>
		We ran two user studies to evaluate (1) the performance and usability of the FabO toolkit for integrating fabrication with various existing games, and (2) the experience of fabricating objects from gameplay. In the first study, 12 participants used the FabO designer interface to integrate fabrication events within the games of their choice. In the second study, 12 participants played a collectible game with integrated fabrication events and used the FabO player interface to fabricate objects from their gameplay. Our user studies' results and the participants' feedback from the post-study interviews show that FabO can successfully generalize across various games. Based on our studies' findings, we discuss insights on how to choose suitable on-screen visual content and gameplay moments for integrating fabrication with existing games. 
		<br>
		<br>
		
		In summary, we contribute:

		<ul>
			<li>A framework to augment existing games by allowing fabrication of objects from the gameplay using on-screen visual content.  </li>
			
			<li>A toolkit that implements our framework through a designer interface for tagging on-screen visual content and a player interface for extracting fabrication files from on-screen visual content. </li>

			<li>Insights from our two user studies, on choosing the suitable on-screen visual content and gameplay moments for successful integration of fabrication with existing games.  </li>
			
		</ul>
		Below, we discuss the FabO toolkit and the user studies.
		<br>
		<br>


		<span class="medium-headline">
		FABO
		</span>
		<br>
		<br>
		In this section, we first explain our framework and how it uses on-screen visual content to support integrating fabrication with existing games. We then demonstrate the implementation of this framework via the FabO toolkit.
		<br>
		<br>
		<b>Framework</b><br>
		We developed a framework that allows designers to use on-screen visual content to integrate fabrication with games, and the players to fabricate objects from their gameplay. As shown in Figure 2, our framework takes existing games as input and outputs fabrication files of game-objects. To achieve this, designers use our system to choose gameplay moments as fabrication events by tagging visual content as cues, such as text or images. Designers can also tag on-screen regions to extract game objects for fabrication. Using our system, the designers then export all the fabrication events in a single file. Players load this events file while playing the game. Our system then monitors their gameplay, searches for cues of the tagged events and outputs the fabrication files of the game objects.
		<br>
		<br>
		<img src="images/fig2-framework.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 2. FabO's framework uses on-screen visual content to (1) allow designers to integrate fabrication with existing games and (2) auto-generate the fabrication files to allow players to fabricate objects from their gameplay.
		</b>
		<br>
		<br>
		<i><b>Extracting information on player's gameplay without access to the source code:</b></i><br>
		Information on a player's gameplay, such as when they encounter significant moments in the game, can be extracted from the on-screen visual content via computer vision. Such moments are typically accompanied by visual content, such as a congratulatory message or image. For example, in the game Pokemon Lets Go, when players capture a Pikachu, the text message 'You have caught a Pikachu' appears on the screen. Using computer vision to monitor and match such cues allows us to extract information that the player acquired a game object in their gameplay. Furthermore, visual content of such gameplay moments can be easily sourced from online recorded gameplay videos or by recording their own gameplay.
		<br>
		<br>
		<i><b>Generating fabrication files from the gameplay without access to the game's assets:</b></i><br>
		Fabrication files can be generated during the gameplay by using object extraction on the on-screen visual content via computer vision. For example, by extracting the outline of the image of Pikachu that appears on-screen in the game Pokemon Lets Go, we can generate its SVG file during a player's gameplay. The players can use this fabrication file to laser cut a Pikachu collectible. Alternatively, the players can mark on-screen objects for generating their fabrication files. If the object intended for fabrication is unavailable on-screen, our system allows linking a custom fabrication file that is released when players encounter that fabrication event.
		<br>
		<br>	
		<b>FabO Toolkit</b><br>
		We developed a framework that allows designers to use on-screen visual content to integrate fabrication with games, and the players to fabricate objects from their gameplay. As shown in Figure 3, our framework takes existing games as input and outputs fabrication files of game-objects. To achieve this, designers use our system to choose gameplay moments as fabrication events by tagging visual content as cues, such as text or images. Designers can also tag on-screen regions to extract game objects for fabrication. Using our system, the designers then export all the fabrication events in a single file. Players load this events file while playing the game. Our system then monitors their gameplay, searches for cues of the tagged events and outputs the fabrication files of the game objects.
		<br>
		<br>
		<i><b>Designer Interface:</b></i><br>
		The first step for designers is to select an existing game, for example, we selected Pokemon Lets Go. The next step is to choose the gameplay moments to integrate with fabrication. We choose the moments when the players capture Pokemons as the fabrication events so they can fabricate a collectible of their Pokemons.
		<br>
		<br>
		<img src="images/fig3-designer-ui.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<br>
		<b>
		Figure 3. To integrate fabrication events, designers use FabO's designer interface to (a) capture the screenshot of the gameplay moment (b) tag the on-screen visual content as cues, such as text or images, (c) set event properties, and (d) choose option for object fabrication.
		</b>
		<br>
		<br>
		To locate such gameplay moments, designers can either use videos of recorded gameplay from video platforms, such as Youtube, or play the game themselves. In our case, we located the Pokemon capturing moment from a gameplay video sourced from Youtube. Designers then use the 'take a new screenshot' feature to import the chosen gameplay moments into FabO's designer interface. For instance, Figure 3 shows the screenshot of our chosen gameplay moment of a player capturing a Pikachu imported into FabO's designer interface.
		<br>
		<br>
		The next step is for designers to tag on-screen visual cues, such as text or images associated with that gameplay moment. For instance, Figure 5b shows our tagged text cue of 'You have encountered a'. We tag this cue because it allows us to create fabrication events for any Pokemon and not just a Pikachu. Alternatively, to limit our fabrication to only Pikachu, we can tag the image of Pikachu's character as a cue. Based on their preference, designers can set if the fabrication event should trigger just once or repeat every time the player encounters it. In our case, we choose to repeat the event to allow for the fabrication of Pokemons every time a player catches one (Figure 3c). 
		<br>
		<br>
		The next step for designers is to define the object for fabrication by choosing one of the three options from the designer interface: (1) specifying the on-screen area where the object appears, (2) providing a custom object's fabrication files, or (3) letting the players determine the object from the visual scene. We specify the on-screen area where Pikachu appears as it allows us to generalize for all the Pokemons that appear in that area (Figure 5d). We then save this fabrication event with its properties. The event can be modified later if needed.
		<br>
		<br>
		After creating all the events, designers finally export the 'Fabrication Events' file which references all the fabrication events with their screenshots, the tagged events, marked regions, and choice of object's fabrication.
		<br>
		<br>
		<i><b>Player Interface:</b></i><br>
		Before playing the respective game, players load the exported 'Fabrication Events' file into the player interface. While playing, the player interface monitors a player's screen, scans for tagged cues, and identifies fabrication events. Once identified, the player interface notifies players with a prompt message. At this point, players can either continue playing or pause the game to fabricate the object from the fabrication event using the player interface (Figure 4a). Based on the setting for the object's fabrication, the player interface either (1) automatically extracts the object's outline from the on-screen region, or (2) loads the pre-linked external fabrication file, or (3) allows players to choose an object for fabrication by marking an area on-screen.  
		<br>
		<br>
		<img src="images/fig4-player-ui.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<br>
		<b>
		Figure 4. FabO's player interface monitors players' screens during the gameplay, and when they encounter a fabrication event, (a) it extracts the tagged object for fabrication from the screenshot of the event (b) allows players to refine the extracted outline and generate fabrication files that (c) players can use with laser cutters and paper cutters to fabricate the object, for example a collectible of Pikachu from the game Pokemon Lets Go.
		</b>
		<br>
		<br>	
		<i><b>Additional Features:</b></i><br>
		In addition to the above workflow, the FabO toolkit also supports functionalities, such as sequencing the fabrication events, previewing the frequency of events, and referencing game-controller outlines within fabrication files.
		<br>
		<br>
		Designers can decide the sequence of fabrication events, such that only when a certain fabrication event has occurred, the subsequent events are unlocked. This feature allows designers to impose linearity in the fabrication of objects during the gameplay (Figure 5a). To support designers with estimating the frequency of their embedded events, the preview feature allows them to check when and how often the embedded events occur in a gameplay video by scanning the source video for tagged cues and highlighting them on the video timeline (Figure 5b). To expand the use of objects extracted from the games that use game-controllers, the player interface has a library of outlines of standard game controllers that players can overlay on their fabrication files. For example, an outline of the game controller Nintendo Switch can be combined with the outline of a sword extracted from an on-screen game object to make a personalized sword-shaped game controller for the game The Legend of Zelda (Figure 5c).
		<br>
		<br>
		<img src="images/fig5-additional-features.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<br>
		<b>
		Figure 5. FabO allows (a) sequencing of events in a desired order to impose linearity (b) auto-scanning gameplay videos to check the frequency of the embedded events (c) inserting game-controller outlines to fit them within fabricated objects, such as a sword from the game the Legend of Zelda.
		</b>
		<br>
		<br>

		<span class="medium-headline">
		USER STUDY 1 - Evaluating FabO for Integrating Fabrication with Existing Games
		</span>
		<br>
		<br>
		In the first user study, we examined FabO's workflow and user's experience for integrating fabrication events within various existing digital games. Insights from the study allowed us to determine how designers can choose  (1) suitable visual content for FabO's workflow and (2) suitable gameplay moments for seamless integration of fabrication within the gameplay. 
		<br>
		<br>
		<b>Study Design</b><br>
		We recruited 12 participants from our institution (6f, 5m, 1n/b) aged between 20-29 years (M=24, STD.=2.82) and with varied experience of playing digital games (10+ yrs to never playing games). We conducted the 60min study remotely over a video call (Zoom). The participants used the FabO toolkit on our computer via Zoom's remote control.
		<br>
		<br>
		Before the study, we asked the participants to choose up to 3 existing digital games, gameplay moments within those games to embed fabrication events, and associated game objects for fabrication. They could source these gameplay moments either from their own gameplay or from online videos. During the study, we first demonstrated the  FabO workflow using the game Pokemon Lets Go. The participants then used the FabO designer interface and their sourced videos to tag as many gameplay moments and associated objects for fabrication as they preferred using text and image cues. They then tested if the FabO player interface successfully detected their embedded fabrication events. Finally, we gathered their feedback through semi-structured interviews and a post-study feedback form.
		<br>
		<br>
		<b>Study Results</b><br>
		Altogether, the 12 participants attempted to integrate fabrication with 35 existing digital games (2-3 games per participant) across 9 genres by tagging 47 events (1-2 events per game). We tested the success of the fabrication events across three conditions: (1) were the participants able to tag on-screen visual content of their chosen gameplay moment, (2) did FabO identify those moments by scanning for the visual cues, and (3) did FabO generate a fabrication file of a game object for laser cutting. If all three conditions were met, we counted an event as a successful fabrication event. 
		<br>
		<br>
		<img src="images/fig7-us1-results.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 6. (a) User study participants tested 35 games (24 successful) across 9 genres. (b) Fabricated game-objects using FabO.
		</b>
		<br>
		<br>
		From the 35 games attempted shown in Figure 6a: (1) the participants were able to tag on-screen content for 33 games (94.29%). In 2 games, they struggled to identify a discrete moment to tag a text or image cue for integration. (2) Within the 33 tagged games, participants tagged 47 events - 15 using text cues and 32 using image cues. Of these 47 tagged events, FabO detected 35 events (74.47%) - 12/15 text cues (80%) and 23/32 image cues (71.19%). In total 24 games had successfully detected events. (3) For the 35 detected events, FabO successfully auto-extracted the fabrication files for all game objects as marked by the participants. Thus, in total, the participants successfully integrated fabrication in 24 out of 35 games (68.57%) across all three conditions. Figure 6b shows the objects that we fabricated from the generated files. These objects ranged from commemorative trophies and collectibles to supportive gameplay tools, such as maps.
		<br>
		<br>
		<b>Study Insights</b><br>
		We studied the successful and failed examples of fabrication events from the study to gain the following insights on choosing suitable visual content and gameplay moments:
		<br>
		<br>
		<b><i>#1 Choosing suitable visual content:</i></b><br>
		When tagging an event, the designer has to find a text or image cue on screen that indicates that the event occurred. While most games offer such discrete cues through text messages or images, some games are continuous and do not contain such cues. An example of a game with a discrete cue is the game Prof. Layton [p2] (Figure 7a), in which a text message appears on-screen when players acquire a coin, thus indicating that the event occurred. However, in the 2 games for which participants failed to integrate fabrication events, i.e., Unrailed [p4] (Figure 7a) and Parkitect [p5], the gameplay was continuous with no discrete cues to indicate event occurrence, i.e., the players built a track and a park continuously, thereby making it difficult to select a discrete moment.
		<br>
		<br>
		<!-- <img src="images/Mini_Vid-Sensor-switch.gif" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/Mini_Vid-Sensor-piezo.gif" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/> -->
		<img src="images/fig7-insight-visual-content.png" alt="mosculpt-runner" width="750" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 7. Examples from user study where FabO (a) successfully detected the text cues (that had legible text), (b) failed to detect text cues (that were pixelated) (c) onscreen text was tagged as image cue because of the font.
		</b>
		<br>
		<br>
		Another important consideration in choosing the visual content is to select cues detectable using computer vision, i.e., extractable font and images with a high contrast background (Figure 7b, c). If the font was too thick, artistic or low-res (Figure 7b-bottom), the text extraction was faulty. Similarly, if the background was too noisy (Figure 7c-bottom), the image cue detection was slow and faulty. To increase the detection speed, some participants used a smaller area with less background noise for monitoring and FabO successfully detected the events. 
		<br>
		<br>
		Finally, to fabricate objects from visual content, it is essential to have them present on-screen at the moment selected for fabrication. When the objects were not visually present on-screen, participants linked external files with the event. However, one participant [p6] addressed this constraint by using FabO's sequencing feature for the game Final Fantasy, by using one fabrication event to trigger another event, wherein the fabrication object was on-screen.
		<br>
		<br>
		

		<b><i>#2 Choosing gameplay moments suitable for seamlessly integrating with fabrication:</i></b><br>
		When analyzing the successfully embedded fabrication events from the study, we observed that the timing of integration within the gameplay was crucial. We noted that fabrication was integrated either at the start (7/47 events, 15%), during the gameplay when there are natural pauses (31/47 events, 66%), or at the end of the gameplay (9/47 events, 19%) when the player can shift focus to fabrication.
		<br>
		<br>
		<!-- <img src="images/Mini_Vid-Sensor-switch.gif" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/>
		<img src="images/Mini_Vid-Sensor-piezo.gif" alt="mosculpt-runner" width="350" style="padding-bottom:0px; margin-bottom:10px"/> -->
		<img src="images/fig8-insights-moments.png" alt="mosculpt-runner" width="750" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 8. User study #1 examples of gameplay moments when fabrication was integrated at the start, during, or end of the gameplay. [(e): fabrication event, and (o): fabrication object.]
		</b>
		<br>
		<br>
		Examples of embedded fabrication events at the start of the gameplay included moments when the player created new objects, such as a dress (Animal Crossing, Figure 8-1), customized game-objects, such as their skateboard (Tony Hawk, Figure 8-2), or received support objects, such as a map (World of Tanks, Figure 8-3). Examples of embedded fabrication events during the gameplay included moments of natural pauses, either because the game paused the playing or the player paused their gameplay voluntarily. Examples of gameplay pauses included moments, such as unlocking characters (Mario Kart, Figure 8-4) or powers (Gris, Figure 8-6), and destroying characters or objects. Examples of player-based pauses included players updating (Skyrim, Figure 8-7) or accessing their inventory (Minecraft, Figure 8-8), referencing support objects, such as maps, accessing scorecards or stat cards, and socially interacting with game characters (Ori and the Blind Forest, Figure 8-9). Because "these are natural pauses" [p9] when players were not concentrating on playing the game, participants chose these moments as suitable for introducing them to a fabrication activity. Examples of embedded fabrication moments at the end of the gameplay included moments when the players completed building, such as a house (Sims, Figure 8-11), or had won the game (Grand Tourismo Sport, Figure 8-12).  
		<br>
		<br>
		In summary, we observed that participants were mindful of the gameplay timing while integrating fabrication within it, such that it would not distract or interrupt the players while playing. 
		<br>
		<br>
		While these examples cover the various games that the participants explored, our study does not cover the full design space of existing games. Thus, more extended studies are needed to understand how to choose suitable visual content and gameplay moments for integrating fabrication with existing games while also augmenting the player's experience. 
		<br>
		<br>

		<span class="medium-headline">
		USER STUDY 2 - Evaluating the Player Experience during Gameplay
		</span>
		<br>
		<br>
		In the second user study, we examined player's experience of playing an existing digital game integrated with fabrication events, and then fabricating objects from their gameplay using FabO.
		<br>
		<br>
		<b>Study Design</b><br>
		For the study, we selected the game Pokemon Planet because it has a short gameplay and an open-ended story line. Using FabO, we embedded fabrication events that corresponded to when players received either a Pokeball or captured a Pokemon. We recruited 12 new participants from our institution (8f, 4m) aged between 17-28 years (M=22.75, STD.=4) with varied experience of playing games from few times a month to everyday. We conducted the 30min study per participant remotely over Zoom, where they played the game for 15 mins on our computer via Zoom's remote control. We did not brief the participants about the FabO system and simply asked them to play the game as they normally would on their own. When they encountered a fabrication event in the game, FabO notified them with a text prompt. At this point, they could either continue playing or pause the game to fabricate the object. For fabrication, the participants first reviewed the auto-generated fabrication files in the FabO player interface and then fabricated the objects, such as Pokeballs and their captured Pokemons using a remote paper-plotter via Zoom's remote control feature, and watched their objects get fabricated over the video call. We then collected feedback on their experience in a semi-structured interview and a post-study feedback form. 
		<br>
		<br>
		<img src="images/fig10-userstudy2-old.png" alt="mosculpt-runner" width="720" style="padding-bottom:0px; margin-bottom:10px"/>
		<br>
		<b>
		Figure 8. (a) User study participants tested 35 games (24 successful) across 9 genres. (b) Fabricated game-objects using FabO.
		</b>
		<br>
		<br>
		<b>Participant Feedback</b><br>
		<b><i>Fabrication of Objects was Meaningful:</i></b>
		11 out of 12 (91.6%) participants found the ability to have physical versions of digital objects from their gameplay meaningful. For example, p4 said <i>"There are many times during a game where I [have] thought it would be amazing to have a physical version of the equipment"</i> and p7 said <i>"I think it can be nice to build collections and to hold pride about."</i>" However, p8 highlighted the need for closely integrating fabrication - <i>"The main risk of the modified game is for the fabrication event to feel out of place."</i>" Some participants also recommended using the system for educational purposes. For example p12 said <i>"As an educational tool, especially for getting kids excited about fabrication, I can see it being really empowering and engaging while teaching really valuable skills in STEM."</i>
		<br>
		<br>
		<b><i>Choice of Objects for Fabrication:</i></b>
		When asked if they preferred to choose which objects to fabricate and when to fabricate them, 7 out of 12 (58%) participants wanted to choose themselves. For example, p3 said <i>"I would love to see players given the opportunity to design and embed their own events as well - to trade in games"</i>. In contrast, 5 participants preferred the experience designed by someone else because it builds anticipation. For example, p10 said <i>"the anticipation of fabricating pokemon in real life encourages me to keep playing the game to discover new pokemon...so I can make more collectibles. The excitement and anticipation of playing the game and fabrication game items builds on each other."</i> In addition, p4 said <i>"randomizing the fabrication events rather than having them be predictable is fun!"</i>
		<br>
		<br>
		<b><i>Timing of Fabrication Events:</i></b>
		7 out of 12 (58%) participants found the idea of fabricating objects during their gameplay enjoyable. From the other 5 participants, 3 stated that they preferred to fabricate the objects after the gameplay and not while playing the game as it halted their gameplay. For example, p2 said <i>"depending on the pace of gameplay, e.g., on a mission or adventure, it may feel distracting to keep having to switch out of the game to fabricate."</i> and p12 said <i>"perhaps pausing [the] game to make fab files print-ready was a bit intrusive and detracted a bit from gameplay."</i>
		<br>
		<br>
		<b>Study Insights</b><br>
		We thus observed that from the player's perspective, it was important that the fabrication does not hinder the gameplay and is integrated meaningfully for a definite purpose. If integrated well, our study participants' feedback shows that it may increase player's motivation, excitement, and engagement with the game without distracting their gameplay. 
		<br>
		<br>
		
		<span class="medium-headline">
		
		DISCUSSION
		</span>
		<br>
		<br>
		<b><i>Visual Cues Required:</i></b> Because our framework uses on-screen visual content, we cannot extract information from moments that either (1) do not have distinct visual cues or (2) that have non-visual cues, for example sound. To address the first limitation, we can explore if machine learning techniques can be used to automatically identify significant moments and auto-label fabrication objects. For the second limitation, we can expand our system to tag and identify audio cues to include events that may not have distinct visual cues.
		<br>
		<br>
		<b><i>Trade-off in detection speed and object fidelity:</i></b> Because analyzing visual content during the gameplay requires significant computation power, the speed of detection is dependent on the player's screen resolution and the processing power of their computers. While reducing the screen resolution may improve detection speed, it reduces the fabrication file’s fidelity. This trade-off in performance speed and fabrication file’s fidelity can be addressed by using more efficient algorithms for object detection.
		<br>
		<br>
		<b><i>2D fabrication Only:</i></b> Because we use 2D object extraction techniques for generating files, the resulting fabrication files are for 2D fabrication only. However, 3D fabrication, such as 3D printing, can also be integrated with gameplay using our framework by linking custom STL files of 3D objects to the fabrication events. For future versions of our system, we can generate 3D models from 2D visual content by (1) mapping 2D images to 3D models repositories, or (2) reconstructing the 3D geometry from 2D images through advanced graphics techniques, such as multi-view object construction.
		<br>
		<br>
		<b><i>Extending the use of the fabricated objects in games:</i></b> While incorporating the fabricated objects back into the game's mechanics is beyond the scope of our current work, it is an avenue for future work. Toolkits like \textit{Nintendo LABO} already incorporate objects fabricated from 2D materials within games for immersive gameplay. By fabricating tangible objects and configuring them to influence the gameplay can integrate the loop of play and fabrication more tightly.
		<br>
		<br>
		<b><i>Educational and social maker-games:</i></b> The design of our framework also allows for applications in educational and social maker games. For instance, an educator can use the sequencing feature of our toolkit to embed increasingly difficult fabrication activities for their students. Similarly, in a social setting with multiple users, every user can use the designer interface to design unique fabrication events within the game or add to each others' fabrication events. The users can then play each others' unique versions or the combined version, encounter the unique fabrication events and fabricate objects, thereby creating novel social interactions using gaming and fabrication.  
		<br>
		<br>
		

		<span class="medium-headline">
		CONCLUSION
		</span>
		<br>
		<br>
		In conclusion, we showed that fabricating objects from player's gameplay, such as collectibles, can be accomplished using our FabO framework, which allows designers to use on-screen content instead of source files for integration and auto-generation of fabrication files. We implemented our framework in the FabO toolkit and demonstrated FabO's workflow that uses computer vision for tagging on-screen visual cues for embedding events and extracting on-screen objects for fabrication. Through two user studies, we showed that FabO successfully allowed the participants to integrate fabrication with a wide variety of existing games to augment player's experience. We discussed the insights from our studies for choosing suitable on-screen visual content and gameplay moments for seamlessly integrating fabrication with the myriad existing games, thereby tapping their potential to expand players' engagement through fabrication.   


 
		<br>
		<br>
		

		<span class="medium-headline">
		ACKNOWLEDGMENTS
		</span>
		<br>
		<br>
		We thank Supramaya Prasad and Joshua Verdejo for their input in the project. We thank the <a href="https://mitili.mit.edu/">MIT Learning Initiative</a> and the <a href="https://news.mit.edu/2019/mitnano-awards-inaugural-ncsoft-seed-grants-gaming-technologies-0930"> MIT.nano NCSoft </a> innovations in gaming technology initiative for partial funding of this research. This work is also supported by the National Science Foundation under Grant No. 2008116.
		
		<br>
		<br>


		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>
		<br>


	</div>
	</div>
</div>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="http://hcie.csail.mit.edu/images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="https://hcie.csail.mit.edu/js/headerstrap-for-subpage.js"></script>
<!-- lightbox -->
<script type="text/javascript" src="../../js/lightbox.js"></script>

</body>
</html>
