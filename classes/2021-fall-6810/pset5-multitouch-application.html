<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Prism for adding the code snippets-->
	<link href="../../css/prism.css" rel="stylesheet" />

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<script src="../../js/prism.js"></script> 

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2021)</a><br></h>
        <h2 class="headline">Pset 5: Blob Detection & Gesture Recognizer</h2>

       <hr>

In this problem set, you will add blob detection to the grayscale image you generated last time to extract the touch point coordinates for each finger. After this, you will implement a gesture recognizer that can detect different finger gestures on your multi-touch pad and then execute the corresponding functionality.<br><br>

<b>Steps:</b>
<ol>
	<li><a href="#BLOBDETECTION">Implement Blob Detection</a></li>
	<li><a href="#COORDINATES">Drawing Blobs into the Image</a></li>
	<li><a href="#GESTURERECOGNIZER">Impelement Gesture Recognizer</a></li>
	<li><a href="#FINGERGESTURES">Design Customized Finger Gestures</a></li>
</ol><br>

<h3 id="BLOBDETECTION">(1) Implement Blob Detection</h3><br>

In pset4, you created a grayscale image in which brighter pixels correspond to higher signal values. While you can now determine with your eyes where the multi-touch pad was touched, your program should also be able to do this automatically and tell you at which (x,y) coordinate it sees a touch point. To do this, we need to write code that detects the white 'blobs' in the image. This is called 'blob detection'.<br><br> 

<img src="images/pset4/UI-blob-touch-1.png" width="240px"> 
<img src="images/pset4/UI-blob-touch-2.png" width="240px"> 
<img src="images/pset4/UI-blob-touch-3.png" width="240px"> <br><br>

<b>Library for Blob Detection:</b> Luckily, somebody else has already provided a library for blob detection and you can see the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">BlobDetection Reference here</a> for details on the different functions. You already installed that library at the beginning of the last pset and we already imported it into the skeleton code using the following line:<br>

<pre><code class="language-processing">import blobDetection.*;</code></pre><br>

<b>Constructing BlobDetection Object:</b> First, construct a new object of the BlobDetection class the large interpolated grayscale PImage height and width. You can find the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">BlobDetection Reference here</a>.<br><br>

<b>Performing Blob Detection:</b> Next, you can use the BlobDetection instance's <code class="language-processing">computeBlobs()</code> function to detect the blobs.<br><br>

<b>Retrieving Blob Number:</b> Use the BlobDetection instance's <code class="language-processing">getBlobNb()</code> function to retrieve the number of blobs (i.e. fingers) in the image. If you get '0' for your blob number no matter how much you press on the multi-touch pad, read below in 'improving your results' in Section (2) on how to debug this.<br><br>


<h3 id="COORDINATES">(2) Drawing Blobs into the Image</h3><br>

<b>Drawing Blob Centers and Contours Into the Image:</b> Next, we want to extract for each blob: (1) the center of the blob (i.e., center of the touch point), and (2) the contour of blob (i.e., the outline of the touch point) and then draw the touch point center as a dot point, the touch point outline, and the touch point coordinates as (x,y) near the touch point center into our PImage. To retrieve the information from a specific blob, you can use the BlobDetection instance's <code class="language-processing">getBlob()</code> function, which returns a 'Blob' object and then use its instance functions to get its blob center and edges (refer to <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">the 'Blob' class in the documentation here</a>). Note that the contour is provided as a list of edges (i.e. lines).<br><br>

Implement this in the <code class="language-processing">drawBlobsAndEdges()</code> function in the skeleton code:<br><br>

<img src="images/pset4/drawBlobsAndEdges-skeleton-code.png" width="600px"> <br><br>

<b>Testing your Blob Detection:</b> We recommend you test your blob detection with 1 finger, 2 fingers, and 3 fingers touching the multi-touch pad to see if it can reliably determine where the finger is touching. This will become important for the next part of this problem set in which we will build a gesture recognizer.<br><br>

<b>Improving your Results with a Custom Threshold:</b> If your blob detection is not finding all the blobs in your image, consider the following: Before performing blob detection, the library automatically converts your image from a grayscale image into a black/white image. The reason the library does this is because blob detection groups pixels of similar color together to find 'blobs' and that is easier to do when pixels are either white or black and not something in between as would be the case for grayscale. The library automatically picks a 'threshold' value, for instance, it may decide to convert all pixels with a value below '100' to black (i.e. '0') and all pixels above to white (i.e. '255'). If you are not getting any blobs, it may mean that during thresholding all pixels were converted to black and no white pixels are left because the threshold value was set too high. To see if a lower threshold would leave you with more white pixels, you can use the <code class="language-processing">setThreshold()</code> function to determine which brightness levels from 0-255 should be taking into account when searching for blobs. Note that the threshold takes as input a float value from 0-1 so you need to scale your 0-255 values accordingly.<br><br>


You can still use the pre-recorded multi-touch pad data from PSet 4 for testing the blob detection here, in case your multi-touch pad or other hardware does not work properly.<br><br>

<h3 id="GESTURERECOGNIZER">(3) Impelement Gesture Recognizer</h3><br>

Once you have the blob detection working, you can move on to build the gesture recognizer. The goal is to be able to draw with your finger onto your multi-touch pad and have the multi-touch pad recognize what was drawn. For instance, if you swipe from left to right, your code should recognize that you made this gesture and if you draw an 'A' your code should know that an 'A' was drawn.<br><br>

<h4>$1 Unistroke Gesture Recognizer</h4><br>

Luckily, you don't have to implement the gesture recognizer from scratch. Instead, you can use the $1 Unistroke Recognizer, which is available as a Processing library. The $1 Unistroke Gesture Recognizer is part of the so-called $1 Gesture Recognizer family, i.e. a set of different gesture recognizers with different capabilities, such as single stroke gesture recognition, multi-stroke gesture recognition etc. In this Pset, we are going to use the $1 Unistroke Recognizer since it is the only one that is available for Processing as a library.<br><br>

<b>What Gestures can the $1 Unistroke Recognizer Detect?:</b> The 'unistroke' means that the gesture recognizer can only recognize gestures that are made by <i>one finger</i> in <i>one continous stroke</i>. For instance, consider the 2 gestures shown below designed for the letter "A". The left one is written in one continous stroke and thus can be recognized by the $1 Unistroke recognizer. The right one, however, is written in two separate strokes and thus cannot be recognized. Similarly, you can recognize a single finger swipe left/right gesture, but you cannot recognize a zoom gesture with two fingers since that would require combining two separate strokes into one gesture. The $1 family has also has a multi-stroke recognizer but as mentioned previously it is not available as a library for Processing.<br><br>

<img src="images/pset5/drawing-pattern-requirement-1.png" width="300px"> <br><br>

<i>Stroke Orientation:</i> The gesture recognizer ignores the stroke 'orientation'. Consider the example below, which shows the same gesture but executed in a different orientation on the multi-touch pad. The gesture recognizer automatically compensates for the change in orientation and will detect that both gestures are the same.<br><br>

<i>Stroke Direction:</i> While the recognizer ignores stroke orientation, it recognizes stroke direction, i.e. the vector direction of the line along which your finger moves. For example, the two gestures shown below are visually identical on first sight but in the first gesture we start on the top left, while in the second gesture we start on the bottom left. Therefore, although visually the same gesture, the $1 recognizer is able to distinguish between them as two different gestures.<br><br>

 <img src="images/pset5/stroke-orientation-direction.png" width="700px"> <br>

<b>Try the Gesture Recognizer for yourself on the Demo Webpage:</b> Before you move on, try out the interactive demo of the $1 Unistroke Recognizer that you can <a href="http://depts.washington.edu/acelab/proj/dollar/index.html">find here on this website</a>. Scroll down to the 'Demo' section and you see 16 different gestures you can draw into the gray window on the right side. Draw each of the gestures once to get a feel for how different gestures are designed and how well they can be classified. After you drew a gesture, you will see its classified name at the top of the window. You can also make your own gesture by first drawing your custom gesture into the gray drawing window and then giving it a name in the 'Add as example of custom type:' field, then click 'Add'. If you now draw your custom gesture again, it should be correctly classified.<br><br> 

<a href="http://depts.washington.edu/acelab/proj/dollar/index.html"><img src="images/pset5/one-dollar-demo.png" width="500px"></a> <br><br>

<h4>Implementing Gesture Recognition in Processing</h4> <br>

<b>Import $1 Unistroke Recognizer Library into Processing:</b> Now that you know a bit more about what the $1 Unistroke recognizer can and cannot do, you can move on to use the recognizer to detect gestures on your multi-touch pad. Start by importing the '$1 Unistroke Recognizer' library by going to 'Sketch -> Import Library' and finding it through the search bar. Next, import the $1 Unistroke Recognizer library into your Pset Processing code.<br>

<pre><code class="language-processing">import de.voidplus.dollar.*; </code></pre> <br>

<b>Instantiate new Gesture Recognizer:</b> Construct a new object of the one dollar recognizer with:
<pre><code class="language-processing">OneDollar oneDollar = new OneDollar(this);</code></pre> <br>

<b>Turn on Console Plotting:</b> Finally, turn on console plotting for the gesture recognizer using the line below. As soon as the gesture recognizer detects a gesture it will immediately print it on the console and also provide how similar it is to the closest match from the recorded template gestures (e.g. 84% similar).<br>

<pre><code class="language-processing">oneDollar.setVerbose(true); // activates console plotting</code></pre> <br>

<b>Creating a new finger gesture:</b> Next, you will create a new finger gesture. For starters, we will use a 'V' gesture, which is one of the most reliable ones that can be detected. Other reliable gestures are triangle, zigzag, checkmark, and anything that has some unique features. A circle, for instance, is one of the harder gestures to recognize due to its smooth outline that is easily susceptible to noise. To create a new finger gesture, you can use the following function:<br>
<pre><code class="language-processing">oneDollar.learn(String gesture-name, int[] x-y-coordinates); // {x1, y1, x2, y2, x3, y3, x4, y4, ...} </code></pre> 

<i>Recording x/y coordinates for New Finger Gesture:</i> While you could obtain the x-y-coordinates for your gesture template by tracking finger input from your multi-touch pad, we recommend not doing this and instead recommend that you draw the gesture with the mouse on screen and record the x-y coordinates from the mouseDragged() event. The reason for this is that the drawing with the mouse will give you very clear tracking points for creating your template gesture. Later on, when you draw the gesture with your multi-touch pad during actual use, this will be more fault resistant. In contrast, if you record your template gesture on the multi-touch pad, you will already have some noise in the data and it will be harder to match a gesture when there is additional noise during actual use later on. Below you see a recording of all the coordinates for our 'V' gesture that we then feed into the <code class="language-processing">learn()</code> function to add our new gesture.<br><br>

<img src="images/pset5/add-gesture3.png" width="650px"> <br><br>

<b>Connect Finger Gesture to a Callback Function:</b> Now that you have added your custom 'V' finger gesture to the gesture recognizer, you next have to define which function should be called when the gesture is being detected. Such a function is called a 'callback function' and it will be triggered every time the algorithm detects the corresponding finger gesture. You can bind the finger gesture to a callback function via the command below. Note that the call-back-function-name is only the same of the function without parenthesis and parameters.<br>

<pre><code class="language-processing">oneDollar.bind(String gesture-name, String callback-function-name); </code></pre><br>

<b>Implement the Callback Function:</b> Next, you need to implement the callback function. The callback function always has to be in the format as shown below, i.e. contain the same number of parameters and parameter types and return void. The function name and parameter names, however, are up to you. If upon recognizing the gesture, you just want to execute some action, you can simply add that code into the function body. If you want to build a location dependent gesture, you could also check where on the multi-touch pad the gesture started or ended etc.<br>

<pre><code class="language-processing">void foo(String gesture-name, float percentOfSimilarity, int startX, int startY, int centroidX, int centroidY, int endX, int endY){
	// do something when the gesture is detected
 }
</code></pre> <br>

<b>Tracking the finger gestures:</b> Now that you have defined how your gesture looks like and what function should be called when it is detected, you still have to tell your code when it should start looking for the gesture. We want to start tracking gestures when our code detects a 'finger blob'.
Therefore, you need to add the tracking function below in the blob detection method where it determines if a finger was successfully detected. The x/y coordinate in the function are the x/y coordinate of your touch point.<br>

<pre><code class="language-processing">oneDollar.track(int x-coordinate, int y-coordinate);</code></pre>

Note that you don't have to 'stop' tracking. The <code class="language-processing">track()</code> function will only be executed when Processing goes through its loop and detects a finger blob, i.e. knows a finger is present. If no finger has been present for a while, i.e. no new data is fed to the gesture recognizer, the recognizer will automatically determine that the gesture has ended and will provide its best guess at what gesture was executed. <br><br>

<b>Visualize Gesture Detection Result:</b> Finally, once your gesture is detected, your callback function should visualize the detected gesture. You can do this by writing the name of the detected gesture into the Processing UI window as shown in the top left corner of the images below.<br>

<img src="images/pset5/UI-recognizer-A2.png" width="350px">
<img src="images/pset5/UI-recognizer-C2.png" width="350px"> <br><br>

<h3 id="FINGERGESTURES">(4) Design Customized Finger Gestures</h3>

Now that you have mastered the gesture recognizer, let's go ahead and implement 3 different finger gestures that can be recognized by your multi-touch pad!<br><br>

For example, you can implement finger input for recognizing letters (A, C etc.), 2D shapes (triangle, circle etc.), and symbols (check mark etc.). Note that in good gesture design, the gesture should be related to what function is being called. For instance, you cannot draw a 'star' and associate it with a 'smile'. Instead, it would be better to draw a 'half circle', which more closely resembles a 'smile'. Similarly, gestures cannot be too abstract. For instance, you cannot have your finger draw one straight line on the multi-touch pad and the system recognizes it as 'fish'. Instead, there are better one-stroke gestures that could represent a fish outline. <br><br>

To further display the gestures you customized, we ask you to draw out touch points (i.e. center of the blob detected) that's been recognized by your multi-touch pad. For this parts, you should still keep the drawing from blob detection in Section (2), and keep the center points (not the blob contour) on the UI for a while (e.g. 2 seconds) even after you lift your finger from multi-touch pad. So that we can see the input gesture patterns recorded from your multi-touch pad. You can implement this by storing the touch points in a separate array, always draw the array points when it's not empty, and clear the points in array whenever it exceeds the 2 seconds time (recall the <code class="language-processing">millis()</code> and <code class="language-processing">System.nanoTime()</code> functions from <a href="pset4-visualization.html#REDUCENOISE">PSet 4</a>). The image down below shows an example of the UI when the "Z" gesture is recognized. <br>

<img src="images/pset5/UI-recognizer-Z-draw-points.png" width="350px"> <br><br>

<h3>(3) Rotoscope</h3>

Finally, as the last part of this problem set, you will create presentation materials that you can use for your website or other project portfolio to showcase your work.<br><br>

Create a rotoscope of your multi-touch pad. You can find information on <a href="visualization-rotoscope.html">how to do this here</a>. Below are some examples from last year.<br><br>

<img src="images/pset5/roto-amadou.png" width="350px">
<img src="images/pset5/roto-april.png" width="350px">
<img src="images/pset5/roto-emily.png" width="350px">
<img src="images/pset5/roto-olivia.png" width="350px">

<h3>(4) Photos</h3>

Take some photos using a clean backdrop. You can find information on <a href="visualization-rotoscope.html">how to do this here</a>. Below are some examples from last year. Note that last year we also implemented a 3D visualization that you can see in some of the photos below. We took it out this year since you already have enough to do.<br><br>

<img src="images/pset5/photo-olivia.jpg" width="350px">
<img src="images/pset5/photo-olivia2.jpg" width="350px">
<img src="images/pset5/photo-katya.jpg" width="350px">
<img src="images/pset5/photo-will.jpg" width="350px"><br><br>

<h3>(5) Video (optional)</h3>

The video is optional but may be a nice addition to your project portfolio. You can find information on <a href="visualization-video.html">how to do this here</a>. Below are some examples from last year.<br>

 <video width="350" height="260" controls>
  <source src="images/pset5/video-olivia.mp4" type="video/mp4">
</video> 

 <video width="350" height="260" controls>
  <source src="images/pset5/video-katie.mp4" type="video/mp4">
</video> 
<h3>Upload your Processing Code and Video of Gestures</h3>

For grading, please upload the following to your google drive student folder:<br>


Gesture Recognizer
		<ul>
			<li>a drawing on paper that shows your 3 custom finger gestures so we know which gestures you implemented, use the same notation as in the pset to indicate the start point for executing the gesture</li>
			<li>the .pde file of your Processing program</li>
			<li>3 photos showing your Processing UI successfully recognizing the 3 different finger gestures </li>
			<li>a short video showing your Processing UI successfully recognizing the 3 different finger gestures (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>

<h3>Grading</h3>

We will give 20 pts in total:
<ul>
	<li>5 pts: you implemented the blob detection correctly
	<li>5 pts: you drew the touch center, outline, and coordinate into the visualization correctly, and showed that your touch pad can recognize 1, 2, and 3 fingers touching correctly.</li>
	<li>5pts: you implemented 3 different gestures using the learn(), bind(), and track() functions from the $1 Unistroke Recognizer</li>
	<li>5pts: you submitted a drawing showing all three gestures you implemented and show in the video that all three gestures are correctly recognized</li>
	<!-- <li>5pts: the remaining 5pts will be given for creating presentation materials for your multi-touch pad. This will be part of the weekly Friday labs and you will work on this over the next weeks in small steps.
		<ul>
		<li><a href="visualization-rotoscope.html">drawing a rotoscope (1.5pts) (released)</a></li>
		<li><a href="visualization-photos.html">taking some high-quality photos (1.5pts) (released)</a></li>
		<li><a href="visualization-video.html">making a short 2-3 sequences video (2pts) (released)</a></li>
	</ul>
		</li> -->
</ul>


<!-- 
For this part of the PSet, you will be doing the following steps: <br>
<ol>
	<li>Understand how the $1 Unistroke Recognizer works conceptually</li>
	<li>Import the Processing Library for the $1 Unistroke Recognizer</li>
	<li>Add a new gesture by giving it a name and recording a set of touch points that make up the gesture</li>
	<li>Connect the gesture to a function that is called every time the gesture is recognized</li>
	<li>Start gesture detection every time a touch point is recognized from the blob detection</li>
	<li>Visualize the gesture detection result on screen</li>
</ol>
<br>
 -->

        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br /><br>

                 <h4 class="medium.headline" style="padding-bottom:10px;"><a href="pset-multi-touch-pad.html"><b>Pset Series: Multi-Touch Pad</b></a><br></h4>

<ul>
          <li><a href="pset1-circuit-design.html">Pset1: Generating the Fabrication Files</a></li>
          <li><a href="pset2-multitouch-assembly.html">Pset2: Assembling the Circuit</a></li>
          <li><a href="pset3-signal-processing.html">Pset3: Sensing Multi-Touch Input</a></li>
          <li><a href="pset4-visualization.html">Pset4: Visualizing Multi-Touch Input</a></li>
          <li><a href="pset5-multitouch-application.html">Pset5: Gesture Detection</a><br></li>
</ul><br>


<img src="images/pset-multi-touch-pad/pset-processing1.png" width="350px">
<br><br>

In this pset series, you will create an inkjet printed multi-touch pad. You will first write a Processing program that automatically generates the fabrication files for the multi-touch pad. Next, you will print your multi-touch pad, assemble it, and then build the circuit for sensing touch input. You will then write the microcontroller code for reading the touch signals from each electrode. Next, you will extend your code to draw the touch signals into an image and then extract the touch points via computer vision. Finally, you will write a gesture recognizer that can differentiate between different user inputs.<br><br>

</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
