<!DOCTYPE html>
<html>
<head>
	<title>6.810 Pset5 Gesture Recognizer and Touch Pressure Visualizer</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Prism for adding the code snippets-->
	<link href="../../css/prism.css" rel="stylesheet" />

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<script src="../../js/prism.js"></script> 

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2021)</a><br></h>
        <h2 class="headline">Pset 5: Gesture Recognizer & Touch Pressure Visualizer</h2>

       <hr>

In this problem set, you will implement a gesture recognizer that can detect different finger gestures on your multi-touch pad and then execute the corresponding functionality. After this, you will visualize how hard the user is pressing with each finger on the multi-touch pad by visualizing touch pressure in 3D.<br><br>

<b>Steps:</b>
<ol>
	<li><a href="#LEARNGESTURERECOGNIZER">Learn about $1 Unistroke Gesture Recognizer</a></li>
	<li><a href="#RECORDTOUCH">Train Gesture and Connect to Call-Back Function</a></li>
	<li><a href="#TRACKGESTURES">Track and Visualize Gestures</a></li>
	<li><a href="#3DVIEWPORT">3D Visualization: Draw 3D Viewport</a></li>
	<li><a href="#CAMERAMOTION">Implement Camera Motion</a></li>
	<li><a href="#TOUCHPRESSURE">Draw Touch Pressure into 3D Viewport</a></li>
</ol><br>

 <div style="color:black; border: black 1px solid; padding: 20px;margin-bottom:20px;">
<b>Get a new multi-touch pad from a TA:</b><br>
If your printed multi-touch pad is degrading, let us know and we can give you one that we fabricated to test if it's really the multi-touch pad or something in your circuit/code.</div>


<h3 id="LEARNGESTURERECOGNIZER">(1) Learn about $1 Unistroke Gesture Recognizer</h3>

Now that you can recognize where the user is touching, we can also implement <code>gesture recognition</code>. A gesture is a <code>series of touch points</code> that <code>execute a specific function</code>. For instance, if the gesture is swiping from left to right, your code should recognize that the user made this gesture and if the gesture is a drawn 'A' your code should know that an 'A' was drawn.<br><br>

<b>$1 Unistroke Gesture Recognizer:</b> Luckily, you don't have to implement the gesture recognizer from scratch. Instead, you can use the <code>$1 Unistroke Recognizer</code>, which is available as a <code>Processing library.</code> The $1 Unistroke Gesture Recognizer is part of the so-called <code>$1 Gesture Recognizer family,</code> i.e. a set of different gesture recognizers with different capabilities, such as single stroke gesture recognition, multi-stroke gesture recognition etc. In this pset, we are going to use the $1 Unistroke Recognizer since it is the only one that is available for Processing as a library.<br><br>

<b>Gestures the $1 Unistroke Recognizer can detect:</b> <code>Unistroke</code> means that the gesture recognizer can only recognize gestures that are made by <code>one finger</code> in <code>one continous stroke</code>. For instance, consider the 2 gestures shown below designed for the letter "A". The left one is written in one continous stroke and thus can be recognized by the $1 Unistroke recognizer. The right one, however, is written in two separate strokes and thus cannot be recognized. Similarly, you can recognize a single finger swipe left/right gesture but you cannot recognize a zoom gesture with two fingers since that would require combining two separate strokes into one gesture. The $1 family also has a multi-stroke recognizer but as mentioned previously it is not available as a library for Processing.<br><br>

<img src="images/pset5/drawing-pattern-requirement-1.png" width="300px"> <br><br>

<i>Stroke Orientation:</i> The gesture recognizer <code>ignores the stroke orientation</code>. Consider the example below on the left, which shows the same gesture but executed in different orientations on the multi-touch pad. The gesture recognizer automatically compensates for the change in orientation and will detect that both of the gestures are the same.<br><br>

<i>Stroke Direction:</i> While the recognizer ignores stroke orientation, it <code>considers stroke direction</code>, which corresponds to the vector direction of the line along which your finger moves. For example, the two gestures shown below on the right are visually identical but in the first gesture we start on the top left, while in the second gesture we start on the bottom left. Therefore, although visually the same gesture, the $1 recognizer is able to distinguish them as two different gestures.<br><br>

 <img src="images/pset5/stroke-orientation-direction.png" width="700px"> <br>

<b>Try the Gesture Recognizer for yourself on the Demo Webpage:</b> Before you move on, try out the <code>interactive demo of the $1 Unistroke Recognizer</code> that you can <a href="http://depts.washington.edu/acelab/proj/dollar/index.html">find here on this website</a>. Scroll down to the <code>Demo section</code> and you see 16 different gestures you can draw into the gray window on the right side. Draw each of the gestures once to get a feel for how different gestures are designed and how well they can be classified. After you drew a gesture, you will see its classified name at the top of the window. You can also <code>make your own gesture</code> by first drawing your custom gesture into the gray drawing window and then giving it a name in the <code>Add as example of custom type:</code> field, then click <code>Add</code>. If you now draw your custom gesture again, it should be correctly classified.<br><br> 

<a href="http://depts.washington.edu/acelab/proj/dollar/index.html"><img src="images/pset5/one-dollar-demo.png" width="500px"></a> <br><br>

<h3 id="RECORDTOUCH">(2) Train Gesture and Connect to Call-Back Function</h3>

Now that you know a bit more about what the $1 Unistroke recognizer can and cannot do, you can move on to use the recognizer to detect gestures. <br><br>

<b>Import $1 Unistroke Recognizer Library into Processing:</b> Start by <code>importing the '$1 Unistroke Recognizer' library</code> by going to <code>Sketch -> Import Library</code> and finding it through the search bar. Next, import the $1 Unistroke Recognizer library into your Pset Processing code.<br>

<pre><code class="language-processing">import de.voidplus.dollar.*; </code></pre> <br>

<b>Instantiate new Gesture Recognizer:</b> Construct a new object of the one dollar recognizer with:
<pre><code class="language-processing">OneDollar oneDollar = new OneDollar(this);</code></pre> <br>

<b>Turn on Console Plotting:</b> Turn on <code>console plotting</code> for the gesture recognizer using the line below. Later, when you make your gestures, the gesture recognizer will immediately <code>print the name of the gesture</code> on the console and also <code>provide how similar it is to the closest match</code> from the recorded template gestures (e.g. 84% similar).<br>

<pre><code class="language-processing">oneDollar.setVerbose(true); // activates console plotting</code></pre> <br>

<b>Load Pre-Recorded Gesture Training Data:</b> Before we can detect a gesture, we have to provide the gesture recognizer with <code>training data</code>. <span style="color:red">We will provide you with prerecorded training data for three different finger gestures: a 'V', a 'XX', and a 'YY'. This prerecorded training data is contained in your Processing program in array gestureVtraining[].  Below you see the array that contains the gesture coordinates for our 'V' gesture.<br><br></span>
 
<img src="images/pset5/add-gesture3.png" width="650px"> <br><br>


<b>Train New Gesture Based on Training Data:</b> Next, use the pre-recorded training data to and feed it into the <code>learn()</code> function to add the new gesture. <br><br>

<pre><code class="language-processing">oneDollar.learn(String gesture-name, int[] x-y-coordinates); // {x1, y1, x2, y2, x3, y3, x4, y4, ...} </code></pre> <br>

<b>Adding a Call-Back Function to your Gesture:</b> Now that you have added your custom 'V' finger gesture to the gesture recognizer, you next have to define <code>which function should be called</code> when the gesture is being detected. Such a function is called a <code>callback function</code> and it will be triggered every time the algorithm detects the corresponding finger gesture. You can bind a finger gesture to a callback function via the command below. Note that the parameter <code>callback-function-name</code> is the name of the function without parenthesis and parameters. For now just create the functions and we will add the rest later.<br>

<pre><code class="language-processing">oneDollar.bind(String gesture-name, String callback-function-name); </code></pre><br>

<b>Implement the Callback Function:</b> Next, you need to implement the callback function. The callback function has to have the format shown below, i.e. contain the <code>same number of parameters and parameter types</code> and <code>return void</code>. The function name and parameter names, however, are up to you. If upon recognizing the gesture, you just want to execute some action, you can simply add that code into the function body. If you want to build a location dependent gesture, you can also check where on the multi-touch pad the gesture started or ended using the coordinates provided by the callback function.<br>

<pre><code class="language-processing">void foo(String gesture-name, float percentOfSimilarity, int startX, int startY, int centroidX, int centroidY, int endX, int endY){
	// do something when the gesture is detected
 }
</code></pre> <br>

<h3 id="TRACKGESTURES">(3) Track and Visualize Gestures</h3>

Now that you have defined how your gesture looks like and what function should be called when it is detected, you still have to tell your code <code>when</code> it should start looking for the gesture. <br><br>

<b>Start tracking when Finger Blob Detected:</b> We want to start tracking gestures when our code <code>detects a finger blob</code>. Therefore, you need to add the tracking function below in the blob detection method where it determines if a finger was successfully detected. The x/y coordinate in the function are the x/y coordinate of your touch point.<br>

<pre><code>oneDollar.track(int x-coordinate, int y-coordinate);</code></pre>

Note that you don't have to 'stop' tracking. The <code>track()</code> function will only be executed when Processing goes through its loop and detects a finger blob, and if no finger has been present for a while, i.e. no new data is fed to the gesture recognizer, the recognizer will automatically determine that the gesture has ended and will provide its best guess at what gesture was executed. <br><br>

<b>Load Pre-Recorded Gesture Data:</b> <span style="color:red">Next, let's load some pre-recorded gesture data that was actually recorded on a multi-touch pad. We will use prerecorded data from a text file again to make this easier for you.</span> Open the Arduino program that can stream prerecorded data and replace the file path with 'gesture.txt' that you can download <a href="software/XXXX.txt" style="color:blue">from here</a>. The data contains <span style="color:red">two V gestures (first 5 seconds), followed by two XX gestures (second 5 seconds), followed by two YY gestures (third 5 seconds).</span> <br><br>

<b>Visualize Gesture Detection Result:</b> Finally, once your gesture is detected, your callback function should visualize the detected gesture. For this, you should <code>write the name of the detected gesture into the Processing UI window</code> as shown in the top left corner of the images below and also <code>draw the touch point centers into the window</code> (discard the blob contour). The touch point centers should be visible in the window for a while even after you lift your finger from the multi-touch pad (e.g., show them for another 2 seconds before removing them from the window). You can implement this by <code>storing the touch point centers</code> in a separate array. Draw the array points when the array is not empty, and clear the points in the array whenever it exceeds the 2 seconds time after the gesture was recognized (use the <code>millis()</code> and <code>System.nanoTime()</code> functions from <a href="pset4-visualization.html#REDUCENOISE">PSet4</a> for this). The image down below shows an example of the UI when the "Z" gesture is recognized.<br><br>

<img src="images/pset5/UI-recognizer-Z-draw-points.png" width="350px"> <br><br>

<h3 id="3DVIEWPORT">(4) 3D Visualization: Draw 3D Viewport</h3>

In the previous section, we used the touch locations to recognize gestures. Next, we will also detect <code>touch pressure</code> and visualize it in the user interface. To visualize the touch pressure, you are going to use a <code>3D visualization,</code> in which the <code>height of the touch point represents how much pressure is applied</code>. Below you can see an example: on the left side only little pressure is applied and the touch bar is small, whereas on the right side a lot of pressure is applied and the bar is much higher.<br><br>


<img src="images/pset5/UI-3d-visualization-1-finger-1.png" width="354px">
<img src="images/pset5/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

<b>Load Pre-Recorded Touch Data with Different Pressure Levels:</b> We prerecorded another set of touch data for you that has different pressure levels, it comes again in the form of a <code>text file</code> that you can download <a href="software/XXX.txt" style="color:blue">from here</a>. It contains <span style="color: red">one finger touching three times at different pressure levels (first 5 seconds), then two fingers touching three times at different pressure levels... Change the file path in your Arduino program to send this data to the Serial port.</span><br><br>

<b>Skeleton Code:</b> If you look at your code, you will see that we already defined several variables and functions for you that are related to the 3D visualization (e.g., the axis and camerapos parameters and the <code>drawAxis(),</code> <code>cameraViewControl()</code> and <code>update3DImage()</code> functions).<br><br>

<span style="color:red">replace image</span><br>
<img src="images/pset5/pset5-skeleton-code.png" width="650px"> <br><br>

<b>Drawing the XYZ axes for 3D Visualization:</b> Let's start by drawing the XYZ axes for 3D visualization in the <code>drawAxis()</code> function. Note that we are in 3D now, not 2D so you need 3 coordinates for defining the positions of your points/lines. <br><br>

<i>Direction of Axis:</i> The question that arrises is <code>which axis is x, y, and z</code>, and which way is <code>positive</code> and <code>negative</code>. In Processing, it uses the convention as shown below. Note that because of this convention, you need to draw the Y axis and the sensing data on the y-axis in negative direction so it is pointing "up".<br><br>

<!-- , which is a 'left-handed' cartesian system. You can replicate this 'left-handed' system by doing the following: Take your left hand, point your index finger in the positive y direction (down) and your thumb in the positive x direction (to the right), the rest of your fingers will point towards the positive z-direction. If that matches the image below, you know your system is left h -->


<img src="images/pset5/processing-3d-system.png" width="250px"> <br><br>

<i>Length of Axis:</i> The length of the x and y axes should be the <code>width and height</code> of your interpolated PImage.
The length of the Y axis should be slightly higher than the <code>maximum pixel value (brightness)</code> of your interpolated PImage so you have some space at the top. <br><br>

<i>Color of the Axis:</i> In addition to generating the axis lines, you need to color the axis by drawing a colored dot at their end. We will use <code>red for the x-axis</code>, <code>green for the y-axis</code>, and <code>blue for the z-axis</code> (since "RGB" corresponding to "XYZ").<br><br>

Once you finish the <code>drawAxis()</code> function, you should have something that looks like this (note that the height of your y-axis depends on the brightness values you get in your PImage and may not be shown to scale here):<br><br>

<img src="images/pset5/UI-3d-visualization-drawAxis.png" width="400px"> <br><br>

<h3 id="CAMERAMOTION">(5) Implement Camera Motion</h3>

Without camera motion, it is kind of hard to see if you really succeeded in drawing your axis correctly or if it just 'looks' correct from the 2D perspective but in reality the lines are all over the place in 3D space. To facilitate debugging, we will implement camera control, i.e. you will be able to move around the camera by pressing keys on the keyboard to zoom in/out and move the camera around the scene.<br><br>

<b>Initialize Camera in Processing:</b> Processing provides a <code>Camera Class</code> for implementing camera movement. You can find the <a href="https://processing.org/reference/camera_.html">camera class documentation here</a>. To get started, <code>initialize a new camera</code> in Processing with the following method:<br><br>

<pre>camera(float eyeX, float eyeY, float eyeZ, float centerX, float centerY, float centerZ, float upX, float upY, float upZ); </pre>

<i>eye x/y/z:</i> This is the <code>camera's 3D position</code> in space, i.e. where it would be physically located if it was a camera in real-life.<br><br>

<i>center x/y/z:</i> The <code>center of the scene</code> defines how the camera is angled. For instance, if the center of the scene is lower than the camera, then the camera will look down, resulting in a bird's eye view. Similarly, if the center of the scene is higher than the camera, then the camera will look up, resulting in a frog's eye view.</span><br><br>

<i>up x/y/z:</i> This defines <code>which axis is facing upwards</code>. You can use values from 0 to 1 to -1. If you use a -1, the camera will be flipped and face up-side-down. In our case, we want to keep it simple and use for x,y,z 0,1,0 since y is pointing upwards.<br><br>

When you initialize your camera, you may want to start by using the default parameter values we provide in the code, which should give you a reasonable start for a good viewport. If you see nothing in your view after initializing the camera, you may be facing the wrong way with you camera, e.g. the scene may be behind you, which causes you to only see a white screen with nothing on it.<br><br>

<b>Zooming in/out:</b> Moving the <code>camera position (eye position)</code> closer or further away from the 3D visualization results in a zooming in/out effect (see images below). Implement a zooming effect so that when you press the <code>key "E" you zoom in</code> and if you press the <code>key "Q" you zoom out</code>.<br><br>

<img src="images/pset5/UI-3d-visualization-cameraViewControl-zoom-out.png" width="350px">
<img src="images/pset5/UI-3d-visualization-cameraViewControl-zoom-in.png" width="350px"> <br><br>

<b>Rotating in the X-Z Plane:</b> To move the camera in the x-z plane around the 3D visualization, you need to keep it at the <code>same distance from the 3D visualization</code> at all times (see images below). This is best accomplished by <code>moving the camera on a circle</code> around the scene. Update the <code>camera position (eye position)</code> to accomplish this (you have to do some math here, check out how radians work again to help you with this). If you <code>press the key 'D' rotate right</code> and if you press the <code>key 'A' rotate left</code>. Note that you only have to update the camera's position. Since you set the scene's center x/y/z to be the location of your 3D visualization, the camera will automatically rotate while moving around the 3D visualization and always look at it.<br><br>

<img src="images/pset5/UI-3d-visualization-cameraViewControl-rotate-1.png" width="350px">
<img src="images/pset5/UI-3d-visualization-cameraViewControl-rotate-2.png" width="350px"> <br><br>

<b>Changing the Camera's Y-Axis Height:</b> To move the camera along the y-axis, you need to change the <code>camera's eye position</code> again, but this time only along the y-axis. Pressing the <code>key 'W' should move the camera up</code> and pressing the <code>key 'S' should move the camera down</code> along the y-axis as shown in the image below.<br><br>

<img src="images/pset5/UI-3d-visualization-cameraViewControl-view-height-1.png" width="350px">
<img src="images/pset5/UI-3d-visualization-cameraViewControl-view-height-2.png" width="350px"> <br><br>

<b>Adjusting Frame Rate:</b> If you are experiencing some lagging in your application, you can also set the frame rate for the camera for better performance using the line below:<br>

<pre>frameRate(30);</pre><br>

<h3 id="TOUCHPRESSURE">(6) Draw Touch Pressure into 3D Viewport</h3>

Next, you will draw the touch points and associated <code>touch pressure</code> into the 3D visualization based on your interpolated PImage.<br><br>

<span style="color:red">can we update this section to work only with blobs to discard the noise?</span><br>

<b>Drawing 3D Points in Processing:</b> To draw the touch points and their pressure, you will create a set of 3D points, where the <code>X,Z coordinates</code> are the <code>row & column number of the pixel on the PImage</code>, and the <code>Y-coordinate is the interpolated value</code> of the pixel. As mentioned previously, the brighter pixels are in an area the more pressure was applied to this area. As a result, the brighter a pixel, the higher the touch bar will be at this point.<br><br>

<b>Mapping the Touch Pressure (Y Value) to a Color Gradient:</b> You also need to <code>color the touch bars</code> with a color gradient that represents the "height" (i.e. y-axis coordinate) information. We chose a <code>color gradient</code> of red when only touching with light pressure and yellow when touching with high pressure. You can choose your own color gradient, just make sure that the colors are very different so you can actually see a difference. To implement the y-height to color mapping, you may find it helpful to use the <a href="https://processing.org/reference/map_.html"><code>map() function</code></a> that you already used in the previous psets. The resulting 3D visualization should look like below.<br><br>

<span style="color:red">can we update these images and remove noise</span><br>
One finger touching: <br>
<img src="images/pset5/UI-3d-visualization-1-finger-1.png" width="350px">
<img src="images/pset5/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

Two fingers touching:<br>
<img src="images/pset5/UI-3d-visualization-2-finger-2.png" width="350px">
<img src="images/pset5/UI-3d-visualization-2-finger-4.png" width="350px"> <br><br>

<span style="color:red">Note that all this noise is because we use the PImage that also contains brightness values outside the recognized 'finger blobs'. If you want a cleaner visualization, you can create a new PImage that has all pixels black except those pixels that belong to a blob. Note that this is not mandatory for this pset but if you'd like some clean output you can experiment with this.</span>
<br><br>


<h3 id="BICUBIC">(7) Use Your Own Multi-touch Pad Data</h3>

So far, we have only used prerecorded multi-touch data. In this last section, you will use your own multi-touch pad and replicate what you did so far with the prerecorded data.<br><br>

<b>Read your own Multi-touch Pad Data from the Serial Port into Processing:</b> So far, you have always read our prerecorded multi-touch data from the Serial port. Now, you will read your own multi-touch data from the Serial Port. You will notice that your own live data will be a lot more messy, i.e. sometimes a dataline will be completely missing, and othertimes there may be unrecognized characters. Make sure your <code>readSerial()</code> function in Processing properly handles these unexpected characters and formats, and stores only the valid data. <span style="color:red">You can look at the stringXX class for how to clean up the data.</span> This messy data was also the reason why we update the array line by line not as a whole since we don't want that a single missing/contaminated data line holds up the entire 2D array update. <br><br>

<b>Recording Noise Baseline Signal:</b> Next, you need to re-record the noise baseline signal since your own multi-touch data has a different noise baseline than our prerecorded data. Record at least 2 seconds of non-touch data and then average the values to create the individual noise baseline value for each position in the multi-touch pad as you did before with the prerecorded data. <br><br>

<b>Subtract Noise Baseline from Live Touch Data:</b> Next subtract the noise baseline from your live touch data. Donâ€™t worry too much if you are not getting exactly 0 for the subtracted results, as long as you are getting distinguishable numbers when you are touching vs. not touching the multi-touch pad. If you are not getting good results for the noise baseline, you can increase the baseline time period to a longer time period, such as 5 seconds instead of only 2 seconds, to be able to average more numbers together before substracting the result.<br><br> 

<b>Converting Sensor Signals into Grayscales:</b> The <code>map()</code> function required you to define the <code>start1/stop1 (source):</code> parameters (i.e., min-input-range, and max-input-range). You need to adjust these values now that you have live touch data. Remember, to find a good value for the min-input-range, you can print out the no-touch baseline values (i.e. sensing values after the noise substruction), and pick a "middle-to-low" value from the different values in the array. To find a good value for the max-input-range, look at the printed values when you are touching the entire multi-touch pad, and pick a "middle-to-high" value. You might want to play around with these values later after we scaled up the image to achieve a more "clean" visualization.<br><br>

<b>Checking Grayscale Values after Scaling and Interpolating the Image:</b> If after scaling the grayscale image, your results don't look, try <code>changing how you map your sensors signals to the grayscale values of 0 - 255</code>. Remember, when you mapped your sensor signals to the 0 - 255 range, we had recommended that you use the <code>middle-to-low</code> sensor reading when not touching to map to 0 (black), and the <code>middle-to-high</code> sensor reading when touching to 255 (white). Let's assume for a moment your middle-to-high sensor reading when touching was 1500 and you mapped it to 255 (white). As a result, any sensor reading above 1500 is now leading to white pixels, whereas anything below will lead to darker pixels. If you find that you <code>only see dark areas</code> in your image and <code>nothing is white</code> (or not "white" enough) when touched, then perhaps <code>the 1500 threshold for middle-to-high is too high</code>. Consider using a lower value to increase what signal values are being treated as 'touched' white pixels. Similarly, if you see too much noise in your image, i.e. <code>everything is white</code> (or multiple areas get "too white" when the touch data reads only one finger), perhaps your <code>middle-to-low sensor value</code> is too low and you should set it to something higher, so that more signal values are treated as black non-touch pixels. Note that you may have to slightly adjust these values when you change the setup in which you use your multi-touch pad. For instance, if place your multi-touch pad on a different surface (glass vs. wooden table) or if anything else in your environment undergoes a significant change (e.g. increase in humidity), the sensor values and the noise level may change. We therefore recommend you find yourself a place where you do the calibration once and then try to not change it too much.<br><br>

<b>Adjusting Blob Detection with Custom Threshold:</b> If your blob detection is not finding all the blobs in your image, remember that you can adjust the custom threshold. Consider the following: Before performing blob detection, the library automatically <code>converts your image from a grayscale image (pixel values ranging from 0 - 255) into a black/white image (pixels are either 0 or 255)</code>. The reason the library does this is because blob detection groups pixels of similar color together to find blobs and that is easier to do when pixels are either black or white and not something in between. The library automatically picks a <code>threshold</code> value, for instance, it may decide to convert <code>all pixels with a value below '100' to black (i.e. '0')</code> and <code>all pixels above '100' to white (i.e. '255')</code>. If you are not getting any blobs (i.e., groups of white pixels), it is likely that during thresholding all pixels were converted to black and no white pixels are left because the threshold value was set too high. To see if a lower threshold would leave you with more white pixels, you can use the <code>setThreshold()</code> function to determine which brightness levels from 0-255 should be taking into account when searching for blobs. Note that the threshold takes as input a <code>float value from 0-1</code> so you need to scale your 0-255 values accordingly. Once you set your custom threshold, check again if you now get the right blob number. Note that the threshold value will be different for our prerecorded data vs. live data from your multi-touch pad, so you will have to adjust it when you switch to live data. <span style="color:red">The threshold value will also be different depending on the environment conditions (e.g., more humid air one day than the other) or the surface your multi-touch pad is placed on, so be prepared to adjust the threshold for these situations.</span><br><br>

<b>Testing your Blob Detection with Live Data:</b> We recommend you test your blob detection with 1 finger, 2 fingers, and 3 fingers touching the multi-touch pad to see if it can reliably determine where the finger is touching. This will become important for the next part of this problem set in which we will build a gesture recognizer. You may want to go back to section (1) and further refine your threshold until you find a setting that works reliably.<br><br>

<b>Testing your Detection of Touch Pressure:</b> Finally, adjust the 3D visualization to also show the live touch pressure.<br><br>

<b>Gesture Recognition on Live Data:</b> We will not do the gesture recognition on live data since that would be too much work to change. However, if you want to give it a try, feel free to do so.<br><br>


<!-- For starters, we will use a 'V' gesture, which is one of the most reliable ones that can be detected. Other reliable gestures are triangle, zigzag, checkmark, and anything that has some unique features. A circle, for instance, is one of the harder gestures to recognize due to its smooth outline that is easily susceptible to noise. To create a new finger gesture, you can use the following function:<br>

<i>Recording training data for x/y coordinates for New Finger Gesture:</i> While you could obtain the <code>x-y-coordinates for your gesture template</code> by tracking finger input from your multi-touch pad, we recommend not doing this and instead recommend that you <code>draw the gesture with the mouse</code> on screen and record the x-y coordinates from the <code>mouseDragged()</code> event. The reason for this is that the drawing with the mouse will give you very clear tracking points for creating your template gesture. Later on, when you draw the gesture with your multi-touch pad during actual use, having the clean data from the mouse dragging on screen will make the gesture recognition more fault resistant. In contrast, if you record your template gesture on the multi-touch pad, you will already have some noise in the data and it will be harder to match a gesture when there is additional noise during actual use later on. Below you see a recording of all the coordinates for our 'V' gesture that we then feed into the <code>learn()</code> function to add our new gesture.<br><br>

<b>Design Custom Finger Gestures: </b> Now that you have implemented the basic gesture recognition, let's go ahead and implement <code>3 different finger gestures</code> that can be recognized by your multi-touch pad. You can implement recognizing <code>letters</code> (A, C etc.), <code>2D shapes</code> (triangle, circle etc.), and <code>symbols</code> (check mark etc.). Note that in good gesture design, the gesture should be related to what function is being called. For instance, you cannot draw a 'star' and associate it with a 'smile'. Instead, it would be better to draw a 'half circle', which more closely resembles a 'smile'. Similarly, gestures cannot be too abstract. For instance, you cannot have your finger draw one straight line on the multi-touch pad and the system recognizes it as 'fish'. Instead, there are better one-stroke gestures that could represent a fish outline. Once you have three gestures working, record a short video as described at the end of this pset in 'deliverables'.<br><br>
 -->

<h3>Deliverables</h3>

For grading, please upload the following to your google drive student folder:<br>


Gesture Recognizer
		<ul>
			<li>a drawing on paper that shows your 3 custom finger gestures so we know which gestures you implemented, use the same notation as in the pset to indicate the start point for executing the gesture</li>
			<li>the .pde file of your Processing program</li>
			<li>3 photos showing your Processing UI successfully recognizing the 3 different finger gestures </li>
			<li>a short video showing your Processing UI successfully recognizing the 3 different finger gestures (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>

<h3>Grading</h3>

We will give 20 pts in total:
<ul>
	<li>5pts: gesture recognizer</li>
	<li>5pts: touch pressure</li>
	<li>10pts: making it work on your own multi-touch pad</li>
	<!-- <li>5pts: the remaining 5pts will be given for creating presentation materials for your multi-touch pad. This will be part of the weekly Friday labs and you will work on this over the next weeks in small steps.
		<ul>
		<li><a href="visualization-rotoscope.html">drawing a rotoscope (1.5pts) (released)</a></li>
		<li><a href="visualization-photos.html">taking some high-quality photos (1.5pts) (released)</a></li>
		<li><a href="visualization-video.html">making a short 2-3 sequences video (2pts) (released)</a></li>
	</ul>
		</li> -->
</ul>


<!-- 
For this part of the PSet, you will be doing the following steps: <br>
<ol>
	<li>Understand how the $1 Unistroke Recognizer works conceptually</li>
	<li>Import the Processing Library for the $1 Unistroke Recognizer</li>
	<li>Add a new gesture by giving it a name and recording a set of touch points that make up the gesture</li>
	<li>Connect the gesture to a function that is called every time the gesture is recognized</li>
	<li>Start gesture detection every time a touch point is recognized from the blob detection</li>
	<li>Visualize the gesture detection result on screen</li>
</ol>
<br>
 -->

        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br /><br>

                 <h4 class="medium.headline" style="padding-bottom:10px;"><a href="pset-multi-touch-pad.html"><b>Pset Series: Multi-Touch Pad</b></a><br></h4>

<ul>
          <li><a href="pset1-circuit-design.html">Pset1: Generating the Fabrication Files</a></li>
          <li><a href="pset2-multitouch-assembly.html">Pset2: Assembling the Circuit</a></li>
          <li><a href="pset3-signal-processing.html">Pset3: Sensing Multi-Touch Input</a></li>
          <li><a href="pset4-visualization.html">Pset4: Visualizing Multi-Touch Input</a></li>
          <li><a href="pset5-multitouch-application.html">Pset5: Gesture Detection</a><br></li>
</ul><br>


<img src="images/pset-multi-touch-pad/pset-processing1.png" width="350px">
<br><br>

In this pset series, you will create an inkjet printed multi-touch pad. You will first write a Processing program that automatically generates the fabrication files for the multi-touch pad. Next, you will print your multi-touch pad, assemble it, and then build the circuit for sensing touch input. You will then write the microcontroller code for reading the touch signals from each electrode. Next, you will extend your code to draw the touch signals into an image and then extract the touch points via computer vision. Finally, you will write a gesture recognizer that can differentiate between different user inputs.<br><br>

</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
