<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h4>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset4/pset2-overall.png" width="400px">
<img src="images/pset4/UI-blob-touch-3.png" width="340px"> <br>

          <hr>


<h2 class="headline">Problem Set 3 (due Friday, Oct. 23, 2020, 11.59pm)</h2>

Now that you have the hardware ready and Arduino sensing code prepared, you will write the Processing code that visualizes the sensor signals and extracts the touch point coordinates. To accomplish this, you will do the following five steps:<br>

<ol>
	<li>Read the multi-touch sensing data into Processing via the Serial Port.</li>
	<li>Clean the sensing data from noise by determining the noise baseline and subtracting it from the received signals.</li>
	<li>Convert the received signals into grayscale values that can be displayed as pixels in a 9x8 pixel image.</li>
	<li>Scale the image to 500x500px using bicubic interpolation to facilitate touch point extraction.</li>
	<li>Implement blob detection to extract the touch point locations.</li>
</ol>
<br>

<h3>Skeleton Code</h3>

Start by downloading <a href="software/pset3_visualization_skeleton_v3.zip">the skeleton code for the PSet3 from  here</a>. <br>
Before you can execute it, you first have to install two libraries (see next step). <br><br>

<h3>Install external libraries</h3>

For this PSet, we will be using two Processing libraries: 
<ol>
	<li>OpenCV (a library that helps with image processing, in our case we will draw our touch signals into an image, see picture at start of pset3)</li>
	<li>BlobDetection (a library that helps to find blobs inside of images, in our case the blobs are the touch points on the multi-touch pad)</li>
</ol>

You can install them directly in Processing by going to:<br>
<b>Sketch/Import Library/Add Library</b> and searching for their name.<br><br>

<img src="images/pset4/processing-library-opencv.png" width="370px">
<img src="images/pset4/processing-library-blob-detection.png" width="370px"> <br><br>


<h3>(1) Read the Multi-Touch Sensing Data into Processing</h3><br>

<b>Refresher from PSet2</b><br>
In pset2, you already wrote the <b>Arduino</b> code that reads the receiver pins from the multi-touch pad and writes the resulting data onto the serial port in the following way: <br><br>

<pre>
0,50,83,58,79,108,75,82,54   //columm0, row0val, row1val, row2val
1,55,92,120,84,63,61,88,53   //columm1, row0val, row1val, row2val
2,61,64,73,66,92,78,67,57
3,65,117,116,84,48,81,91,71
4,65,128,116,54,76,81,88,59
5,61,86,66,54,114,78,64,64
6,59,86,120,83,85,75,93,63
7,56,86,116,70,72,83,80,64
8,23,82,74,68,98,64,62,52
...
</pre> <br>

<b>Read Data from Serial Port Into Processing</b><br>
Next, you need to read this data from the serial port into Processing.<br>
We have already shown you how to read data from the serial port into Processing in Lab 1 & 2 and you can check your prior code from back then to see how to do it.<br><br>

In the skeleton code, put your code for reading the data into the <b>readSerial()</b> function: <br>
<img src="images/pset4/readSerial-skeleton-code.png" width="700px"> <br>

<b>Save Data for One Complete Scanning Pass into a 2D Array</b><br>
The data for a single pass on all columns and rows should be saved into a 2D array.<br>
When considering the size of your 2D array, remember that we build a 9x8 multi-touch pad.<br>
Printing the 2D array to the Processing command line should look something like this:<br><br>

<img src="images/pset4/serial-read-array-one-pass.png" width="750px"> <br><br>

Every time a new pass over all columns/rows starts, you need to override your array values.<br><br>

<h3>(2) Cleaning the Received Signal by Reducing Noise</h3><br>

<b>What is noise?</b><br>
As you may have noticed, even when you are not touching the multi-touch pad, the receiver pins almost always receive some value although you would expect that they should receive '0'.<br>
This is what we call "noise" in signal processing.<br><br>

<b>What causes noise?</b><br>
Noise can be caused by a variety of factors.<br>
On the entire multi-touch pad, noise can be caused by the substrate the multi-touch pad is placed on, i.e. depending on if you place it on a glass table or a wooden table the noise level will be different.<br>Additionally, noise can be different at each of the connection points in the circuit. For instance, each FPC connector and each wire may have different noise levels since the conductive lines all make slightly different levels of contact.<br>
Furthermore, since the inkjet printed circuit may not be perfectly consistent everywhere either because of uneven printing or because of different aging levels over time, the different printed paths may also have different levels of noise.<br>
Finally, in addition to the reasons above, we can also have different noise levels on a single row of electrodes because the distance between the receiver pin and each electrode in the row is different, thus the electodes further away accumulate noise over a larger distance.<br>
All of these factors together result in different noise levels at each point in your multi-touch pad.<br><br> 

<b>Why is noise an issue?</b><br>
Noise is an issue because it can lead to false positives, i.e. your code may think the multi-touch pad was touched although no interaction occured.<br>
To prevent this, we need to eliminate the noise in our signal.<br><br>

<b>How can we eliminate the noise?</b><br>
In order to eliminate the noise and have more "clean" data, there are many different processing steps that one can take.<br> 
In our case, we will implement a simple noise baseline filter.<br><br>

<b>How does a noise baseline filter work?</b><br>
A noise baseline filter works in two steps:<br><br>

<i>(1) Record Noise Baseline Signal:</i> First, you need to record the signal when you are <b>not</b> touching the multi-touch pad. The signal is recorded over some period of time and the recorded values are then averaged and saved as the noise baseline value. You only have to do this once at the beginning (and please only do it once). In our case, we found that recording <b>2 seconds</b> of non-touch sensing data is sufficient for the multi-touch noise reduction. Note that we record the noise baseline for every position in our multi-touch pad for the reasons mentioned above, i.e. that every position has its own level of noise. We therefore record the noise baseline for every received <b>value</b> and thus we are having 9x8 = 72 noiseline base values in total.<br><br> 

Implement the noise baseline filtering in the <b>setBaseLine()</b> function and change the <b>boolean baseLineSet</b> in the skeleton code to "true" once the baseline is set. <br>
You will find the Java's native <b><i>System.nanoTime()</i></b> or Processing's <b><i>millis()</i></b> funtion to be helpful. For more details, you can look at <a href="https://processing.org/reference/millis_.html"> the reference here </a>.<br><br>

<img src="images/pset4/setBaseLine-skeleton-code.png" width="600px"> <br>

<i>(2) Subtract Noise Baseline from Usage Signal:</i> After you stored the noise baseline for each value, we can look into how to use it to create better signals during the actual user interaction. During interaction, the signal for each value is again recorded but this time you subtract the previously recorded noise baseline value from each new incoming signal value. Thus, if you don't touch the multi-touch pad, the noisy incoming signal minus the noise baseline will equal 0 (or at least be close to it). Therefore, the chance of false positives is much smaller, i.e. the chance that your code thinks that a touch occured although there was none is greatly reduced.<br><br>  

Implement the noise baseline substraction in the <b>subtractBaseLine()</b> function and subtract the baseline values from corresponding new incoming raw values <b>only when the baseline is set.</b> <br>
You should also make sure the "clean" sensing values stored after the subtraction are not negative.<br><br>

<img src="images/pset4/substractBaseLine-skeleton-code.png" width="600px"> <br>


<!-- <h3>(3) Set the background image color and bicubic interpolation</h3> -->
<h3>(3) Converting the Received Signals into Grayscale Values that can be Displayed as an Image</h3>

Now that we have "clean" sensing values, our next step is to find out where the multi-touch pad was actually touched.<br><br>

<b>Using Image Processing for Detecting Touch Points</b><br>
While there are multiple different ways to find our where the multi-touch pad was touched, we will use an approach based on image-processing.<br>
First, we will convert our sensor signals into grayscale values and then draw the grayscale values into a 9x8 pixel image.<br> 
Brighter pixels will correspond to higher sensor signals (i.e. touch points) while darker pixels will correspond to areas that were not touched.<br>
Since image processing on such a small 9x8 pixel image is too difficult, we will increase the size of the image to 500x500px using bicubic interpolation to fill in for the missing pixel values.<br>
After this, we can use 'blob' detection to extract the white blobs in the image (i.e. the touch points) and get the touch coordinates from the blobs.<br>
Let's do this one step at a time.<br>

<img src="images/pset4/UI-blob-touch-3.png" width="400px"><br>

<b>Creating the Image</b><br>
For our visualization, we will construct an image (PImage) the same size as our 2D array that contains the sensing data.<br>
Note that this PImage will be tiny (9x8 pixel) and you will likely have to search a bit for it on screen if you decide to display it for debugging purposes.<br><br>

<b>Creating a Grayscale Image</b><br>
The PImage will be gray-scale since for our purposes we are only interested in brighter vs. darker spots for the purpose of touch detection.<br>
If you set a PImage's pixels to a color value from 0 (black) - 255 (white), it will automatically be treated as grayscale.<br><br>

<b>Converting the Sensor Signals into Grayscale Values</b><br>

Before we can draw our sensor signals into the PImage as grayscale values, we have to make sure they have the correct range, i.e. are between 0-255.<br>
We therefore have to scale our sensor signals accordingly.<br><br> 

To scale your sensor values to a range of 0 - 255, you can use <a href="https://processing.org/reference/map_.html">the <b>map()</b> function</a> in Processing.<br>
As you can see in <a href="https://processing.org/reference/map_.html">the map() function reference,</a> apart from the incoming value to be converted, we also have to set four "scaling" values:<br><br>

<img src="images/pset4/map-function-parameters.png" width="400px"> <br><br>

<b>start2/stop2 (target):</b> The last two parameters, i.e. our target range, are pretty straight forward since we already discussed we want to scale to 0 - 255.<br><br>

<b>start1/stop1 (source):</b> For the source parameters, use the middle-to-low end sensor reading when you are not touching for start1, and middle-to-high sensor reading when you are touching for stop1. You can think of these 2 parameters as "min-input-range" and "max-input-range" of your incoming sensing data. We will revisit this later and it will become more clear why that is a good strategy.<br><br>

After you scaled your sensor values, use them to set the pixel color of each pixel in the 9x8 pixel PImage.<br><br>

Put your code into the <b>setColors()</b> function in the skeleton code: <br><br>
<img src="images/pset4/setColors-skeleton-code.png" width="700px"> <br>


<h3>(4) Scaling up the Generated Image Using Bicubic Interpolation</h3>

<b>Why do we need to scale our image?</b><br>
While the tiny 9x8 pixel PImage that we just created is accurately representing our sensor values, it is not exactly the best image for visualization, nor it is a good representation of the physical size of our multi-touch pad. <br>
We therefore have to scale up the image to a size that is more suitable for further image processing, such as blob detection.<br>
In our case, we want to scale up the image to <b>500px x 500px.</b><br>
Since our original tiny 9x8 pixel image only contains 9x8 values, we need to figure out what values we should use for all the newly generated 'gaps' in the 500x500px image since it needs 500x500 values to be completely filled in at each pixel.<br>
To do this, we use a technique called interpolation.<br><br>

<b>What is interpolation?</b><br>
Interpolation can help fill in values where no actual values are present and can thus help us determine what value the 'in-between' pixels in our 500x500px image should have.<br>
There are many different interpolation techniques that can be used to interpolate the pixel values to create a smooth scaled up appearance of the original input image.<br>
Some of simplest interpolation methods are bi-linear and bi-cubic interpolation. <br><br>

<b>What is the difference between bi-linear and bi-cubic interpolation?</b><br>

<i>Bi-linear interpolation:</i> considers the closest 2x2 neighborhood of known pixel values surrounding the unknown pixel. It then takes a weighted (by distance) average of these 4 pixels to arrive at its final interpolated value.<br><br>
<img src="images/pset4/linear-1.png" width="230px">
<img src="images/pset4/linear-2.png" width="230px">
<img src="images/pset4/linear-3.png" width="230px"> <br><br>

<i>Bi-cubic interpolation:</i> is considering the closest 4x4 neighborhood of known pixels â€” for a total of 16 pixels. Since these are at various distances from the unknown pixel, closer pixels are given a higher weighting in the calculation. Bicubic produces noticeably smoother images than the bi-linear interpolation. 

<br> <br>

<img src="images/pset4/cubic-1.png" width="230px">
<img src="images/pset4/cubic-2.png" width="230px">
<img src="images/pset4/cubic-3.png" width="270px"> <br><br>

Thus, a benefit of bicubic interpolation is that it gives you a smoother interpolation result since it considers more values than bilinear interpolation. However, this also comes at the drawback of needing higher computational power since more values need to be processed to generate the interpolated value. For our small multi-touch pad with only a few hundred interpolated values this is not an issue but if you had to do more interpolation it may slow the processing down and the multi-touch pad detection would start lagging behind the user's finger movements.<br><br>


You can see the difference of bi-liner vs. bi-cubic interpolation in the images below (these are from our multi-touch pad with roughly the same location touched by finger, <b><i>Left: Bilinear vs. Right: Bicubic) </i></b>.<br> 

<img src="images/pset4/multi-touch-interpolate-bilinear.png" width="350px"> 
<img src="images/pset4/multi-touch-interpolate-bicubic.png" width="350px"> <br><br>

For a more close look, below are the resulting interpolated images from a 5x5 input data via bi-linear and bi-cubic.<br><br>

<img src="images/pset4/linear-interpolation.png" width="300px"> 
<img src="images/pset4/cubic-interpolation.png" width="300px"> <br><br>

<b>Implementing Bi-Cubic Interpolation with OpenCV</b><br>

Luckily, you do not have to implement the bicubic interpolation from scratch.<br>
There is an image processing and computer vision library called <a href="https://opencv.org/courses/"><b>OpenCV.</b></a><br>
OpenCV exists as a library for many different programming languages, in our case we will use OpenCV's implementation for Java and Processing.<br>
Remember, at the beginning of pset3, you already installed the <b>OpenCV library.</b><br>
If you scroll up in your skeleton code, you can see that we already imported several classes of the library with<br><br>

<pre>
import gab.opencv.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.core.Mat;
import org.opencv.core.Size;
</pre><br>

If you look <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/imgproc/Imgproc.html">at the documentation of the <b>Imgproc class,</b></a> you will see that it provides a <b>resize()</b> function, which also allows us to specify which interpolation method we want to use.<br><br>

<img src="images/pset4/imgproc-resize.png" width="750px"><br><br>

So let's see how we can fill out the parameters.<br><br>

<b>Mat src:</b> This is the input image (9x8px) that we want to resize. As you can see, the image here is in the format <b>Mat</b> (Matrix) and not PImage. We therefore first have to convert our PImage into a Mat. In image processing, images are handles as 2D matrices, where every matrix entry is the color value of one pixel (i.e. 0-255 in our case). You can also think of the 2D matrix as a 2D array with pixel values in rows and columns.<br>
Before we can convert our PImage into a Mat, we first have to construct a new OpenCV object.<br> 
For this, you need to call the corresponding OpenCV constructor in the <a href="http://atduskgreg.github.io/opencv-processing/reference/gab/opencv/OpenCV.html">OpenCV class documentation</a>.<br>
Once you constructed your OpenCV object, you can call the <b>getGray()</b> function on it to retrieve the image as a Mat (check <a href="http://atduskgreg.github.io/opencv-processing/reference/gab/opencv/OpenCV.html">the OpenCV class documentation</a> again). This is your src input parameter for interpolation. <br><br>

<b>Mat dest:</b> This is the output image, i.e. the resized 500x500px image. Again it needs to be in the format of a matrix. For the output image, we want to create a new empty matrix of our desired size. You can find the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/core/Mat.html"><b>OpenCV Mat Reference</b> here</a>. 

<img src="images/pset4/mat-constructor.png" width="350px"><br>

Similar to how other variables have a type, e.g. int or float, matrices also have <a href="http://ninghang.blogspot.com/2012/11/list-of-mat-type-in-opencv.html">types,</a> which describe what types of values can be stored in them. Since we want to make sure the input  and the output images match, you need to reference the image type of your source matrix as the type argument. If you check the Mat reference again, you can see that there is an instance function for getting the matrix type. <br><br>

<b>Size dsize:</b> You can find the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/core/Size.html"><b>OpenCV Size</b> Reference</a> here. Construct a new Size object with 500x500px. <br><br>

<b>double fx, double fy:</b> These are scale factors but since we don't want to scale our image any further, we set these to 0.<br><br>

<b>int interpolation:</b> These are the different interpolation options. If you look at the different fields in the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/imgproc/Imgproc.html"><b>OpenCV Imgproc</a></b> Reference, you will see that there are several options, such as:<br>

<img src="images/pset4/interlinear.png" width="750px"><br>
<img src="images/pset4/intercubic.png" width="720px"><br><br>

We want to use the inter-cubic option as our last parameter.<br><br>

Implement this part in the <b>interpolate()</b> function in the skeleton code: <br><br>

<img src="images/pset4/interpolate-skeleton-code.png" width="600px"> <br><br>

<b>Convert Matrix back to PImage:</b> Finally, after you resized the image with our desired interpolation method, you have to convert your output matrix back into a PImage. For this, you can go back to the <a href="http://atduskgreg.github.io/opencv-processing/reference/"><b>OpenCV class</b> documentation</a> and look for the method that gives you a PImage.<br> This should also still go into the interpolate function in the skeleton code.<br><br>

<b>Checking Your Results:</b><br>

Once you finished the interpolation, you should be able to see images like the ones below. The image should be darker when the multi-touch pad is not touched and have some bright spots when the multi-touch pad is touched. What exactly you see depends on your specific noise level and how well your noise baseline works. <br><br>

<i>Not Touched:</i><br>
<img src="images/pset4/UI-no-blob-no-touch-1.png" width="350px"> 
<img src="images/pset4/UI-no-blob-no-touch-2.png" width="350px"> <br><br>

<i>Touched:</i><br>
left: 1 finger touching, right: 2 fingers touching <br>
<img src="images/pset4/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset4/UI-no-blob-touch-2.png" width="350px"> <br><br>

<b>Improving your Results</b><br>
If your results don't look as good as above, try changing how you map your sensors signals to the grayscale values of 0 - 255.<br> 
Remember, when you mapped your sensor signals to the 0 - 255 range, we had recommended that you use the middle-to-low sensor reading when not touching to map to 0 (black), and the middle-to-high sensor reading when touching to 255 (white). <br>
Let's assume for a moment your middle-to-high sensor reading when touching was 1500 and you mapped it to 255 (white). As a result, any sensor reading above 1500 is now leading to white pixels, whereas anything below will lead to darker pixels. If you find that you only see dark areas in your image and nothing is white (or not "white" enough) when touched, then perhaps the 1500 threshold is too high. Consider using a lower value to increase what signal values are being treated as 'touched' white pixels.<br> 
Similarly, if you see too much noise in your image, i.e. everything is white (or multiple areas get "too white" when only touching with one finger), perhaps your non-touch sensor reading value is too low and you should set it to something higher, so that more signal values are treated as black non-touch pixels. <br>
Note that you may have to slightly adjust these values when you change the setup in which you use your multi-touch pad. For instance, if place your multi-touch pad on a different surface (glass vs. wooden table) or if anything else in your environment undergoes a significant change (e.g. increase in humidity), the sensor values and the noise level may change. We therefore recommend you find yourself a place where you do the calibration once and then try to not change it too much.<br><br>

<h3>(5) Implement blob detection</h3>

While you can now determine with your eyes where the multi-touch pad was touched, we also want to teach our program to do this automatically and tell us at which (x,y) coordinate it sees a touch point.<br>
To do this, we need to write code that detects the white 'blobs' in the image.<br>
This is called 'blob detection'.<br><br> 

<b>Library for Blob Detection</b><br>
Lucky for us, somebody else has already provided a library for blob detection.<br>
You already installed that library at the beginning of this pset and we already imported it into the skeleton code using the following line:<br>

<pre>import blobDetection.*;</pre><br>

You can see the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html"><b>BlobDetection Reference</b> here</a> for details on the different functions.<br><br>

Let's briefly review how blob detection works.<br><br>

<b>Blob Detection works on Black/White Images:</b><br>
Blob detection (also called 'Connected Components' search) works on black and white images. <br>
Thus, a first step before blob detection can actually be performed is to convert our grayscale image into a black and white image.<br>
To do this, the algorithm needs as input a 'threshold', i.e. a number that defines which grayscale values will be set to white and which will be set to black.<br>
You can see an example of thresholding below: all the dark gray values (e.g., all pixels below a grayscale value of 100) were set to black and all the brighter gray values (e.g., all pixels above a gray scale value of 100) were set to white.<br><br>

<img src="images/pset4/threshold.jpg" width="550px"> <br><br>

Our library does the thresholding automatically when we call its computeBlobs() function (we will do this in a minute, keep reading). However, if your blob detection is not finding all the blobs in your image, consider using the <b>setThreshold()</b> function to determine which grayscale level from 0-255 will be used to determine which pixels will become white and thus potential blobs and which pixels will be set to black. Note that the threshold takes as input a float value from 0-1 so you need to scale your values accordingly.<br><br>

<b>Blob Detection Looks for 'Connected Components'</b><br>
Once the image is converted into black/white, the blob detection starts to look for areas that form larger blobs.<br>
The outcome of blob detection looks something like the image below.<br>
Every white area ('blob') has a label, e.g. 1, 2, 3. You can see that there are 'original labels' (big numbers) and 'parent labels' (tiny blue numbers in the upper right corners).<br>
If you consider the parent labels for those pixels that have them, you can see that each 'blob' has the same overall label, e.g. there is one blob with label '1' and one blob with label '3'.<br><br>

<img src="images/pset4/blobdetection-step_10.png" width="650px"> <br><br>

So let's see how we get there from an unlabeled black/white image.<br>
First, we start by going over each pixel.<br>
We start with the top left corner pixel and check if the pixel is white.<br>
If it is not white like in our case, we continue searching through the row before moving onto the next row.<br>
In row 2, pixel 3 we finally find a white pixel.<br><br>

<img src="images/pset4/blobdetection-step_1.png" width="650px"> <br><br>

Once we found a white pixel, we need to check if any of its neighbor pixels are white and if yes, if they already have a label.<br><br>

<img src="images/pset4/blobdetection-step_2.png" width="650px"> <br><br>

In our case, none of the neighbor pixels has a label yet, so we give our selected pixel a new label. We use the label '1' since this is the first label we are creating. In addition, since there is no neighbor pixel with a lower label count, we also set the parent label (tiny blue number) to '1'. Note that when the parent is equal to the original label, we don't show it in the images further down.<br><br>

<img src="images/pset4/blobdetection-step_3.png" width="650px"> <br><br>

We then check the next pixel. This time the pixel has a white neighbor pixel that already has a label (i.e. the one we just labeled).<br><br>

<img src="images/pset4/blobdetection-step_4.png" width="650px"> <br><br>

We therefore set it to the same label '1' and also set its parent label to '1' since there is no neighbor with a lower label number.<br><br>

<img src="images/pset4/blobdetection-step_5.png" width="650px"> <br><br>

The next pixel does not have a white neighbor pixel with a label.<br><br>

<img src="images/pset4/blobdetection-step_6.png" width="650px"> <br><br>

We therefore increase the label count to '2'.<br>
In addition, since there is no lower label in the neighborhood we set the parent to '2' as well.<br><br>

<img src="images/pset4/blobdetection-step_7.png" width="650px"> <br><br>

Let's fast forward a bit and look at a pixel that has several white neighbors with different labels.<br> 
You can see below this happens in row 3.<br>
To resolve this, we need to check all the different labels (i.e. we find '1' and '2') and choose the smallest available label (i.e. '1') as the label for the pixel.<br>
In addition, we update the 'parent label' of all the neighbors to be the smallest available label. As you can see, the neighbor with the label '2' now has a parent label '1' in the top right corner.<br><br>

<img src="images/pset4/blobdetection-step_8.png" width="650px"> <br><br>

If you repeat this process for every pixel in the image a few more times, you end up with the image we showed you at the beginning and which is shown below again for your reference.<br>
Note that the bottom area (row 8) where the main label is 6 and the parent label is 3 required several passes over all pixels to propagate the '3' parent label from the pixels further on the right.<br>
Once there are no more changes in labels, you can be sure that all pixels that have the same label belong to one 'blob', i.e. 'connected component'.<br><br>

<img src="images/pset4/blobdetection-step_10.png" width="650px"> <br><br>

<b>Implementating Blob Detection: Finding Blobs in the Image</b><br>
Lucky for us, somebody else has already provided a library with the blob detection algorithm.<br>
The library automatically converts your grayscale image into black and white, performs blob detection, and then returns you the number of blobs, their locations in the image, as well as their outlines.<br><br>

You can find the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html"><b>BlobDetection Reference</b> here</a> for details on the different functions.<br><br>

First, construct a new object of the BlobDetection class using our interpolated PImage height and width.<br>
You can then use the <b>computeBlobs()</b> function to detect the blobs.<br>
Finally, the <b>getBlob()</b> and <b>getBlobNb()</b> functions can help you to retrieve the blobs.<br>
If your blob detection is not finding all the blobs in your image, consider using the <b>setThreshold()</b> function to determine which brightness levels from 0-255 should be taking into account when searching for blobs. Note that the threshold takes as input a float value from 0-1 so you need to scale your values accordingly.<br><br>

Implement this in the <b>drawBlobsAndEdges()</b> function in the skeleton code (note that we will do the drawing of the blob outline in the next step): <br><br>

<img src="images/pset4/drawBlobsAndEdges-skeleton-code.png" width="600px"> <br><br>

<b>Drawing Blob Centers and Contours Into the Image</b><br>

Next, we want to extract for each blob:
<ol>
	<li>the center of the blob (i.e., center of the touch point)</li>
	<li>the contour of blob (i.e., the outline of the touch point)</li>
</ol>

And then draw the touch point center, the touch point outline, and the touch point coordinates as (x,y) into our PImage.<br><br>

To extract the touch point and contour, you can go back to the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">documentation of the BlobDetection library</a>.<br>
The Blob class should give you all you need. <br>
Note that the contour is provided as a list of edges (i.e. lines).<br>
Drawing the contour, touch point, and text coordinates into the image is similar to what you already did in your pset1 Processing application.<br><br>


<b>Checking your Results</b><br>

Once you finished this part, you should be seeing something like this depending on if one or multiple fingers are touching: <br><br>

<img src="images/pset4/UI-blob-touch-1.png" width="370px"> 
<img src="images/pset4/UI-blob-touch-2.png" width="370px"> 
<img src="images/pset4/UI-blob-touch-3.png" width="370px"> <br>

<br>

<h3>Upload your Processing Code and Pictures of your Visualization</h3>

For grading, please upload the following to your google drive student folder:<br>

<ul>
	<li>the .pde file of your Processing program</li>
	<li>3 photos showing your Processing UI works with one, two, and three fingers touching the multi-touch pad (take the photos so that the multi-touch pad and Processing Window are seen at the same time)</li>
	<li>a short video showing your Processing UI works with one, two, and three fingers touching the multi-touch pad (take the video so that the multi-touch pad and Processing Window are seen at the same time)
</ul>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5 pts: you read the sensing data from the serial port into Processing and saved it correctly into a 2D array.</li>
	<li>5 pts: you recorded 2 seconds of noise, set the noise baseline correctly based on the recorded noise, and are substracting the noise baseline correctly from the incoming live signal during use</li>
	<li>5 pts: you created a grayscale image of the correct size and converted the sensor signals into the correct grayscale values using the map function.</li>
	<li>5 pts: you scaled the image correctly using bicubic interpolation</li>
	<li>5 pts: you implemented the blob detection and drew the touch center, outline, and coordinate into the visualization. You showed that your touch pad can recognize 1, 2, and 3 fingers touching correctly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
