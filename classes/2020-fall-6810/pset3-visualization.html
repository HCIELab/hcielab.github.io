<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="400px">
<img src="images/pset3/multi-touch_3-fingers.png" width="300px"> <br>

          <hr>


<h2 class="headline">Problem Set 3 (due Friday, Oct. 23, 2020, 11.59pm)</h2>

Now that you have the hardware ready and Arduino sensing code prepared, you will write some code for the visualization part of your multi-touch pad. In particular, you are going to do the following four steps:<br>

<ol>
	<li>Read the multi-touch sensing data (generated by your PSet2 Arduino code) from serial port and save them properly. </li>
	<li>Set the noise baseline.</li>
	<li>Set the background image color update and perform bicubic interpolation.</li>
	<li>Implement blob detection.</li>
</ol>
<br>

<h3>Skeleton Code</h3>

Start by downloading <a href="software/pset3_visualization_skeleton.zip">the skeleton code for the PSet3 from  here</a>. <br>
Before you can execute it, you first have to install two libraries (see next step). <br><br>

<h3>Install external libraries</h3>

For this PSet, we will be using two Processing libraries: 
<ol>
	<li>OpenCV (a library that helps with image processing, in our case we will draw our touch signals into an image, see picture at start of pset3)</li>
	<li>BlobDetection (a library that helps to find blobs inside of images, in our case the blobs are the touch points on the multi-touch pad)</li>
</ol>

You can install them directly in Processing by going to:<br>
<b>Sketch/Import Library/Add Library</b> and searching for their name.<br><br>

<img src="images/pset3/processing-library-opencv.png" width="370px">
<img src="images/pset3/processing-library-blob-detection.png" width="370px"> <br><br>


<h3>(1) Read the Multi-Touch Sensing Data into Processing</h3><br>

<b>Refresher from PSet2</b><br>
In pset2, you already wrote the <b>Arduino</b> code that reads the receiver pins from the multi-touch pad and writes the resulting data onto the serial port in the following way: <br><br>

<pre>
0,50,83,58,79,108,75,82,54   //columm0, row0val, row1val, row2val
1,55,92,120,84,63,61,88,53   //columm1, row0val, row1val, row2val
2,61,64,73,66,92,78,67,57
3,65,117,116,84,48,81,91,71
4,65,128,116,54,76,81,88,59
5,61,86,66,54,114,78,64,64
6,59,86,120,83,85,75,93,63
7,56,86,116,70,72,83,80,64
8,23,82,74,68,98,64,62,52
...
</pre> <br>

<b>Read Data from Serial Port Into Processing</b><br>
Next, you need to read this data from the serial port into Processing.<br>
We have already shown you how to read data from the serial port into Processing in Lab 1 & 2 and you can check your prior code from back then to see how to do it.<br><br>

In the skeleton code, put your code for reading the data into the <b>readSerial()</b> function: <br>
<img src="images/pset3/readSerial-skeleton-code.png" width="700px"> <br>

<b>Save Data for One Complete Scanning Pass into a 2D Array</b><br>
The data for a single pass on all columns and rows should be saved into a 2D array.<br>
When considering the size of your 2D array, remember that we build a 9x8 multi-touch pad.<br>
Printing the 2D array to the Processing command line should look something like this:<br><br>

<img src="images/pset3/serial-read-array.png" width="700px"> <br><br>

<span style="color:red">every time a new pass over all columns/rows starts, we first want to clear the 2D array, i.e. remove all the old data, and then write the data for the new pass into the 2D array.</span><br><br>

<h3>(2) Cleaning the Received Signal by Reducing Noise</h3><br>

<b>What is noise?</b><br>
As you may have noticed, even when you are not touching the multi-touch pad, the receiver pins almost always receive some value although you would expect that they should receive '0'.<br>
This is what we call "noise" in signal processing.<br>
Noise is an issue because it can lead to false positives, i.e. your code may think the multi-touch pad was touched although no interaction occured.<br>
To prevent this, we need to eliminate this noise in our signal.<br><br>

<b>How can we eliminate the noise?</b><br>
In order to eliminate the noise and have more "clean" data, there are many different processing steps that one can take.<br> 
In our case, we will implement a simple noise baseline filter.<br><br>

<b>How does a noise baseline filter work?</b><br>
A noise baseline filter works in two steps:<br><br>

<i>(1) Record Noise Baseline Signal:</i> First, you need to record the signal when you are <b>not</b> touching the multi-touch pad. The signal is recorded over some period of time and the recorded values are then averaged and saved as the noise baseline value. You only have to do this once at the beginning. In our case, we found that recording <b>2 seconds</b> of non-touch sensing data is sufficient for the multi-touch noise reduction. Note that we record the noise baseline for every received <b>value</b> of our multi-touch pad, i.e. we are having 8x9 = 72 values in total. Although we could use the same noise baseline for all our receiver signals, it is probably not a good idea to average everything together. Keep in mind that each receiver value will have its own noise due to various factors, including <span style="color:red">the resistance of the printed pad, the breadboard wire connection, the pin quality, environmental factors, and what else.</span>.<br>
Therefore, it is better to have a separate noise baseline for each receiver value.<br>
<span style="color:red">why different for each value? and not for each signal line?</span><br><br> 

<i>(2) Compare Usage Signal to Noise Baseline:</i> After you stored the noise baseline for each value, your multi-touch pad is ready for regular use. During interaction, the signal for each value is again recorded but this time you subtract the previously recorded noise baseline value from each new incoming signal value. Thus, if you don't touch the multi-touch pad, the noisy incoming signal minus the noisy baseline will equal 0 (or at least be close to it). Therefore, the chance of false positives is much smaller, i.e. the chance that your code thinks that a touch occured although there was none is greatly reduced.<br><br>  

Implement your noise baseline filtering in the <b>setBaseLine()</b> function and change the <b>boolean baseLineSet</b> in the skeleton code to "true" once the baseline is set: <br><br>

<img src="images/pset3/setBaseLine-skeleton-code.png" width="700px"> <br>

<span style="color:red">what does set baseline mean here? is this only averaging the noise or is this also subtracting the noise? if not, where am I subtracting the noise then?</span>

<!-- <h3>(3) Set the background image color and bicubic interpolation</h3> -->
<h3>(3) Preparing the Received Signals for Visualization</h3>

Now that we have "clean" sensing values, our next step is to find out where the multi-touch pad was actually touched.<br><br>

<b>Using Image Visualization for Detecting Touch Points</b><br>
While there are multiple different ways to find our where the multi-touch pad was touched, we will use an approach based on image-processing:<br>
First, we will draw the sensor signals at each [x,y] point of the multi-touch pad into an image.<br>
The higher the sensed signal, the brighter we will color the spot in the image.<br>
Thus, a touched point will occur as a bright white spot and a non-touched point will occur as a dark black spot in the image.<br>
While a single sensor value will just be a white pixel in the image, if we draw all sensor values together, we will see patterns of large white and black areas form.<br>
Once we have this, we will apply image processing techniques to extract white 'blobs' in the image, i.e. these are the points where the user touched.<br><br>

<img src="images/pset3/multi-touch_3-fingers-cropped.png" width="300px"><br><br>

<b>Preparing the Sensor Signals for Visualization</b><br>
For our visualization, we will construct an image (PImage) the same size as our 2D array that contains the sensing data.<br><br>

Before we can draw our sensor signals into the PImage, we have to make sure they have the correct range.<br>
The PImage in RGB color mode has pixels with color values from 0 - 255.<br>
We therefore have to scale our sensor signals accordingly.<br> 

To scale your sensor values to a range of 0 - 255, you can use <a href="https://processing.org/reference/map_.html">the <b>map()</b> function</a> in Processing.<br>
We recommend you don't use the full range up to 255 since <span style="color:"red">why not?</span><br>
Try out a few numbers to find the most suitable factors for you circuit so that the visualization is clear.<br>
<br>

You should be implementing this in the <b>setColors()</b> function in the skeleton code: <br>
<img src="images/pset3/setColors-skeleton-code.png" width="700px"> <br>

Once have the scaled values (in PImage), we will then interpolate them into a bigger and "smoother" plane (500px * 500px) to fit our display window (also 500px * 500px). <br>
We will implement this via bicubic interpolation. <br>

 we will scale them up/down into the values that distinguish them enough and optimized for visualization, and interpolate them into a bigger and "smoother" plane (in our case, 500px * 500px). <br><br>


<h3>Bicubic Interpolation</h3>

Recall from Lab 1 & 2, you have implemented a (simple) linear interpolation for the slider. <br>
The bicubic interpolation has the similar idea, except for it is in 2D, which makes it perfect for image processing. <br><br>

In image processing, bicubic interpolation is often chosen over bilinear or nearest-neighbor interpolation in image resampling, when speed is not an issue.<br> 
In contrast to (bi-) linear interpolation, which only takes 4 pixels (2×2) into account, bicubic interpolation considers 16 pixels (4×4). <br>
Images resampled with bicubic interpolation are smoother and have fewer interpolation artifacts. <br><br>

You can compare the difference between (bi-) linear & (bi-) cubic interpolations over the following interpolation results based on the same data points (5x5): <br>
<img src="images/pset3/linear-interpolation.png" width="300px"> 
<img src="images/pset3/cubic-interpolation.png" width="300px"> <br><br>

Luckily, you do not have to implement the whole interpolation calculation from scratch, as you can using the "wheels" from OpenCV. <br>

You should be implementing this in the <b>interpolate()</b> function in the skeleton code: <br>
<img src="images/pset3/interpolate-skeleton-code.png" width="700px"> <br>

Once you finished this part, you should be seeing the following when the multi-touch pad is not touch: <br>
<img src="images/pset3/UI-no-blob-no-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-no-touch-2.png" width="350px"> <br>

And something like this when touched, with the correct relative position "light up" (1 finger touching on the left, 2 fingers touching on the right): <br>
<img src="images/pset3/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-touch-2.png" width="350px"> <br>

<h3>(4) Implement blob detection</h3>

The last part of this PSet is to implement the "blob" detection for "touched location" of the multi-touch pad. <br>
The blob detection is implemented on top of the interpolated image (e.g. PImage of 500px * 500px) from part 3. <br><br>

You will be drawing 2 things on each detected blob: 1) the edge of the blob, and 2) the center point of the blob. <br>
The blob detection is implemented via the BlobDetection library. For more details, you can look at <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">here </a>. <br><br>

You should be implementing this in the <b>drawBlobsAndEdges()</b> function in the skeleton code: <br>
<img src="images/pset3/drawBlobsAndEdges-skeleton-code.png" width="700px"> <br>

Once you finished this part, you should be seeing the following differences compared to Part 3 (Part 3 vs Part 4, with one finger touching at the same location): <br>
<img src="images/pset3/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-1.png" width="350px"> <br><br>

When you have multiple fingers touching the multi-touch pad, you should see something similar to the following (2 fingers touching vs. 3 fingers touching): <br>
<img src="images/pset3/UI-blob-touch-2.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-3.png" width="350px"> <br>

<br>

<h3>Upload your Processing Code and pictures of Your visualization UI</h3>

For grading, please upload the following to your google drive student folder:<br>

<ul>
	<li>the .pde file of your Processing program</li>
	<li>3 photos showing your Processing UI works with one, two, and three fingers touching the multi-touch pad</li>
	<li>a short video showing the Processing program working, i.e. show that the Processing UI correctly identifying one, two, and three fingers respectively touching the multi-touch pad and displays at the correct locations. Make sure the Processing UI, your multitouch pad and your fingers are visible at the same time.</li>
</ul>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5 pts: you finished all for steps of the task.</li>
	<li>5 pts: read sensing data from serial port and save them correctly.</li>
	<li>5 pts: set the noise baseline correctly, e.g. record the first 2 sec of the sensing data as noise baseline.</li>
	<li>5 pts: update the background image color and perform bicubic interpolation correctly.</li>
	<li>5 pts: implement blob detection correctly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
