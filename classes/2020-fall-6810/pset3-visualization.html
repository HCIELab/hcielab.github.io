<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="400px">
<img src="images/pset3/multi-touch_3-fingers.png" width="300px"> <br>

          <hr>


<h2 class="headline">Problem Set 3 (due Friday, Oct. 23, 2020, 11.59pm)</h2>

Now that you have the hardware ready and Arduino sensing code prepared, you will write the Processing code that visualizes the sensor signals and extracts the touch point coordinates. To accomplish this, you will do the following five steps:<br>

<ol>
	<li>Read the multi-touch sensing data into Processing via the Serial Port.</li>
	<li>Clean the sensing data from noise by determining the noise baseline and subtracting it from the received signals.</li>
	<li>Convert the received signals into grayscale values that can be displayed as pixels in a 9x8 pixel image.</li>
	<li>Scale the image to 500x500px using bicubic interpolation to facilitate touch point extraction.</li>
	<li>Implement blob detection to extract the touch point locations.</li>
</ol>
<br>

<h3>Skeleton Code</h3>

Start by downloading <a href="software/pset3_visualization_skeleton.zip">the skeleton code for the PSet3 from  here</a>. <br>
Before you can execute it, you first have to install two libraries (see next step). <br><br>

<h3>Install external libraries</h3>

For this PSet, we will be using two Processing libraries: 
<ol>
	<li>OpenCV (a library that helps with image processing, in our case we will draw our touch signals into an image, see picture at start of pset3)</li>
	<li>BlobDetection (a library that helps to find blobs inside of images, in our case the blobs are the touch points on the multi-touch pad)</li>
</ol>

You can install them directly in Processing by going to:<br>
<b>Sketch/Import Library/Add Library</b> and searching for their name.<br><br>

<img src="images/pset3/processing-library-opencv.png" width="370px">
<img src="images/pset3/processing-library-blob-detection.png" width="370px"> <br><br>


<h3>(1) Read the Multi-Touch Sensing Data into Processing</h3><br>

<b>Refresher from PSet2</b><br>
In pset2, you already wrote the <b>Arduino</b> code that reads the receiver pins from the multi-touch pad and writes the resulting data onto the serial port in the following way: <br><br>

<pre>
0,50,83,58,79,108,75,82,54   //columm0, row0val, row1val, row2val
1,55,92,120,84,63,61,88,53   //columm1, row0val, row1val, row2val
2,61,64,73,66,92,78,67,57
3,65,117,116,84,48,81,91,71
4,65,128,116,54,76,81,88,59
5,61,86,66,54,114,78,64,64
6,59,86,120,83,85,75,93,63
7,56,86,116,70,72,83,80,64
8,23,82,74,68,98,64,62,52
...
</pre> <br>

<b>Read Data from Serial Port Into Processing</b><br>
Next, you need to read this data from the serial port into Processing.<br>
We have already shown you how to read data from the serial port into Processing in Lab 1 & 2 and you can check your prior code from back then to see how to do it.<br><br>

In the skeleton code, put your code for reading the data into the <b>readSerial()</b> function: <br>
<img src="images/pset3/readSerial-skeleton-code.png" width="700px"> <br>

<b>Save Data for One Complete Scanning Pass into a 2D Array</b><br>
The data for a single pass on all columns and rows should be saved into a 2D array.<br>
When considering the size of your 2D array, remember that we build a 9x8 multi-touch pad.<br>
Printing the 2D array to the Processing command line should look something like this:<br><br>

<img src="images/pset3/serial-read-array-one-pass.png" width="750px"> <br><br>

Every time a new pass over all columns/rows starts, you need to override your array values.<br><br>

<h3>(2) Cleaning the Received Signal by Reducing Noise</h3><br>

<b>What is noise?</b><br>
As you may have noticed, even when you are not touching the multi-touch pad, the receiver pins almost always receive some value although you would expect that they should receive '0'.<br>
This is what we call "noise" in signal processing.<br><br>

<b>What causes noise?</b><br>
Noise can be caused by a variety of factors.<br>
On the entire multi-touch pad, noise can be caused by the substrate the multi-touch pad is placed on, i.e. depending on if you place it on a glass table or a metal table the noise level will be different.<br>Additionally, noise can be different at each of the connection points in the circuit. For instance, each FPC connector and each wire may have different noise levels since the conductive lines all make slightly different levels of contact.<br>
Furthermore, since the inkjet printed circuit may not be perfectly consistent everywhere either because of uneven printing or because of different aging levels over time, the different printed paths may also have different levels of noise.<br>
Finally, in addition to the reasons above, we can also have different noise levels on a single row of electrodes because the distance between the receiver pin and each electrode in the row is different, thus the electodes further away accumulate noise over a larger distance.<br>
All of these factors together result in different noise levels at each point in your multi-touch pad.<br><br> 

<b>Why is noise an issue?</b><br>
Noise is an issue because it can lead to false positives, i.e. your code may think the multi-touch pad was touched although no interaction occured.<br>
To prevent this, we need to eliminate the noise in our signal.<br><br>

<b>How can we eliminate the noise?</b><br>
In order to eliminate the noise and have more "clean" data, there are many different processing steps that one can take.<br> 
In our case, we will implement a simple noise baseline filter.<br><br>

<b>How does a noise baseline filter work?</b><br>
A noise baseline filter works in two steps:<br><br>

<i>(1) Record Noise Baseline Signal:</i> First, you need to record the signal when you are <b>not</b> touching the multi-touch pad. The signal is recorded over some period of time and the recorded values are then averaged and saved as the noise baseline value. You only have to do this once at the beginning. In our case, we found that recording <b>2 seconds</b> of non-touch sensing data is sufficient for the multi-touch noise reduction. Note that we record the noise baseline for every position in our multi-touch pad for the reasons mentioned above, i.e. that every position has its own level of noise. We therefore record the noise baseline for every received <b>value</b> and thus we are having 9x8 = 72 noiseline base values in total.<br><br> 

Implement the noise baseline filtering in the <b>setBaseLine()</b> function and change the <b>boolean baseLineSet</b> in the skeleton code to "true" once the baseline is set: <br><br>

<img src="images/pset3/setBaseLine-skeleton-code.png" width="600px"> <br>

<i>(2) Subtract Noise Baseline from Usage Signal:</i> After you stored the noise baseline for each value, we can look into how to use it to create better signals during the actual user interaction. During interaction, the signal for each value is again recorded but this time you subtract the previously recorded noise baseline value from each new incoming signal value. Thus, if you don't touch the multi-touch pad, the noisy incoming signal minus the noise baseline will equal 0 (or at least be close to it). Therefore, the chance of false positives is much smaller, i.e. the chance that your code thinks that a touch occured although there was none is greatly reduced.<br><br>  

Implement the noise baseline substraction in the <b>subtractBaseLine()</b> function and <span style="color:red">what ever else todo.</span><br><br>

<span style="color:red">replace image with substractBaseline() function</span><br>
<img src="images/pset3/setBaseLine-skeleton-code.png" width="600px"> <br>


<!-- <h3>(3) Set the background image color and bicubic interpolation</h3> -->
<h3>(3) Converting the Received Signals into Grayscale Values that can be Displayed as an Image</h3>

Now that we have "clean" sensing values, our next step is to find out where the multi-touch pad was actually touched.<br><br>

<b>Using Image Processing for Detecting Touch Points</b><br>
While there are multiple different ways to find our where the multi-touch pad was touched, we will use an approach based on image-processing.<br>
First, we will convert our sensor signals into grayscale values and then draw the grayscale values into a 9x8 pixel image.<br> 
Brighter pixels will correspond to higher sensor signals (i.e. touch points) while darker pixels will correspond to areas that were not touched.<br>
Since image processing on such a small 9x8 pixel image is too difficult, we will increase the size of the image to 500x500px using bicubic interpolation to fill in for the missing pixel values.<br>
After this, we can use 'blob' detection to extra the white blobs in the image (i.e. the touch points) and get the touch coordinates from the blobs.<br>
Let's do this one step at a time.<br><br>

<img src="images/pset3/multi-touch_3-fingers-cropped.png" width="300px"><br><br>

<b>Creating the Image</b><br>
For our visualization, we will construct an image (PImage) the same size as our 2D array that contains the sensing data.<br>
Note that this PImage will be tiny (9x8 pixel) and you will likely have to search a bit for it on screen if you decide to display it for debugging purposes.<br><br>

<b>Creating a Grayscale Image</b><br>
The PImage will be gray-scale since for our purposes we are only interested in brighter vs. darker spots for the purpose of touch detection.<br>
If you set a PImage's pixels to a color value from 0 (black) - 255 (white), it will automatically be treated as grayscale.<br><br>

<b>Converting the Sensor Signals into Grayscale Values</b><br>

Before we can draw our sensor signals into the PImage as grayscale values, we have to make sure they have the correct range, i.e. are between 0-255.<br>
We therefore have to scale our sensor signals accordingly.<br><br> 

To scale your sensor values to a range of 0 - 255, you can use <a href="https://processing.org/reference/map_.html">the <b>map()</b> function</a> in Processing.<br>
As you can see in the map() function reference, we have set four different values:<br><br>

<img src="images/pset3/map-function-parameters.png" width="400px"> <br><br>

<b>start2/stop2 (target):</b> The last two parameters, i.e. our target range, are pretty straight forward since we already discussed we want to scale to 0 - 255.<br><br>

<b>start1/stop1 (source):</b> For the source parameter, use the highest sensor reading when you are not touching and lowest sensor reading when you are touching for now. We will revisit this later and it will become more clear why that is a good strategy.<br><br>

After you scaled them, use them to set the pixel color of each pixel in the 9x8 pixel PImage.<br>
Don't worry at this point if you can't see the effect in the image, just make sure all your pixel values are scaled to a value of 0-255.<br><br> 

Put your code into the <b>setColors()</b> function in the skeleton code: <br>
<img src="images/pset3/setColors-skeleton-code.png" width="700px"> <br>

<span style="color:red">Why is this colorMode RGB? very confusing, can you add some comments that we will use this as grayscale? remove the baseline subtraction from here. the comments are a bit confused in the code as well. what are the different values you are mentioning there? we should explain those values here as well?</span><br><br>

<h3>(4) Scaling up the Generated Image Using Bicubic Interpolation</h3>

<b>Why do we need to scale our image?</b><br>
While the tiny 9x8 pixel PImage that we just created is accurately representing our sensor values, <span style="color:red">(Junyi can you say better why we need to scale it up and can't leave it small?) it is too small to detect the touch positions.</span><br>
We therefore have to scale up the image to a size that is more suitable for further image processing, such as blob detection.<br>
In our case, we want to scale up the image to <b>500px x 500px.</b><br>
Since our original tiny 9x8 pixel image only contains 9x8 values, we need to figure out what values we should use for all the newly generated 'gaps' in the 500x500px image since it needs 500x500 values to be completely filled in at each pixel.<br>
To do this, we use a technique called interpolation.<br><br>

<b>What is interpolation?</b><br>
Interpolation can help fill in values where no actual values are present and can thus help us determine what value the 'in-between' pixels in our 500x500px image should have.<br>
There are many different interpolation techniques of different complexity levels that can be used to interpolate the pixel values in a way that it creates a smooth scaled up appearance of our original color values from our signals.<br>
For our case, we consider bi-linear and bi-cubic interpolation. <br><br>

<b>What is the difference between bi-linear and bi-cubic interpolation?</b><br>

<i>Bi-linear interpolation:</i> is performed using linear interpolation first in one direction, and then again in the other direction. For instance, in the example below where we want to determine the value of the central pixel, we first interpolate the value of the top and bottom pixel, and then interpolate the left and right pixel. We then average both the interpolate values to retrieve the final pixel value).<br><br>

<i>Bi-cubic interpolation:</i> is performed by taking into account <span style="color:red">16 (4x4) (really?)</span> neighboring pixels. To compute the needed pixel value, all values are <span style="color:red">averaged together?</span>

<br> <br>

<img src="images/pset3/bilinear-bicubic.png" width="700px"> <br>

Thus, a benefit of bicubic interpolation is that it gives you a more accurate and smoother interpolation result since it considers more values than bilinear interpolation. However, this also comes at the drawback of higher computational power needed since more values need to be processed to generate the interpolated value. For our small multi-touch pad with only a few hundred interpolated values this is not an issue but if you had to do more interpolation it may slow the processing down and the multi-touch pad would start lagging.<br><br>


You can see the difference of bi-liner vs. bi-cubic interpolation in the images below (these are from the internet and not from our multi-touch pad <span style="color:red">(Junyi: it would be a lot better to show the result at the example of our multi-touch pad being touched, if this is just a opencv function, can you take these pictures?)</span>). In the images the input data was 5x5px and it was scaled up to <span style="color:red">what image size?</span><br><br>

<img src="images/pset3/linear-interpolation.png" width="300px"> 
<img src="images/pset3/cubic-interpolation.png" width="300px"> <br><br>

<b>Implementing Bi-Cubic Interpolation with OpenCV</b><br>

Luckily, you do not have to implement the bicubic interpolation from scratch.<br>
There is an image processing and computer vision library called <a href="https://opencv.org/courses/"><b>OpenCV.</b></a><br>
OpenCV exists as a library for many different programming languages, in our case we will use OpenCV's implementation for Java and more specifically for Processing.<br>
Remember, at the beginning of this pset3, you already installed the <b>OpenCV library.</b><br>
If you scroll up in your skeleton code, you can see that we already imported several classes of the library with<br><br>

<pre>
import gab.opencv.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.core.Mat;
import org.opencv.core.Size;
</pre><br>

If you look <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/imgproc/Imgproc.html">at the documentation of the <b>Imgproc class,</b></a> you will see that it provides a static <b>resize()</b> function, which also allows us to specify which interpolation we want to use.<br><br>

<img src="images/pset3/imgproc-resize.png" width="750px"><br><br>

So let's see how we can fill out the parameters.<br><br>

<b>Mat src:</b> This is the input image (9x8px) that we want to resize. As you can see, the image here is in the format <b>Mat</b> (Matrix) and not PImage. We therefore first have to convert our PImage into a Mat. In image processing, images are handles as 2D matrices, where every matrix entry is the color value of one pixel (i.e. 0-255 in our case). You can also think of the 2D matrix as a 2D array with pixel values in rows and columns.<br>
Before we can convert our PImage into a Mat, we first have to construct a new OpenCV object.<br> 
For this, you need to call the corresponding OpenCV constructor in the <a href="http://atduskgreg.github.io/opencv-processing/reference/gab/opencv/OpenCV.html">OpenCV class documentation</a>.<br>
Once you constructed your OpenCV object, you can call the <b>getGray()</b> function on it to retrieve the image as a Mat (check the class documentation again). This is your src input parameter. <br><br>

<b>Mat dest:</b> This is the output image, i.e. the resized 500x500px image. Again it needs to be in the format of a matrix. For the output image, we want to create a new empty matrix of our desired size. You can find the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/core/Mat.html"><b>OpenCV Mat Reference</b> here</a>. 

<img src="images/pset3/mat-constructor.png" width="350px"><br>

Similar to how other variables have a type, e.g. int or float, matrices also have <a href="http://ninghang.blogspot.com/2012/11/list-of-mat-type-in-opencv.html">types,</a> which describe what types of values can be stored in them. Since we want to make sure the input  and the output images match, you need to reference the image type of your source matrix as the argument. If you check the Mat reference again, you can see that there is an instance function for getting the matrix type, which you should use as the argument. <br><br>

<b>Size dsize:</b> You can find the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/core/Size.html"><b>OpenCV Size</b> Reference</a> here. Construct a new Size object with 500x500px. <br><br>

<b>double fx, double fy:</b> These are scale factors but since we don't want to scale our image any further, we set these to 0.<br><br>

<b>int interpolation:</b> These are the different interpolation options. If you look at the different fields in the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/imgproc/Imgproc.html"><b>OpenCV Imgproc</a></b> Reference, you will see that there are several options, such as:<br>

<img src="images/pset3/interlinear.png" width="750px"><br>
<img src="images/pset3/intercubic.png" width="720px"><br><br>

We want to use the inter-cubic option as our last parameter.<br><br>

Implement this part in the <b>interpolate()</b> function in the skeleton code: <br><br>

<img src="images/pset3/interpolate-skeleton-code.png" width="600px"> <br><br>

<b>Convert Matrix back to PImage:</b> Finally, after you resized the image with our desired interpolation method, you have to convert your output matrix back into a PImage. For this, you can go back to the <a href="http://atduskgreg.github.io/opencv-processing/reference/"><b>OpenCV class</b> documentation</a> and look for the method that gives you a PImage.<br> This should also still go into the interpolate function in the skeleton code.<br><br>

<b>Checking Your Results:</b><br>

Once you finished the interpolation, you should be able to see images like the ones below. The image should be darker when the multi-touch pad is not touched and have some bright spots when the multi-touch pad is touched. What exactly you see depends on your specific noise level and how well your noise baseline works. <br><br>

<i>Not Touched:</i><br>
<img src="images/pset3/UI-no-blob-no-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-no-touch-2.png" width="350px"> <br><br>

<i>Touched:</i><br>
left: 1 finger touching, right: 2 fingers touching <br>
<img src="images/pset3/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-touch-2.png" width="350px"> <br><br>

<b>Improving your Results</b><br>
If your results don't look as good as above, try changing how you map your sensors signals to the values from 0 - 255. Remember, when you mapped your sensor signals to the 0 - 255 range, we had recommended that you use the highest sensor reading when not touching to map to 0 (black), and the lowest sensor reading when touching to 255 (white). Let's assume for a moment your lowest sensor reading when touching was 1500 and you mapped it to 255 (white). As a result, any sensor reading above 1500 is now leading to white pixels, whereas anything below will lead to darker pixels. If you find that you only see dark areas in your image and nothing is white, then perhaps the 1500 threshold is too high. Consider using a lower value to increase what signal values are being treated as 'touched' white pixels. Similarly, if you see too much noise in your image, i.e. everything is white, perhaps your non-touch sensor reading value is too low and you should set it to something higher, so that more signal values are treated as black non-touch pixels. Note that this will also change every time you place your multi-touch pad on a different subtrate (e.g. different surface) or if anything else in your environment undergoes a significant change (e.g. increase in humidity).<br><br>

<h3>(5) Implement blob detection</h3>

Finally, while you can now visually determine where the multi-touch pad was touched yourself, we also want to teach our program to do this automatically and tell us at which (x,y) coordinate it sees a touch point.<br><br>

To do this, we want to write code that detects the white 'blobs' in the image.<br>
This is called 'blob detection'.<br><br> 

<b>How does Blob Detection work?</b><br>
<span style="color:red">add some sentences on how blob detection works.</span><br><br> 

<b>Implementating Blob Detection: Finding Blobs in the Image</b><br>
Lucky for us, somebody else has already provided a library with the blob detection algorithm.<br>
You already installed that library at the beginning of this pset and we already imported it into the skeleton code here:<br>

<pre>import blobDetection.*;</pre>

Next, we want to extract the blobs in our image.<br>
You can see the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html"><b>BlobDetection Reference</b> here</a> for details on the different functions.<br>
First, construct a new object of the class using our interpolated PImage height and width.<br>
You can then use the <b>computeBlobs()</b> function to detect the blobs.<br>
Finally, the <b>getBlob()</b> and <b>getBlobNb()</b> functions can help you to retrieve the blobs.<br>
If your blob detection is not finding all the blobs in your image, consider using the <b>setThreshold()</b> function to determine which brightness levels from 0-255 should be taking into account when searching for blobs.<br><br>

Implement this in the <b>drawBlobsAndEdges()</b> function in the skeleton code (note that we will do the drawing in the next step): <br><br>

<img src="images/pset3/drawBlobsAndEdges-skeleton-code.png" width="600px"> <br><br>

<b>Drawing Blob Centers and Contours Into the Image</b><br>

Next, we want to extract for each blob:
<ol>
	<li>the center of the blob (i.e., center of the touch point)</li>
	<li>the contour of blob (i.e., the outline of the touch point)</li>
</ol>

And then draw the touch point, the finger outline, and the touch coordinates as (x,y) into our PImage.<br><br>

To extract the touch point and contour, you can go back to the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">documentation of the BlobDetection library</a>.<br>
The Blob class should give you all you need. <br>
Note that the contour is provided as a list of edges (i.e. lines).<br>
Drawing the contour, touch point, and text coordinates into the image is similar to what you already did in your pset1 Processing application.<br><br>


<b>Checking your Results</b><br>

Once you finished this part, you should be seeing something like this depending on if one or multiple fingers are touching: <br><br>

<span style="color:red">update images with touch coordinates (x,y)</span><br>
<img src="images/pset3/UI-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-2.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-3.png" width="350px"> <br>

<br>

<h3>Upload your Processing Code and pictures of Your visualization UI</h3>

For grading, please upload the following to your google drive student folder:<br>

<ul>
	<li>the .pde file of your Processing program</li>
	<li>3 photos showing your Processing UI works with one, two, and three fingers touching the multi-touch pad</li>
	<li>a short video showing the Processing program working, i.e. show that the Processing UI correctly identifying one, two, and three fingers respectively touching the multi-touch pad and displays at the correct locations. Make sure the Processing UI, your multitouch pad and your fingers are visible at the same time.</li>
</ul>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5 pts: you finished all for steps of the task.</li>
	<li>5 pts: read sensing data from serial port and save them correctly.</li>
	<li>5 pts: set the noise baseline correctly, e.g. record the first 2 sec of the sensing data as noise baseline.</li>
	<li>5 pts: update the background image color and perform bicubic interpolation correctly.</li>
	<li>5 pts: implement blob detection correctly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
