<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="400px">
<img src="images/pset3/multi-touch_3-fingers.png" width="300px"> <br>

          <hr>


<h2 class="headline">Problem Set 3 (due Friday, Oct. 23, 2020, 11.59pm)</h2>

Now that you have the hardware ready and Arduino sensing code prepared, you will write some code for the visualization part of your multi-touch pad. In particular, you are going to do the following four steps:<br>

<ol>
	<li>Read the multi-touch sensing data (generated by your PSet2 Arduino code) from serial port and save them properly. </li>
	<li>Set the noise baseline.</li>
	<li>Set the background image color update and perform bicubic interpolation.</li>
	<li>Implement blob detection.</li>
</ol>
<br>

<h3>Skeleton Code</h3>

Start by downloading <a href="software/pset3_visualization_skeleton.zip">the skeleton code for the PSet3 from  here</a>. <br>
Before you can execute it, you first have to install two libraries (see next step). <br><br>

<h3>Install external libraries</h3>

For this PSet, we will be using two Processing libraries: 
<ol>
	<li>OpenCV (a library that helps with image processing, in our case we will draw our touch signals into an image, see picture at start of pset3)</li>
	<li>BlobDetection (a library that helps to find blobs inside of images, in our case the blobs are the touch points on the multi-touch pad)</li>
</ol>

You can install them directly in Processing by going to:<br>
<b>Sketch/Import Library/Add Library</b> and searching for their name.<br><br>

<img src="images/pset3/processing-library-opencv.png" width="370px">
<img src="images/pset3/processing-library-blob-detection.png" width="370px"> <br><br>


<h3>(1) Read the Multi-Touch Sensing Data into Processing</h3><br>

<b>Refresher from PSet2</b><br>
In pset2, you already wrote the <b>Arduino</b> code that reads the receiver pins from the multi-touch pad and writes the resulting data onto the serial port in the following way: <br><br>

<pre>
0,50,83,58,79,108,75,82,54   //columm0, row0val, row1val, row2val
1,55,92,120,84,63,61,88,53   //columm1, row0val, row1val, row2val
2,61,64,73,66,92,78,67,57
3,65,117,116,84,48,81,91,71
4,65,128,116,54,76,81,88,59
5,61,86,66,54,114,78,64,64
6,59,86,120,83,85,75,93,63
7,56,86,116,70,72,83,80,64
8,23,82,74,68,98,64,62,52
...
</pre> <br>

<b>Read Data from Serial Port Into Processing</b><br>
Next, you need to read this data from the serial port into Processing.<br>
We have already shown you how to read data from the serial port into Processing in Lab 1 & 2 and you can check your prior code from back then to see how to do it.<br><br>

In the skeleton code, put your code for reading the data into the <b>readSerial()</b> function: <br>
<img src="images/pset3/readSerial-skeleton-code.png" width="700px"> <br>

<b>Save Data for One Complete Scanning Pass into a 2D Array</b><br>
The data for a single pass on all columns and rows should be saved into a 2D array.<br>
When considering the size of your 2D array, remember that we build a 9x8 multi-touch pad.<br>
Printing the 2D array to the Processing command line should look something like this:<br><br>

<img src="images/pset3/serial-read-array-one-pass.png" width="750px"> <br><br>

Every time a new pass over all columns/rows starts, you need to override your array values.<br><br>

<h3>(2) Cleaning the Received Signal by Reducing Noise</h3><br>

<b>What is noise?</b><br>
As you may have noticed, even when you are not touching the multi-touch pad, the receiver pins almost always receive some value although you would expect that they should receive '0'.<br>
This is what we call "noise" in signal processing.<br><br>

<b>What causes noise?</b><br>
Noise can be caused by a variety of factors.<br>
On the entire multi-touch pad, noise can be caused by the substrate the multi-touch pad is placed on, i.e. if you place it on a glass table or a metal table.<br>Additionally, noise can be different at each of the connection points in the circuit. For instance, each FPC connector and each wire may have different noise since the conductive lines all make slightly different levels of contact.<br>
Furthermore, since the inkjet printed circuit may not be perfectly consistent everywhere either because of uneven printing or because of different aging levels over time, the different printed paths may also have different levels of noise.<br>
Finally, in addition to the reasons above, we can also have different noise levels on a single row of electrodes because the distance between each electrode and the receiver pin is different, thus accumulating noise over a larger distance.<br>
All of these factors together result in different noise levels at each point in your multi-touch pad.<br><br> 

<b>Why is noise an issue?</b><br>
Noise is an issue because it can lead to false positives, i.e. your code may think the multi-touch pad was touched although no interaction occured.<br>
To prevent this, we need to eliminate this noise in our signal.<br><br>

<b>How can we eliminate the noise?</b><br>
In order to eliminate the noise and have more "clean" data, there are many different processing steps that one can take.<br> 
In our case, we will implement a simple noise baseline filter.<br><br>

<b>How does a noise baseline filter work?</b><br>
A noise baseline filter works in two steps:<br><br>

<i>(1) Record Noise Baseline Signal:</i> First, you need to record the signal when you are <b>not</b> touching the multi-touch pad. The signal is recorded over some period of time and the recorded values are then averaged and saved as the noise baseline value. You only have to do this once at the beginning. In our case, we found that recording <b>2 seconds</b> of non-touch sensing data is sufficient for the multi-touch noise reduction. Note that we record the noise baseline for every position in our multi-touch pad, i.e. every received value and thus we are having 8x9 = 72 noiseline base values in total.<br><br> 

<i>(2) Compare Usage Signal to Noise Baseline:</i> After you stored the noise baseline for each value, your multi-touch pad is ready for regular use. During interaction, the signal for each value is again recorded but this time you subtract the previously recorded noise baseline value from each new incoming signal value. Thus, if you don't touch the multi-touch pad, the noisy incoming signal minus the noisy baseline will equal 0 (or at least be close to it). Therefore, the chance of false positives is much smaller, i.e. the chance that your code thinks that a touch occured although there was none is greatly reduced.<br><br>  

Implement your noise baseline filtering in the <b>setBaseLine()</b> function and change the <b>boolean baseLineSet</b> in the skeleton code to "true" once the baseline is set: <br><br>

<img src="images/pset3/setBaseLine-skeleton-code.png" width="700px"> <br>

<span style="color:red">what does set baseline mean here? is this only averaging the noise or is this also subtracting the noise? if not, where am I subtracting the noise then?</span>

<!-- <h3>(3) Set the background image color and bicubic interpolation</h3> -->
<h3>(3) Preparing the Received Signals for Visualization</h3>

Now that we have "clean" sensing values, our next step is to find out where the multi-touch pad was actually touched.<br><br>

<b>Using Image Visualization for Detecting Touch Points</b><br>
While there are multiple different ways to find our where the multi-touch pad was touched, we will use an approach based on image-processing:<br>
First, we will draw the sensor signals at each [x,y] point of the multi-touch pad into an image.<br>
The higher the sensed signal, the brighter we will color the spot in the image.<br>
Thus, a touched point will occur as a bright white spot and a non-touched point will occur as a dark black spot in the image.<br>
While a single sensor value will just be a white pixel in the image, if we draw all sensor values together, we will see patterns of large white and black areas form.<br>
Once we have this, we will apply image processing techniques to extract white 'blobs' in the image, i.e. these are the points where the user touched.<br><br>

<img src="images/pset3/multi-touch_3-fingers-cropped.png" width="300px"><br><br>

<b>Preparing the Sensor Signals for Visualization</b><br>
For our visualization, we will construct an image (PImage) the same size as our 2D array that contains the sensing data.<br>
<span style="color:red">Note that this PImage will be tiny (8-9 pixel) and you will likely have to search a bit for it on screen.</span>
Before we can draw our sensor signals into the PImage, we have to make sure they have the correct range.<br>
The PImage in RGB color mode has pixels with color values from 0 - 255.<br>
We therefore have to scale our sensor signals accordingly.<br> 

To scale your sensor values to a range of 0 - 255, you can use <a href="https://processing.org/reference/map_.html">the <b>map()</b> function</a> in Processing.<br>
<span style="color:red">Use the scaled sensor value for each of the R, G, and B channels.</span><br>
We recommend you don't use the full range up to 255 since <span style="color:red">why not?</span><br>
Try out a few numbers to find the most suitable factors for you circuit so that the visualization is clear.<br>
<br>

Put your code into the <b>setColors()</b> function in the skeleton code: <br>
<img src="images/pset3/setColors-skeleton-code.png" width="700px"> <br>
<span style="color:red">baseline subtraction here? the comments are a bit confused in the code as well. we should explain those values here as well.</span><br><br>

<h3>(4) Scaling up the Generated Image Using Bicubic Interpolation</h3>

While the tiny 8x9 pixel PImage that we just created is accurately representing our sensor values, <span style="color:red">it is too small for blob detection.</span><br>
We therefore have to scale up the image to a size that is more suitable for further image processing, such as blob detection.<br>
In our case, we want to scale up the image to <b>500px x 500px.</b><br><br>

<b>How can we scale the image?</b><br>
Simply scaling the image using a function, such as <span style="color:red">what Processing function would be the simplest way to scale?</span> will lead to a result <span style="color:red">how would that look like? a lot of discrete pixels?</span>
<br> 
To create a 'smoother' scaled appearance of the drawn color values from our signal, we therefore use a different approach based on interpolation.<br><br>

<b>What is interpolation?</b><br>
Interpolation can help fill in values where no actual values are present.<br>
Recall from Lab 1 & 2 that when you implemented the slider, you had 5 discrete sensor signals from the slider segments and needed to create a continuous signal over the entire slider length. For this, you used a simple linear interpolation.<br>
Similarly, in this pset, we have discrete sensor signal values in the resolution of 8x9 and need to provide a continous signal over an area of 500px x 500px, i.e. the size of the image. <span style="color:red"> The bicubic interpolation has the similar idea, except for it is in 2D, which makes it perfect for image processing. (this doesn't make much sense to me, couldn't I not also use linear interpolation for 2D? or do you mean it interpolates in two direction simultaenously?)</span><br><br>


<b>What is bicubic interpolation?</b><br>

Bi-Cubic interpolation works by taking the neighboring 16 pixels 


In image processing, bicubic interpolation is often chosen over bilinear or nearest-neighbor interpolation in image resampling, when speed is not an issue.<br> 
In contrast to (bi-) linear interpolation, which only takes 4 pixels (2×2) into account, bicubic interpolation considers 16 pixels (4×4). <br>
Images resampled with bicubic interpolation are smoother and have fewer interpolation artifacts. <br><br>

You can compare the difference between (bi-) linear & (bi-) cubic interpolations over the following interpolation results based on the same data points (5x5): <br>
<img src="images/pset3/linear-interpolation.png" width="300px"> 
<img src="images/pset3/cubic-interpolation.png" width="300px"> <br><br>

Luckily, you do not have to implement the whole interpolation calculation from scratch, as you can using the "wheels" from OpenCV. <br>

You should be implementing this in the <b>interpolate()</b> function in the skeleton code: <br>
<img src="images/pset3/interpolate-skeleton-code.png" width="700px"> <br>

Once you finished this part, you should be seeing the following when the multi-touch pad is not touch: <br>
<img src="images/pset3/UI-no-blob-no-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-no-touch-2.png" width="350px"> <br>

And something like this when touched, with the correct relative position "light up" (1 finger touching on the left, 2 fingers touching on the right): <br>
<img src="images/pset3/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-touch-2.png" width="350px"> <br>

<h3>(4) Implement blob detection</h3>

The last part of this PSet is to implement the "blob" detection for "touched location" of the multi-touch pad. <br>
The blob detection is implemented on top of the interpolated image (e.g. PImage of 500px * 500px) from part 3. <br><br>

You will be drawing 2 things on each detected blob: 1) the edge of the blob, and 2) the center point of the blob. <br>
The blob detection is implemented via the BlobDetection library. For more details, you can look at <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">here </a>. <br><br>

You should be implementing this in the <b>drawBlobsAndEdges()</b> function in the skeleton code: <br>
<img src="images/pset3/drawBlobsAndEdges-skeleton-code.png" width="700px"> <br>

Once you finished this part, you should be seeing the following differences compared to Part 3 (Part 3 vs Part 4, with one finger touching at the same location): <br>
<img src="images/pset3/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-1.png" width="350px"> <br><br>

When you have multiple fingers touching the multi-touch pad, you should see something similar to the following (2 fingers touching vs. 3 fingers touching): <br>
<img src="images/pset3/UI-blob-touch-2.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-3.png" width="350px"> <br>

<br>

<h3>Upload your Processing Code and pictures of Your visualization UI</h3>

For grading, please upload the following to your google drive student folder:<br>

<ul>
	<li>the .pde file of your Processing program</li>
	<li>3 photos showing your Processing UI works with one, two, and three fingers touching the multi-touch pad</li>
	<li>a short video showing the Processing program working, i.e. show that the Processing UI correctly identifying one, two, and three fingers respectively touching the multi-touch pad and displays at the correct locations. Make sure the Processing UI, your multitouch pad and your fingers are visible at the same time.</li>
</ul>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5 pts: you finished all for steps of the task.</li>
	<li>5 pts: read sensing data from serial port and save them correctly.</li>
	<li>5 pts: set the noise baseline correctly, e.g. record the first 2 sec of the sensing data as noise baseline.</li>
	<li>5 pts: update the background image color and perform bicubic interpolation correctly.</li>
	<li>5 pts: implement blob detection correctly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
