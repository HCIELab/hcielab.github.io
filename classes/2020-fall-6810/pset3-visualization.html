<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="400px">
<img src="images/pset3/multi-touch_3-fingers.png" width="300px"> <br>

          <hr>


<h2 class="headline">Problem Set 3 (due Friday, Oct. 23, 2020, 11.59pm)</h2>

Now that you have the hardware ready and Arduino sensing code prepared, you will write some code for the visualization part of your multi-touch pad. In particular, you are going to do the following four steps:<br>

<ol>
	<li>Read the multi-touch sensing data (generated by your PSet2 Arduino code) from serial port and save them properly. </li>
	<li>Set the noise baseline.</li>
	<li>Set the background image color update and perform bicubic interpolation.</li>
	<li>Implement blob detection.</li>
</ol>
<br>

<h3>Skeleton Code</h3>

Start by downloading <a href="software/pset3_visualization_skeleton.zip">the skeleton code for the PSet3 from  here</a>. <br>
Before you can execute it, you first have to install two libraries (see next step). <br><br>

<h3>Install external libraries</h3>

For this PSet, we will be using two Processing libraries: 
<ol>
	<li>OpenCV (a library that helps with image processing, in our case we will draw our touch signals into an image, see picture at start of pset3)</li>
	<li>BlobDetection (a library that helps to find blobs inside of images, in our case the blobs are the touch points on the multi-touch pad)</li>
</ol>

You can install them directly in Processing by going to:<br>
<b>Sketch/Import Library/Add Library</b> and searching for their name.<br><br>

<img src="images/pset3/processing-library-opencv.png" width="370px">
<img src="images/pset3/processing-library-blob-detection.png" width="370px"> <br><br>


<h3>(1) Read the Multi-Touch Sensing Data into Processing</h3><br>

<b>Refresher from PSet2</b><br>
In pset2, you already wrote the <b>Arduino</b> code that reads the receiver pins from the multi-touch pad and writes the resulting data onto the serial port in the following way: <br><br>

<pre>
0,50,83,58,79,108,75,82,54   //columm0, row0val, row1val, row2val
1,55,92,120,84,63,61,88,53   //columm1, row0val, row1val, row2val
2,61,64,73,66,92,78,67,57
3,65,117,116,84,48,81,91,71
4,65,128,116,54,76,81,88,59
5,61,86,66,54,114,78,64,64
6,59,86,120,83,85,75,93,63
7,56,86,116,70,72,83,80,64
8,23,82,74,68,98,64,62,52
...
</pre> <br>

<b>Read Data from Serial Port Into Processing</b><br>
Next, you need to read this data from the serial port into Processing.<br>
We have already shown you how to read data from the serial port into Processing in Lab 1 & 2 and you can check your prior code from back then to see how to do it.<br><br>

In the skeleton code, put your code for reading the data into the <b>readSerial()</b> function: <br>
<img src="images/pset3/readSerial-skeleton-code.png" width="700px"> <br>

<b>Save Data for One Complete Scanning Pass into a 2D Array</b><br>
The data for a single pass on all columns and rows should be saved into a 2D array.<br>
When considering the size of your 2D array, remember that we build a 9x8 multi-touch pad.<br>
Printing the 2D array to the Processing command line should look something like this:<br><br>

<img src="images/pset3/serial-read-array-one-pass.png" width="750px"> <br><br>

Every time a new pass over all columns/rows starts, you need to override your array values.<br><br>

<h3>(2) Cleaning the Received Signal by Reducing Noise</h3><br>

<b>What is noise?</b><br>
As you may have noticed, even when you are not touching the multi-touch pad, the receiver pins almost always receive some value although you would expect that they should receive '0'.<br>
This is what we call "noise" in signal processing.<br><br>

<b>What causes noise?</b><br>
Noise can be caused by a variety of factors.<br>
On the entire multi-touch pad, noise can be caused by the substrate the multi-touch pad is placed on, i.e. if you place it on a glass table or a metal table.<br>Additionally, noise can be different at each of the connection points in the circuit. For instance, each FPC connector and each wire may have different noise since the conductive lines all make slightly different levels of contact.<br>
Furthermore, since the inkjet printed circuit may not be perfectly consistent everywhere either because of uneven printing or because of different aging levels over time, the different printed paths may also have different levels of noise.<br>
Finally, in addition to the reasons above, we can also have different noise levels on a single row of electrodes because the distance between each electrode and the receiver pin is different, thus accumulating noise over a larger distance.<br>
All of these factors together result in different noise levels at each point in your multi-touch pad.<br><br> 

<b>Why is noise an issue?</b><br>
Noise is an issue because it can lead to false positives, i.e. your code may think the multi-touch pad was touched although no interaction occured.<br>
To prevent this, we need to eliminate this noise in our signal.<br><br>

<b>How can we eliminate the noise?</b><br>
In order to eliminate the noise and have more "clean" data, there are many different processing steps that one can take.<br> 
In our case, we will implement a simple noise baseline filter.<br><br>

<b>How does a noise baseline filter work?</b><br>
A noise baseline filter works in two steps:<br><br>

<i>(1) Record Noise Baseline Signal:</i> First, you need to record the signal when you are <b>not</b> touching the multi-touch pad. The signal is recorded over some period of time and the recorded values are then averaged and saved as the noise baseline value. You only have to do this once at the beginning. In our case, we found that recording <b>2 seconds</b> of non-touch sensing data is sufficient for the multi-touch noise reduction. Note that we record the noise baseline for every position in our multi-touch pad, i.e. every received value and thus we are having 9x8 = 72 noiseline base values in total.<br><br> 

Implement the noise baseline filtering in the <b>setBaseLine()</b> function and change the <b>boolean baseLineSet</b> in the skeleton code to "true" once the baseline is set: <br><br>

<img src="images/pset3/setBaseLine-skeleton-code.png" width="600px"> <br>

<i>(2) Subtract Noise Baseline from Usage Signal:</i> After you stored the noise baseline for each value, your multi-touch pad is ready for regular use. During interaction, the signal for each value is again recorded but this time you subtract the previously recorded noise baseline value from each new incoming signal value. Thus, if you don't touch the multi-touch pad, the noisy incoming signal minus the noisy baseline will equal 0 (or at least be close to it). Therefore, the chance of false positives is much smaller, i.e. the chance that your code thinks that a touch occured although there was none is greatly reduced.<br><br>  

Implement the noise baseline substraction in the <b>subtractBaseLine()</b> function and <span style="color:red">what ever else todo.</span><br><br>

<span style="color:red">replace image with substractBaseline() function</span><br>
<img src="images/pset3/setBaseLine-skeleton-code.png" width="600px"> <br>


<!-- <h3>(3) Set the background image color and bicubic interpolation</h3> -->
<h3>(3) Preparing the Received Signals for Visualization</h3>

Now that we have "clean" sensing values, our next step is to find out where the multi-touch pad was actually touched.<br><br>

<b>Using Image Visualization for Detecting Touch Points</b><br>
While there are multiple different ways to find our where the multi-touch pad was touched, we will use an approach based on image-processing:<br>
First, we will draw the sensor signals at each [x,y] point of the multi-touch pad into an image.<br>
The higher the sensed signal, the brighter we will color the spot in the image.<br>
Thus, a touched point will occur as a bright white spot and a non-touched point will occur as a dark black spot in the image.<br>
While a single sensor value will just be a white pixel in the image, if we draw all sensor values together, we will see patterns of large white and black areas form.<br>
Once we have this, we will apply image processing techniques to extract white 'blobs' in the image, i.e. these are the points where the user touched.<br><br>

<img src="images/pset3/multi-touch_3-fingers-cropped.png" width="300px"><br><br>

<b>Creating the Image</b><br>
For our visualization, we will construct an image (PImage) the same size as our 2D array that contains the sensing data.<br>
Note that this PImage will be tiny (9x8 pixel) and you will likely have to search a bit for it on screen if you decide to display it for debugging purposes.<br><br>

<b>Creating a Grayscale Image</b><br>
We will draw into the PImage in grayscale.<br>
The PImage pixels will have color values from 0 - 255.<br>
A higher pixel value will result into a brighter color (255 = white) and a lower pixel value will result into a darker color (0 = black).<br> 
You can set a pixel to a grayscale value by assigning the pixel a single value between 0-255 (rather than using the RGB notation of RGB(value1, value2, value3)).<br>
We recommend you test first if you can set pixel values to grayscale values before moving on to integrate the sensor data.<br><br>

<b>Converting the Sensor Signals into Grayscale Values</b><br>

Before we can draw our sensor signals into the PImage as grayscale values, we have to make sure they have the correct range, i.e. are between 0-255.<br>
We therefore have to scale our sensor signals accordingly.<br><br> 

To scale your sensor values to a range of 0 - 255, you can use <a href="https://processing.org/reference/map_.html">the <b>map()</b> function</a> in Processing.<br>
After you scaled them, use them to set the pixel color of each pixel in the 9x8 pixel PImage.<br>
Don't worry at this point if you can't see the effect in the image, just make sure all your pixel values are scaled to a value of 0-255.<br><br> 

<span style="color:red">I recommend we do the 'figure out which scaling value' later since the students don't really see anything in the 9x8 pixel image so asking them to try to get a 'clear' visualization may not work.</span>.<br><br>

Put your code into the <b>setColors()</b> function in the skeleton code: <br>
<img src="images/pset3/setColors-skeleton-code.png" width="700px"> <br>

<span style="color:red">Why is this colorMode RGB? very confusing, can you add some comments that we will use this as grayscale? remove the baseline subtraction from here. the comments are a bit confused in the code as well. what are the different values you are mentioning there? we should explain those values here as well?</span><br><br>

<h3>(4) Scaling up the Generated Image Using Bicubic Interpolation</h3>

<b>Why do we need to scale our image?</b><br>
While the tiny 9x8 pixel PImage that we just created is accurately representing our sensor values, <span style="color:red">(Junyi can you say better why we need to scale it up and can't leave it small?) it is too small to detect the touch positions.</span><br>
We therefore have to scale up the image to a size that is more suitable for further image processing, such as blob detection.<br>
In our case, we want to scale up the image to <b>500px x 500px.</b><br>
Since our original tiny 9x8 pixel image only contains 9x8 values, we need to figure out what values we should use for all the newly generated 'gaps' in the 500x500px image since it needs 500x500 values to be completely filled in at each pixel.<br>
To do this, we use a technique called interpolation.<br><br>

<b>What is interpolation?</b><br>
Interpolation can help fill in values where no actual values are present and can thus help us determine what value the 'in-between' pixels in our 500x500px image should have.<br>
There are many different interpolation techniques of different complexity levels that can be used to interpolate the pixel values in a way that it creates a smooth scaled up appearance of our original color values from our signals.<br>
For our case, we consider bi-linear and bi-cubic interpolation. <br><br>

<b>What is the difference between bi-linear and bi-cubic interpolation?</b><br>

<i>Bi-linear interpolation:</i> is performed using linear interpolation first in one direction, and then again in the other direction. For instance, in the example below where we want to determine the value of the central pixel, we first interpolate the value of the top and bottom pixel, and then interpolate the left and right pixel. We then average both the interpolate values to retrieve the final pixel value).<br><br>

<i>Bi-cubic interpolation:</i> is performed by taking into account <span style="color:red">16 (4x4) (really?)</span> neighboring pixels. To compute the needed pixel value, all values are <span style="color:red">averaged together?</span>

<br> <br>

<img src="images/pset3/bilinear-bicubic.png" width="700px"> <br>

Thus, a benefit of bicubic interpolation is that it gives you a more accurate and smoother interpolation result since it considers more values than bilinear interpolation. However, this also comes at the drawback of higher computational power needed since more values need to be processed to generate the interpolated value. For our small multi-touch pad with only a few hundred interpolated values this is not an issue but if you had to do more interpolation it may slow the processing down and the multi-touch pad would start lagging.<br><br>


You can see the difference of bi-liner vs. bi-cubic interpolation in the images below (these are from the internet and not from our multi-touch pad <span style="color:red">(Junyi: it would be a lot better to show the result at the example of our multi-touch pad being touched, if this is just a opencv function, can you take these pictures?)</span>). In the images the input data was 5x5px and it was scaled up to <span style="color:red">what image size?</span><br><br>

<img src="images/pset3/linear-interpolation.png" width="300px"> 
<img src="images/pset3/cubic-interpolation.png" width="300px"> <br><br>

<b>Implementing Bi-Cubic Interpolation with OpenCV</b><br>

Luckily, you do not have to implement the bicubic interpolation from scratch.<br>
There is an image processing and computer vision library called <a href="https://opencv.org/courses/"><b>OpenCV.</b></a><br>
OpenCV exists as a library for many different programming languages, in our case we will use OpenCV's implementation for Java and more specifically for Processing.<br>
Remember, at the beginning of this pset3, you already installed the <b>OpenCV library.</b><br>
If you scroll up in your skeleton code, you can see that we already imported several classes of the library with<br><br>

<pre>
import gab.opencv.*;
import org.opencv.imgproc.Imgproc;
import org.opencv.core.Mat;
import org.opencv.core.Size;
</pre><br>

If you look <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/imgproc/Imgproc.html">at the documentation of the <b>Imgproc class,</b></a> you will see that it provides a static <b>resize()</b> function, which also allows us to specify which interpolation we want to use.<br><br>

<img src="images/pset3/imgproc-resize.png" width="750px"><br><br>

So let's see how we can fill out the parameters.<br><br>

<b>Mat src:</b> This is the input image (9x8px) that we want to resize. As you can see, the image here is in the format <b>Mat</b> (Matrix) and not PImage. We therefore first have to convert our PImage into a Mat. In image processing, images are handles as 2D matrices, where every matrix entry is the color value of one pixel (i.e. 0-255 in our case). You can also think of the 2D matrix as a 2D array with pixel values in rows and columns.<br>
Before we can convert our PImage into a Mat, we first have to construct a new OpenCV object.<br> 
For this, you need to call the corresponding OpenCV constructor in the <a href="http://atduskgreg.github.io/opencv-processing/reference/gab/opencv/OpenCV.html">OpenCV class documentation</a>.<br>
Once you constructed your OpenCV object, you can call the <b>getGray()</b> function on it to retrieve the image as a Mat (check the class documentation again). This is your src input parameter. <br><br>

<b>Mat dest:</b> This is the output image, i.e. the resized 500x500px image. Again it needs to be in the format of a matrix. For the output image, we want to create a new empty matrix of our desired size. You can find the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/core/Mat.html"><b>OpenCV Mat Reference</b> here</a>. 

<img src="images/pset3/mat-constructor.png" width="350px"><br>

Similar to how other variables have a type, e.g. int or float, matrices also have <a href="http://ninghang.blogspot.com/2012/11/list-of-mat-type-in-opencv.html">types,</a> which describe what types of values can be stored in them. Since we want to make sure the input  and the output images match, you need to reference the image type of your source matrix as the argument. If you check the Mat reference again, you can see that there is an instance function for getting the matrix type, which you should use as the argument. <br><br>

<b>Size dsize:</b> You can find the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/core/Size.html"><b>OpenCV Size</b> Reference</a> here. Construct a new Size object with 500x500px. <br><br>

<b>double fx, double fy:</b> These are scale factors but since we don't want to scale our image any further, we set these to 0.<br><br>

<b>int interpolation:</b> These are the different interpolation options. If you look at the different fields in the <a href="https://docs.opencv.org/3.4/javadoc/org/opencv/imgproc/Imgproc.html"><b>OpenCV Imgproc</a></b> Reference, you will see that there are several options, such as:<br>

<img src="images/pset3/interlinear.png" width="750px"><br>
<img src="images/pset3/intercubic.png" width="720px"><br><br>

We want to use the inter-cubic option as our last parameter.<br><br>

Implement this part in the <b>interpolate()</b> function in the skeleton code: <br><br>

<img src="images/pset3/interpolate-skeleton-code.png" width="600px"> <br><br>

<b>Convert Matrix back to PImage:</b> Finally, after you resized the image with our desired interpolation method, you have to convert your output matrix back into a PImage. For this, you can go back to the <a href="http://atduskgreg.github.io/opencv-processing/reference/"><b>OpenCV class</b> documentation</a> and look for the method that gives you a PImage.<br> This should also still go into the interpolate function in the skeleton code.<br><br>

<b>Checking Your Results:</b><br>

Once you finished the interpolation, you should be able to see images like the ones below. The image should be darker when the multi-touch pad is not touched and have some bright spots when the multi-touch pad is touched. What exactly you see depends on your specific noise level and how well your noise baseline works. <br><br>

<i>Not Touched:</i><br>
<img src="images/pset3/UI-no-blob-no-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-no-touch-2.png" width="350px"> <br><br>

<i>Touched:</i><br>
left: 1 finger touching, right: 2 fingers touching <br>
<img src="images/pset3/UI-no-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-no-blob-touch-2.png" width="350px"> <br><br>

<b>Improving your Results</b><br>
If your results don't look as good as above, try changing how you map your sensors signals to the values from 0 - 255. For instance, if you had decided earlier that you want to map all sensor readings of 170 to the value of 255 to indicate them as bright touch points but now you don't see any bright spots when you touch, then this value was probably too high and too many touch points are being dismissed and rendered as black. Try 150 or 130 and see if that improves your results. If you go too low, this may however then also recognize noise as a touch point.<br><br>

<h3>(4) Implement blob detection</h3>

Finally, while you can now visually determine where the multi-touch pad was touched yourself, we also want to teach our program to do this automatically and tell us at which (x,y) coordinate it sees a touch point.<br><br>

To do this, we want to write code that detects the white 'blobs' in the image.<br>
This is called 'blob detection'.<br><br> 

<b>How does Blob Detection work?</b><br>
<span style="color:red">add some sentences on how blob detection works.</span><br><br> 

<b>Implementating Blob Detection: Finding Blobs in the Image</b><br>
Lucky for us, somebody else has already provided the blob detection algorithm in a <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">BlobDetection library, which you can find here here </a>. <br>
You can <span style="color:red">install the library?</span> from here <span style="color:red">link</span>.

First, construct a new object of the class using our interpolated PImage height and width.<br>
Next, make sure you set the <b>setPosDiscrimination()</b> function to detect bright areas.<br>
You can then use the <b>computeBlobs()</b> function to detect the blobs.<br>
Finally, the <b>getBlob()</b> function can help you to retrieve individual blobs.<br>
If your blob detection is not finding all the blobs in your image, consider using the <b>setThreshold()</b> function to determine which brightness levels from 0-255 should be taking into account when searching for blobs.<br>
<span style="color:red">anything I forgot?</span><br><br>

<b>Determining the Blobs Center and Contours to Identify Touch Positions and Drawing them Into the PImage</b><br>

Next, we want to extract for each blob:
<ol>
	<li>the center of the blob (i.e., center of the touch point)</li>
	<li>the contour of blob (i.e., the outline of the finger touch point)</li>
</ol>

And then draw the touch point, the finger outline, and <span style="color:red">the touch coordinates</span> into our PImage.<br><br>

To extract the touch point and contour, you can go back to the <a href="http://www.v3ga.net/processing/BlobDetection/index-page-documentation.html">documentation of the BlobDetection library</a>.<br>
The Blob class should give you all you need. <br>
Note that the contour is provided as a list of edges (i.e. lines).<br>
Drawing the contour, touch point, and text coordinates into the image is similar to what you already did in your pset1 Processing application.<br><br>

Implement this in the <b>drawBlobsAndEdges()</b> function in the skeleton code: <br><br>

<img src="images/pset3/drawBlobsAndEdges-skeleton-code.png" width="600px"> <br><br>

<b>Checking your Results</b><br>

Once you finished this part, you should be seeing something like this depending on if one or multiple fingers are touching: <br><br>

<span style="color:red">update images with touch coordinates (x,y)</span><br>
<img src="images/pset3/UI-blob-touch-1.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-2.png" width="350px"> 
<img src="images/pset3/UI-blob-touch-3.png" width="350px"> <br>

<br>

<h3>Upload your Processing Code and pictures of Your visualization UI</h3>

For grading, please upload the following to your google drive student folder:<br>

<ul>
	<li>the .pde file of your Processing program</li>
	<li>3 photos showing your Processing UI works with one, two, and three fingers touching the multi-touch pad</li>
	<li>a short video showing the Processing program working, i.e. show that the Processing UI correctly identifying one, two, and three fingers respectively touching the multi-touch pad and displays at the correct locations. Make sure the Processing UI, your multitouch pad and your fingers are visible at the same time.</li>
</ul>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5 pts: you finished all for steps of the task.</li>
	<li>5 pts: read sensing data from serial port and save them correctly.</li>
	<li>5 pts: set the noise baseline correctly, e.g. record the first 2 sec of the sensing data as noise baseline.</li>
	<li>5 pts: update the background image color and perform bicubic interpolation correctly.</li>
	<li>5 pts: implement blob detection correctly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
