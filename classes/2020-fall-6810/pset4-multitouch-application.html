<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="300px">
<img src="images/pset4/UI-recognizer-A.png" width="240px">
<img src="images/pset4/UI-3d-visualization-2-finger.png" width="200px"> <br>

          <hr>

<h2 class="headline">Problem Set 4 (due Friday, Nov. 6, 2020, 11.59pm)</h2>

Now that you have the Processing code that reads and visualizes sensing data in 2D, you are ready to extend it with two more functionalities:<br>

<ul>
	<li>(1) Gesture Recognition: Enable the multi-touch pad to recognize different finger gestures.</li>
	<li>(2) 3D Visualization: Render the signal strength as a 3D shape.</li>
</ul><br>

<h3>(1) Gesture Recognizer</h3>
Let's first build a finger gesture recognizer.<br>
The goal here is to be able to draw with your finger onto your multi-touch pad and have the multi-touch pad recognize what was drawn. For instance, if you swipe from left to right, your code should recognize that you made this gesture and if you draw an 'A' your code should know that an 'A' was drawn.<br><br>

For this part of the PSet, you will be following steps: <br>
<ol>
	<li>Understand how the $1 Unistroke Recognizer works conceptually</li>
	<li>Import the Processing Library for the $1 Unistroke Recognizer</li>
	<li>Add a new gesture by giving it a name and recording a set of touch points that make up the gesture</li>
	<li>Connect the gesture to a function that is called every time the gesture is recognized</li>
	<li>Start gesture detection every time a touch point is recognized from the blob detection</li>
	<li>Visualize the gesture detection result on screen</li>
</ol>
<br>

<h4>Understanding the $1 Unistroke Recognizer</h4> <br>

Luckily, you don't have to implement the gesture recognizer from scratch.<br>
Instead, you can use the <b>$1 Unistroke Recognizer</b>, which is also available as a Processing library.<br>
This gesture recognizer was first published at our HCI conference ACM UIST and you can find the <a href="http://faculty.washington.edu/wobbrock/pubs/uist-07.01.pdf">UIST 2017 Paper here</a>.<br>
Before we get to the implementation, let's briefly review what gestures this recognizer can detect and what gestures cannot be detected.<br><br>

<b>What Gestures can the $1 Unistroke Recognizer Detect?</b><br>

<i>2D Recognizer:</i> First, the $1 Unistroke Recognizer is a <b>2D recognizer.</b> Therefore, you can use it for your 2D multi-touch pad but you cannot use it with a hardware setup, such as the Kinect, where you can move you finger in 3D space in the air.<br><br>

<i>Unistroke:</i> Second, the recognizer is a 'unistroke' recognizer. What this means is that it can only recognize gestures that are made by <b>one finger</b> in <b>one continous stroke</b>. For instance, consider the 2 gestures shown below designed for the letter "A". The left one is written in one continous stroke and thus can be recognized by the $1 recognizer. The right one, however, is written in two separate strokes and thus cannot be recognized. Similarly, you can recognize a single finger swipe left/right gesture, but you cannot recognize <span style="color:red">a zoom gesture with two fingers since that would require combining two separate strokes into one gesture</span>.<br><br>

<img src="images/pset4/drawing-pattern-requirement-1.png" width="500px"> <br><br>

<i>Stroke Direction:</i> In addition, the algorithm recognizes the stroke direction, <span style="color:red">i.e. with which point/line you start your stroke.</span> For example, the two gestures shown below are visually identical but in the first gesture we start on the top left, while in the second gesture we start on the bottom left. Therefore, although visually the same gesture, the $1 algorithm is able to distinguish them as two different gestures. <span style="color:red"> Note that this is only possible because the algorithm ignores the stroke 'orientation'. If the algorithm would consider the stroke orientation, both gestures would be the same. (is this really true? I can't make them match up)</span><br><br>

<img src="images/pset4/drawing-pattern-requirement-3.png" width="500px"> <br><br>

<i>Stroke Orientation:</i> In addition, the algorithm <span style="color:red">ignores the stroke "orientation" (what does ignore mean? it doesn't matter how it's oriented and it will still be recognized?)</span>.<br>
However, it will likely recognize the 2 finger gestures shown below as the same gesture. <span style="color:red">(why? because it ignores orientation or it considers orientation?).</span><br><br>

<img src="images/pset4/drawing-pattern-requirement-2.png" width="500px"> <br><br>

<i>$1 Dollar:</i> Finally, the recognizer is called <b>"$1"</b> because in machine learning terms, <span style="color:red">$1 is an instance-based nearest-neighbor classifier with a 2-D Euclidean distance function, i.e., a geometric template matcher.</span> <br><br>

<b>Try the Gesture Recognizer for yourself on the Demo Webpage</b><br>
We recommend before you move on, you try out the interactive demo of the $1 Unistroke Recognizer that you can <a href="http://depts.washington.edu/acelab/proj/dollar/index.html">find here on this website</a>. Scroll down to the 'Demo' section and you see 16 different gestures you can draw into the gray window on the right side. Draw each of the gestures once to get a feel for how different gestures are designed and how well they can be classified. After you drew a gesture, you will see its classified name at the top of the window.<br><br> 

<a href="http://depts.washington.edu/acelab/proj/dollar/index.html"><img src="images/pset4/one-dollar-demo.png" width="700px"></a> <br><br>

<h4>Processing Library for $1 Unistroke Recognizer</h4> <br>

Lucky for us, the authors of the UIST'07 paper also provide a Processing library with all the functionality you saw above.<br>
Start by importing the <b>$1 Unistroke Recognizer</b> library by going to <b>Sketch -> Import Library</b> and finding it through the search bar.<br>

<img src="images/pset4/$1 Unistroke Recognizer.png" width="700px"> <br><br>

Next, import the $1 Unistroke Recognizer into your PSet3 Processing code.<br>
<pre>import de.voidplus.dollar.*; </pre> <br>

Construct a new object of the one dollar recognizer with:
<pre>OneDollar one = new OneDollar(this);</pre> <br>

<h4>Add finger gesture </h4> <br>
Next, you will create a new finger gesture.<br>
For starters, we will use a <span style="color:red">circle gesture</span>, which is one of the most reliable once that can be detected.<br><br>

To create a new finger gesture, you can use the following function:<br><br>
<pre> OneDollar.learn(String gesture-name, int[] x-y-coordinates); </pre> <br>

To obtain the x-y-coordinates for your gesture you have two options:
<ol>
	<li><i>Finger on Multi-Touch Pad:</i> You can use your finger and draw the pattern on the multi-touch pad while recording the x-y coordinates from the gesture start to end.</li>
	<li><i>Mouse on Screen:</i> Alternatively (and perhaps more conveniently), you can use <span style="color:red">your mouse to draw onto the Processing Canvas and use the mouseDragged() function to record all the points of the mouse gesture.</span></li>
</ol> 

Below you see a recording of all the coordinates for <span style="color:red">our circle gesture (for what gesture is this?).</span><br><br>

<img src="images/pset4/add-gesture2.png" width="650px"> <br><br>

<h4>Connect Finger Gesture to a Callback Function </h4> <br>

Now that you have added a custom finger gesture, you next have to define which function should be called when the gesture is being detected. Such a function is called a <b>callback function</b> and it will be triggered every time the algorithm detects the corresponding finger gesture.<br><br>

You can bind the finger gesture to a callback function via the following function: <br><br>

<pre> OneDollar.bind(String gesture-name, String callback-function-name); </pre><br>

<span style="color:red">Does the function name above include the parameters and parenthesis?</span><br><br>

Next, you need to implement the callback function.<br>
The <span style="color:red">callback function always has to be in the format as shown below, i.e. contain the following parameters and return void.<br>
The function name and parameter names, however, are up to you.</span><br><br>

<pre> void foo(String gesture-name, float percentOfSimilarity, int startX, int startY, int centroidX, int centroidY, int endX, int endY){
    // do something when the gesture is detected
}
</pre> <br>

The parameters contain the following:<br>
<i>Percent of Similarity:</i> This is how confident the recognizer is that it actually detected the right gesture. If this contains '100' the algorithm is very confident, however, if it contains '50' the algorithm is not very confident that this is correct. <span style="color:red">is there a lowest possible value?</span><br>
<i>Start X/Y:</i> This is the start position of the gesture on the multi-touch pad <span style="color:red">are these absolute values on the multi-touch pad, i.e. 0,0 is top left corner? how does it know this?</span> <br>
<i>Centroid X/Y:</i> <span style="color:red">what is this good for?</span><br>
<i>End X/Y:</i> <span style="color:red">same question as above.</span><br><br>

<h4>Tracking the finger gestures </h4> <br>

Now that you have defined how your gesture looks like and what function should be called when it is detected, you still have to tell your code when it should start tracking finger input.<br>

You can start the tracking via the following function: <br><br>
<pre> OneDollar.track(int x-coordinate, int y-coordinate); </pre><br>

<span style="color:red">what is x, y here? I don't understand.</span><br>
<span style="color:red">how do I stop tracking?</span><br>
<span style="color:red">when do we want to start tracking? only when a finger touch point is recognized through blob detection I guess?</span><br><br>

<h4>Visualize Gesture Detection Result</h4> <br>

Finally, once your gesture is detected, your callback function should visualize the detected gesture.<br>
You can do this by writing the name of the detected gesture into the Processing UI window as shown in the top left corner of the images below.<br>
<span style="color:red">can we update these images to show the drawn gesture as well?</span><br><br>

<img src="images/pset4/UI-recognizer-A.png" width="350px">
<img src="images/pset4/UI-recognizer-C.png" width="350px"> <br><br>

<h4>Create 3 Different Gestures</h4> <br>

Implement 3 different finger gestures that can be recognized by your multi-touch pad. <br>
For example, you can implement finger input for recognizing letters (A, C etc.), 2D shapes (triangle, circle etc.), and symbols (check mark etc.). <br><br>

Note that in good gesture design, the gesture should be related to what function is being called. For instance, you cannot draw a 'star' and associate it with a 'smile'. Instead, it would be better to draw a 'half circle', which more closely resembles a 'smile'. Similarly, gestures cannot be too abstract. For instance, you cannot have your finger draw one straight line on the multi-touch pad and the system recognizes it as 'fish'. Instead, there are better one-stroke gestures that could represent a fish outline. <br><br>

Take a short video of your gesture recognizer showing how you perform all three different gestures and how they are correctly classified and upload this video to your google drive.<br><br>

<h3>(2) Visualize Touch and Pressure in 3D</h3>

So far you have only recognized and visualized touch points.<br>
Next, you will extend your code to also recognize and visualize touch pressure.<br>
To recognize touch pressure, you can look at the brightness of the touch point <span style="color:red">(and why would it be brighter if I pressed harder? because more finger area touches there leading to more cumulate capacitance?)</span><br>
To visualize the touch pressure, you are going to use a 3D visualization.<br>
The height of the touch point represents how much pressure is applied.<br>
Below you can see an example: on the left side only little pressure is applied and the touch bar is small, whereas on the right side a lot of pressure is applied and the bar is much higher.<br><br>

<img src="images/pset4/UI-3d-visualization-1-finger-1.png" width="354px">
<img src="images/pset4/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

For this part of the PSet, <span style="color:red">apart from the initialization in the setup() function,</span> you are going to do the following three steps: <br>
<ol>
	<li>Draw three axis for x, y, z on screen for the 3D visualization coordinate system</li>
	<li><span style="color:red">Implement camera view and control in via keyboard (what is this for?)</span></li>
	<li>Update the 3D visualization based on interpolated PImage (i.e. <span style="color:red"><i>scaledbc</i>)(what is this?)</span></li>
</ol>
<br>

<h4>Download Skeleton Code</h4> <br>
Start by downloading <a href="software/pset4_3d_visualization_skeleton_v1.zip">the skeleton code for the PSet4 3D visualization from here</a>. <br><br>

Once you opened the skeleton code, you will see that there are some new variables and functions that are related to the 3D visualization (e.g., the axis and camerapos parameters and the drawAxis(), cameraViewControl() and update3DImage() functions). The remaining functions (readSerial(), setBaseLine(), substractBaseLine(), setColors(), and interpolate()) are from pset3 and you only have to move your code over to fill them out.<br><br>

<img src="images/pset4/pset4-skeleton-code.png" width="450px"> <br><br>

<h4>Drawing the XYZ axes for 3D Visualization</h4> <br>

You will start by drawing the XYZ axes for 3D visualization in the <i>drawAxis()</i> function. <br>
You can draw them as a standard Cartesian coordinate system. <br><br>

The length of the X and Y axes should be the width and height of your interpolated PImage.<br>
The length of the Z axes should be slightly higher than the maximum pixel value of your interpolated PImage so you have some space at the top.<br><br>

<span style="color:red">In addition, you need to color the axis by drawing a circle at their end.<br>
In 3D modeling, the convention is that green is the x-axis, red is the y-axis, and blue is the z-axis.</span><br><br>

Once you finish the <i>drawAxis()</i> function, you should have something that looks like this <span style="color:red">(note that the length of the X,Y,Z axes in the image below is not accurate) (why not?)</span>. <br><br>

<img src="images/pset4/UI-3d-visualization-drawAxis.png" width="400px"> <br><br>


<h4>Draw the Touch Points and Touch Pressure into the 3D Visualization</h4> <br>

Next, you will draw the touch points and the touch pressure into the 3D visualization based on your interpolated PImage from pset3.<br><br>

<b>Drawing 3D Points in Processing</b><br>
To implement the touch points, you will create a set of 3D points, where the X,Y coordinates are the row & column number of the pixel on the PImage, and the Z-coordinate is the interpolated value of the pixel. <br>
As mentioned previously, the brighter pixels are in an area the more pressure was applied to this area.<br>
As a result, the brighter a pixel, the higher the touch bar will be at this point.<br><br>

<b>Mapping the Z-Height to a Color Gradient</b><br>
You also need to color the touch bars with a color gradient that represents the "height" (i.e. z-axis coordinate) information.<br>
We chose a color gradient of red when only touched with light pressure and yellow when touched with high pressure.<br>
You can choose your own color gradient, just make sure that the colors are very different so you can actually see a difference.<br>
To implement the z-height to color mapping, you may find it helpful to use the <a href="https://processing.org/reference/map_.html">map() function</a> again that you already used in pset3.<br>
The resulting 3D visualization should look like something in the following.<br><br>

One finger touching: <br>
<img src="images/pset4/UI-3d-visualization-1-finger-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

Two fingers touching: <br> <span style="color:red">I'm surprised by all the noise there? are you literally drawing everything from the PImage or only the blobs that were actually recognized as touch points?</span><br>
<img src="images/pset4/UI-3d-visualization-2-finger-2.png" width="350px">
<img src="images/pset4/UI-3d-visualization-2-finger-4.png" width="350px"> <br><br>

<h4>Implement camera view control via keyboard </h4> <br>

Finally, it would be a lot cooler and more helpful in debugging if we could look around in our 3D visualization, e.g. rotate the viewport, zoom in and out etc.<br>
To be able to do this, you will implement a camera viewport that can be controlled via your keyboard. <br><br>

<b>Initialize Camera in Processing</b><br>

Processing provides a Camera Class for implementing camera movement. You can find the <a href="https://processing.org/reference/camera_.html">camera class documentation here</a>.<br>
To get started, initialize a new camera in Processing with the following method:<br><br>

<pre>camera(float eyeX, float eyeY, float eyeZ, float centerX, float centerY, float centerZ, float upX, float upY, float upZ); </pre><br>

Let's look at what all these parameters are:<br>
<i>eye x/y/z:</i> This is the camera's 3D position in space, i.e. where it would be physically located if it was a camera in real-life.<br>
<i>center x/y/z:</i> The center of the scene defines how the camera is angled. For instance, if the center of the scene is lower than the camera, then the camera will look down, resulting in a bird's eye view. Similarly, if the center of the scene is higher than the camera, then the camera will look up, resulting in a frog's eye view.</span><br>
<i>up x/y/z:</i> This defines which axis is facing upwards. <span style="color:red">0.0 means the axis is not pointing upwards, 1.0 means the axis is pointing upwards. We want that the z-axis points upwards and the rest remains where it is. (does this mean if the camera has to flip the viewport?)</span><br><br>

<b>Zooming in/out</b><br>
Moving the camera position (eye position) closer or further away from the 3D visualization results in a zooming in/out effect (see images below).<br>
Implement a zooming effect so that when you press the key "E" you zoom in and if you press the key "Q" you zoom out.<br><br>

<img src="images/pset4/UI-3d-visualization-cameraViewControl-zoom-out.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-zoom-in.png" width="350px"> <br><br>

<b>Rotating in the X-Y Plane</b><br>
To move the camera in the x-y plane around the 3D visualization, you need to keep it at the same distance from the 3D visualization at all times (see images below).<br>
This is best accomplished by moving the camera on a circle around the scene.<br>
Update the camera position (eye position) to accomplish this.<br>
In particular, if you press the key 'D' you should rotate right and if you press the key 'A' you should rotate left.<br>
Note that you only have to update the camera's position.<br>
Since you set the scene's center x/y/z to be the 3D visualization, the camera will automatically rotate while moving around the 3D visualization to always look at it.<br><br>

<img src="images/pset4/UI-3d-visualization-cameraViewControl-rotate-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-rotate-2.png" width="350px"> <br><br>


<b>Changing the Camera's Z-Axis Position</b><br>
To move the camera along the z-axis position, you need to change the <span style="color:red">camera's eye position again, but this time only along the z-axis.</span><br>
Pressing 'W' on the keyboard should move the camera up and pressing 'S' on the keyboard should move the camera down along the z-axis as shown in the image below.<br><br>

<img src="images/pset4/UI-3d-visualization-cameraViewControl-view-height-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-view-height-2.png" width="350px"> <br><br>

Finally, if you are experiencing some lagging in your application, you can also set the frame rate for the camera for better performance.<br><br>

<span style="color:red">sorry accidentally deleted framerate code. add back in.</span>

<br><br>

<h3>Upload your Processing Code and Pictures of your Visualization</h3>

For grading, please upload the following to your google drive student folder:<br>

<ol>
	<li>Gesture Recognizer
		<ul>
			<li>a drawing on paper that shows your 3 custom finger gestures so we know which gestures you implemented, use the same notation as in the pset to indicate the start point for executing the gesture</li>
			<li>the .pde file of your Processing program</li>
			<li>3 photos showing your Processing UI successfully recognizing the 3 different finger gestures </li>
			<li>a short video showing your Processing UI successfully recognizing the 3 different finger gestures (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>
	</li>
	<li>3D Visualization
		<ul>
		<li>the .pde file of your Processing program </li>
		<li>3 photos showing your Processing UI when no finger, one finger, and two fingers touch the multi-touch pad with different pressure levels</li>
		<li>a short video showing your Processing UI when no finger, one finger, and two fingers touch the multi-touch pad with different pressure levels (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>
	</li>
</ol>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5pts: you implemented 3 different gestures using the learn(), bind(), and track() functions from the $1 Unistroke Recognizer</li>
	<li>5pts: you submitted a drawing showing all three gestures you implemented and show in the video that all three gestures are correctly recognized</li>
	<li>5pts: the x,y,z axis are correctly drawn for the 3D visualization and the touch / pressure mapping is correctly rendered, which you show by having at least 2 fingers touch simultaneously</li>
	<li>5pts: the camera motion for zooming in/out, rotating the camera in the x/y plane and moving the camera up/down are correctly implemented</li>
	<li>5pts: the remaining 5pts will be given for creating presentation materials for your multi-touch pad, i.e. drawing a rotoscope (1.5pts), taking some high-quality photos (1.5pts) and making a short 2-3 sequences video (2pts). This will be part of the weekly Friday labs and you will work on this over the next weeks in small steps. We will provide more information on this shortly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
