<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="300px">
<img src="images/pset4/UI-recognizer-A.png" width="240px">
<img src="images/pset4/UI-3d-visualization-2-finger.png" width="200px"> <br>

          <hr>

<h2 class="headline">Problem Set 4 (due Friday, Nov. 6, 2020, 11.59pm)</h2>

Now that you have the Processing code that reads and visualizes sensing data in 2D, you are ready to extend it with two more functionalities:<br>

<ul>
	<li>(1) Gesture Recognition: Enable the multi-touch pad to recognize different finger gestures.</li>
	<li>(2) 3D Visualization: Render the signal strength as a 3D shape.</li>
</ul><br>

<h3>(1) Gesture Recognizer</h3>
Let's first build a finger gesture recognizer.<br>
The goal here is to be able to draw with your finger onto your multi-touch pad and have the multi-touch pad recognize what was drawn. For instance, if you swipe from left to right, your code should recognize that you made this gesture and if you draw an 'A' your code should know that an 'A' was drawn.<br><br>

<h4>Understanding the $1 Unistroke Recognizer</h4> <br>

Luckily, you don't have to implement the gesture recognizer from scratch.<br>
Instead, you can use the <b>$1 Unistroke Recognizer</b>, which is also available as a Processing library.<br>
This gesture recognizer was first published at our HCI conference ACM UIST and you can find the <a href="http://faculty.washington.edu/wobbrock/pubs/uist-07.01.pdf">UIST 2017 Paper here</a>.<br>
Before we get to the implementation, let's briefly review what gestures this recognizer can detect and what gestures cannot be detected.<br><br>

<b>What Gestures can the $1 Unistroke Recognizer Detect?</b><br>

<i>2D Recognizer:</i> First, the $1 Unistroke Recognizer is a <b>2D recognizer.</b> Therefore, you can use it for your 2D multi-touch pad but you cannot use it with a hardware setup, such as the Kinect, where you can move you finger in 3D space in the air.<br><br>

<i>Unistroke:</i> Second, the recognizer is a 'unistroke' recognizer. What this means is that it can only recognize gestures that are made by <b>one finger</b> in <b>one continous stroke</b>. For instance, consider the 2 gestures shown below designed for the letter "A". The left one is written in one continous stroke and thus can be recognized by the $1 recognizer. The right one, however, is written in two separate strokes and thus cannot be recognized. Similarly, you can recognize a single finger swipe left/right gesture, but you cannot recognize <span style="color:red">a zoom gesture with two fingers since that would require combining two separate strokes into one gesture</span>.<br><br>

<img src="images/pset4/drawing-pattern-requirement-1.png" width="500px"> <br><br>

<i>Stroke Direction:</i> In addition, the algorithm recognizes the stroke direction, <span style="color:red">i.e. with which point/line you start your stroke.</span> For example, the two gestures shown below are visually identical but in the first gesture we start on the top left, while in the second gesture we start on the bottom left. Therefore, although visually the same gesture, the $1 algorithm is able to distinguish them as two different gestures. <span style="color:red"> Note that this is only possible because the algorithm ignores the stroke 'orientation'. If the algorithm would consider the stroke orientation, both gestures would be the same. (is this really true? I can't make them match up)</span><br><br>

<img src="images/pset4/drawing-pattern-requirement-3.png" width="500px"> <br><br>

<i>Stroke Orientation:</i> In addition, the algorithm <span style="color:red">ignores the stroke "orientation" (what does ignore mean? it doesn't matter how it's oriented and it will still be recognized?)</span>.<br>
However, it will likely recognize the 2 finger gestures shown below as the same gesture. <span style="color:red">(why? because it ignores orientation or it considers orientation?).</span><br><br>

<img src="images/pset4/drawing-pattern-requirement-2.png" width="500px"> <br><br>

<i>$1 Dollar:</i> Finally, the recognizer is called <b>"$1"</b> because in machine learning terms, <span style="color:red">$1 is an instance-based nearest-neighbor classifier with a 2-D Euclidean distance function, i.e., a geometric template matcher.</span> <br><br>

<b>Try the Gesture Recognizer for yourself on the Demo Webpage</b><br>
We recommend before you move on, you try out the interactive demo of the $1 Unistroke Recognizer that you can <a href="http://depts.washington.edu/acelab/proj/dollar/index.html">find here on this website</a>. Scroll down to the 'Demo' section and you see 16 different gestures you can draw into the gray window on the right side. Draw each of the gestures once to get a feel for how different gestures are designed and how well they can be classified. After you drew a gesture, you will see its classified name at the top of the window.<br><br> 

<a href="http://depts.washington.edu/acelab/proj/dollar/index.html"><img src="images/pset4/one-dollar-demo.png" width="700px"></a> <br><br>

<h4>Processing Library for $1 Unistroke Recognizer</h4> <br>

Lucky for us, the authors of the UIST'07 paper also provide a Processing library with all the functionality you saw above.<br>
Start by importing the <b>$1 Unistroke Recognizer</b> library by going to <b>Sketch -> Import Library</b> and finding it through the search bar.<br>

<img src="images/pset4/$1 Unistroke Recognizer.png" width="700px"> <br><br>

Next, import the $1 Unistroke Recognizer into your PSet3 Processing code.<br>
<pre>import de.voidplus.dollar.*; </pre> <br>

Construct a new object of the one dollar recognizer with:
<pre>OneDollar one = new OneDollar(this);</pre> <br>

<h4>Add finger gesture </h4> <br>
Next, you will create a new finger gesture.<br>
For starters, we will use a <span style="color:red">circle gesture</span>, which is one of the most reliable once that can be detected.<br><br>

To create a new finger gesture, you can use the following function:<br><br>
<pre> OneDollar.learn(String gesture-name, int[] x-y-coordinates); </pre> <br>

To obtain the x-y-coordinates for your gesture you have two options:
<ol>
	<li><i>Finger on Multi-Touch Pad:</i> You can use your finger and draw the pattern on the multi-touch pad while recording the x-y coordinates from the gesture start to end.</li>
	<li><i>Mouse on Screen:</i> Alternatively (and perhaps more conveniently), you can use <span style="color:red">your mouse to draw onto the Processing Canvas and use the mouseDragged() function to record all the points of the mouse gesture.</span></li>
</ol> 

Below you see a recording of all the coordinates for <span style="color:red">our circle gesture (for what gesture is this?).</span><br><br>

<img src="images/pset4/add-gesture2.png" width="650px"> <br><br>

<h4>Connect Finger Gesture to a Callback Function </h4> <br>

Now that you have added a custom finger gesture, you next have to define which function should be called when the gesture is being detected. Such a function is called a <b>callback function</b> and it will be triggered every time the algorithm detects the corresponding finger gesture.<br><br>

You can bind the finger gesture to a callback function via the following function: <br><br>

<pre> OneDollar.bind(String gesture-name, String callback-function-name); </pre><br>

<span style="color:red">Does the function name above include the parameters and parenthesis?</span><br><br>

Next, you need to implement the callback function.<br>
The <span style="color:red">callback function always has to be in the format as shown below, i.e. contain the following parameters and return void.<br>
The function name and parameter names, however, are up to you.</span><br><br>

<pre> void foo(String gesture-name, float percentOfSimilarity, int startX, int startY, int centroidX, int centroidY, int endX, int endY){
    // do something when the gesture is detected
}
</pre> <br>

The parameters contain the following:<br>
<i>Percent of Similarity:</i> This is how confident the recognizer is that it actually detected the right gesture. If this contains '100' the algorithm is very confident, however, if it contains '50' the algorithm is not very confident that this is correct. <span style="color:red">is there a lowest possible value?</span><br>
<i>Start X/Y:</i> This is the start position of the gesture on the multi-touch pad <span style="color:red">are these absolute values on the multi-touch pad, i.e. 0,0 is top left corner? how does it know this?</span> <br>
<i>Centroid X/Y:</i> <span style="color:red">what is this good for?</span><br>
<i>End X/Y:</i> <span style="color:red">same question as above.</span><br><br>

<h4>Tracking the finger gestures </h4> <br>

Now that you have defined how your gesture looks like and what function should be called when it is detected, you still have to tell your code when it should start tracking finger input.<br>

You can start the tracking via the following function: <br><br>
<pre> OneDollar.track(int x-coordinate, int y-coordinate); </pre><br>

<span style="color:red">what is x, y here? I don't understand.</span><br>
<span style="color:red">how do I stop tracking?</span><br>
<span style="color:red">when do we want to start tracking? only when a finger touch point is recognized through blob detection I guess?</span><br><br>

<h4>Visualize Gesture Detection Result</h4> <br>

Finally, once your gesture is detected, your callback function should visualize the detected gesture.<br>
You can do this by writing the name of the detected gesture into the Processing UI window as shown in the top left corner of the images below.<br>
<span style="color:red">can we update these images to show the drawn gesture as well?</span><br><br>

<img src="images/pset4/UI-recognizer-A.png" width="350px">
<img src="images/pset4/UI-recognizer-C.png" width="350px"> <br><br>

<h4>Create 3 Different Gestures</h4> <br>

Implement 3 different finger gestures that can be recognized by your multi-touch pad. <br>
For example, you can implement finger input for recognizing letters (A, C etc.), 2D shapes (triangle, circle etc.), and symbols (check mark etc.). <br><br>

Note that in good gesture design, the gesture should be related to what function is being called. For instance, you cannot draw a 'star' and associate it with a 'smile'. Instead, it would be better to draw a 'half circle', which more closely resembles a 'smile'. Similarly, gestures cannot be too abstract. For instance, you cannot have your finger draw one straight line on the multi-touch pad and the system recognizes it as 'fish'. Instead, there are better one-stroke gestures that could represent a fish outline. <br><br>

Take a short video of your gesture recognizer showing how you perform all three different gestures and how they are correctly classified and upload this video to your google drive.<br><br>

<h3>(2) Visualize the touching data in 3D</h3>

In this part, you will be implementing the multi-touch pad touching data in 3D, where the "height" of the roughly reflects how "hard" you are touching (remember in PSet3, the brightness of the 2D blob area roughly reflects how "hard" you are touching). <br>
<img src="images/pset4/UI-3d-visualization-1-finger-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

Start by downloading <a href="software/pset4_3d_visualization_skeleton_v1.zip">the skeleton code for the PSet4 3D visualization from  here</a>. <br><br>

Once you open up the skeleton code, you will notice that it is quite similar to the PSet3 skeleton code. <br>
As a matter of fact, if you have the PSet3 all implemnted corrected, you already have most of the code written! <br>

<img src="images/pset4/pset4-skeleton-code.png" width="500px"> <br>

For the functions that share the same name with PSet3 (i.e. readSerial(), setBaseLine(), substractBaseLine(), setColors(), and interpolate()), you can directly copy your PSet3 code into the corresponding functions. <br><br>

For this part of the PSet, apart from the initialization in the setup() function, you are going to do the following three steps: <br>
<ol>
	<li>Draw XYZ axes for 3D visualization</li>
	<li>Implement camera view and control in via keyboard</li>
	<li>Update the 3D visualization based on interpolated PImage (i.e. <i>scaledbc</i>)</li>
</ol>
<br>

<h4>Draw XYZ axes for 3D visualization </h4> <br>

You will start by drawing out XYZ axes for 3D visualization in the <i>drawAxis()</i> function. <br>
You can draw them as a standard Cartesian coordinate system. <br><br>

The length of the X & Y axes should be the width and height of your interpolated PImage.<br>
The length of the Z axes should be slightly over the pixel value range of your interpolated PImage for better visualization. <br><br>

Once you finish the <i>drawAxis()</i> function, you should have something look like this (notice that the length of the X,Y,Z axes in the following image might not be accurate): <br>

<img src="images/pset4/UI-3d-visualization-drawAxis.png" width="500px"> <br>

<h4>Implement camera view control via keyboard </h4> <br>

In this part, you will be implementing an active camera view, which has the ability to change in zoom in/out, height in Z-axis and rotation in XY-plane. <br>
The camera view will be controlled via keyboard buttons. <br><br>

You should first initialize the "camera view" in Processing with the following method:
<pre>camera(float eyeX, float eyeY, float eyeZ, float centerX, float centerY, float centerZ, float upX, float upY, float upZ); </pre><br>

It sets the position of the camera through setting the eye position, the center of the scene, and which axis is facing upward. <br>
Moving the eye position and the direction it is pointing (the center of the scene) allows the images to be seen from different angles. <br><br>
You can also set the frame rate for better performance (e.g. if you are experiencing sketch laggs) via <i>frameRate(int rate)</i> function. <br>
For more details about the camera in Processing, please refer to the offical documents <a href="https://processing.org/reference/camera_.html">here</a>.<br><br>

You will implement the camera view control so that:
<ol>
	<li>press key "Q" and "E" control change in camera zoom in / out </li>
	<li>press key "W" and "S" control change in camera height in Z-axis </li>
	<li>press key "A" and "D" control change in camera rotation in XY-plane </li>
</ol>
<br>

The changes in camera view point should look like something in the following.<br>
Change in zoom in/out: <br>
<img src="images/pset4/UI-3d-visualization-cameraViewControl-zoom-out.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-zoom-in.png" width="350px"> <br><br>

Change in height in Z-axis: <br>
<img src="images/pset4/UI-3d-visualization-cameraViewControl-view-height-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-view-height-2.png" width="350px"> <br><br>

Change in rotation in XY-plane: <br>
<img src="images/pset4/UI-3d-visualization-cameraViewControl-rotate-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-rotate-2.png" width="350px"> <br><br>


<h4>Update 3D visualization </h4> <br>

The last part of the 3D visualization is to actually implement the 3D visualization based on your intepolated PImage. <br><br>

Recall from the PSet 3, we intepolated the 9x8 multi-touch pad touching data into a much larger PImage and assign the intepolated value pixel by pixel. <br>
We will be implementing something quite similar here, except the intepolated value now become the "height" (i.e. z-axis coordinate) + color. <br><br>

You will implement the 3D visualization as a set of 3D points, where the X,Y coordinates are the row & column number of the pixel on PImage, and the Z coordinate the intepolated value of the pixel. <br>
You will also implement a color gradient (of your choice) that represents the "height" (i.e. z-axis coordinate) information.<br>
You might find your old friend <i>map()</i> to be helpful during some value assignments.<br><br>

The resulting 3D visualization should look like something in the following.<br>

No finger touching: <br>
<img src="images/pset4/UI-3d-visualization-no-finger-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-no-finger-2.png" width="350px"> <br><br>

One finger touching: <br>
<img src="images/pset4/UI-3d-visualization-1-finger-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

Two fingers touching: <br>
<img src="images/pset4/UI-3d-visualization-2-finger-2.png" width="350px">
<img src="images/pset4/UI-3d-visualization-2-finger-4.png" width="350px"> <br><br>



<!-- <h3>(3) An open-ended cool mini-application based on (1) and/or (2) (optional)</h3> -->

<br>

<h3>Upload your Processing Code and Pictures of your Visualization</h3>

For grading, please upload the following to your google drive student folder:<br>

<ol>
	<li>Finger Gesture Recognizer
		<ul>
		<li>the .pde file of your Processing program </li>
		<li>a mock-up drawing of your selected finger gestures (at leaset 3 to get the full points) </li>
		<li>3 photos showing your Processing UI successfully recoginizes 3 different finger gestures </li>
		<li>a short video showing your Processing UI successfully recoginizes 3 different finger gestures (take the video so that the multi-touch pad and Processing Window are seen at the same time, you can try multiple times) </li>
		</ul>
	</li>
	<li>3D Visualization
		<ul>
		<li>the .pde file of your Processing program </li>
		<li>3 photos showing your Processing UI when no finger, one finger, and two fingers touching the multi-touch pad </li>
		<li>a short video showing your Processing UI when no finger, one finger, and two fingers touching the multi-touch pad (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>
	</li>
</ol>


In addition, for pset4 you will create some visual materials to showcase your multi-touch pad. These will be spread over the next couple of weeks (you will see them on the friday lab schedule) and are not due at the deadline. 

<ul>
	<li>(3) Rotoscope: 2D Drawing of your Multi-Touch Pad</li>
	<li>(4) Photos: High Quality Photo taken with a Backdrop</li>
	<li>(5) Video: A short video taken with a backdrop with at least 3 different shots cut together</li>
</ul>


<br>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5 pts: have 3 different finger gestures implemented.</li>
	<li>5 pts: the system correctly recognize all 3 different finger gestures and update the UI accordingly.</li>
	<li>5 pts: XYZ axes correctly draw for 3D visualization and update correctly with the active camera view.</li>
	<li>5 pts: camera view control correctly implemented with ability to change in zoom in/out, height in Z-axis and rotation in XY-plane.</li>
	<li>5 pts: 3D visualization correctly implemented and updates with the touching data (and can at least visualizes 2 fingers touching simultaneously.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
