<!DOCTYPE html>
<html>
<head>
	<title>HCI Engineering Group</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

	<!-- CSAIL ICON -->
	<link rel="CSAIL" href="../../images/icon/csail.ico" type="image/x-icon" />

	<!-- Bootstrap -->
	<link href="../../css/bootstrap.css" rel="stylesheet">
	<link href="../../css/custom-style.css" rel="stylesheet">

	<!-- jQuery -->
	<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Abel" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Barlow" rel="stylesheet">

	<!-- Google Analytic -->
	<script type="text/javascript" src="../../js/analytics.js"></script>

	<style>
	.etech-sch-col1 {width:60px; border: 1px solid black;padding:10px;}
	.etech-sch-col2 {width:120px; border: 1px solid black;padding:10px;}
	.etech-sch-col3 {width:450px; border: 1px solid black;padding:10px;}
	.etech-sch-col4 {width:70px; border: 1px solid black;padding:10px;}
  .etech-sch-col5 {width:70px; border: 1px solid black;padding:10px;}
  /*.etech-sch-col6 {width:170px; border: 1px solid black;padding:10px;}*/
  ul {
    padding:0px;padding-left:10px;margin:0px;
  }
	</style>
</head>

<body>
<header class="main_header">
	<!-- to be filled by javascript, see header.html -->
</header>

<section class="main_container">
	<div class="container">
    <div class="row nothing">

      <section class="col-md-8 pull-right main-content">
</br></br></br></br>
        <h4 class="medium.headline"><a href="6810-engineering-interactive-technologies.html">6.810 Engineering Interactive Technologies (fall 2020)</a><br></h>
        <h2 class="headline">Problem Set Series: Multi-Touch Pad</h2>


<img src="images/pset2-overall.png" width="300px">
<img src="images/pset4/UI-recognizer-A.png" width="240px">
<img src="images/pset4/UI-3d-visualization-2-finger.png" width="200px"> <br>

          <hr>

<h2 class="headline">Problem Set 4 (due Friday, Nov. 6, 2020, 11.59pm)</h2>

Now that you have the Processing code that reads and visualizes sensing data in 2D, you are ready to extend it with two more functionalities:<br>

<ul>
	<li>(1) Gesture Recognition: Enable the multi-touch pad to recognize different finger gestures (build on top of your PSet3 code).</li>
	<li>(2) 3D Visualization of Touch Pressure: Render the signal strength as a 3D shape (via new skeleton code).</li>
</ul><br>

<h3>(1) Gesture Recognizer</h3>
Let's first build the gesture recognizer.<br>
The goal here is to be able to draw with your finger onto your multi-touch pad and have the multi-touch pad recognize what was drawn. For instance, if you swipe from left to right, your code should recognize that you made this gesture and if you draw an 'A' your code should know that an 'A' was drawn.<br><br>

For this part of the PSet, you will be doing the following steps: <br>
<ol>
	<li>Understand how the $1 Unistroke Recognizer works conceptually</li>
	<li>Import the Processing Library for the $1 Unistroke Recognizer</li>
	<li>Add a new gesture by giving it a name and recording a set of touch points that make up the gesture</li>
	<li>Connect the gesture to a function that is called every time the gesture is recognized</li>
	<li>Start gesture detection every time a touch point is recognized from the blob detection</li>
	<li>Visualize the gesture detection result on screen</li>
</ol>
<br>

<h4>Understanding the $1 Unistroke Gesture Recognizer</h4> <br>

Luckily, you don't have to implement the gesture recognizer from scratch.<br>
Instead, you can use the <b>$1 Unistroke Recognizer</b>, which is available as a Processing library.<br><br>

<i>$1 Gesture Recognizer Family:</i> The $1 Unistroke Gesture Recognizer is part of the so-called $1 Gesture Recognizer family, i.e. a set of different gesture recognizers with different capabilities, such as single stroke gesture recognition, multi-stroke gesture recognition etc. It was first published at the HCI conference ACM UIST and you can find the <a href="http://faculty.washington.edu/wobbrock/pubs/uist-07.01.pdf">UIST 2007 Paper here</a>.<br><br>

<i>$1 Unistroke Gesture Recognizer:</i> In this Pset, we are going to use the $1 Unistroke Recognizer from this family since it is the only one that is available for Processing as a library. Before we get to the implementation, let's briefly review what gestures this recognizer can detect and what gestures it cannot detect.<br><br>

<b>What Gestures can the $1 Unistroke Recognizer Detect?</b><br>

<i>2D Recognizer:</i> First, the $1 Unistroke Recognizer is a <b>2D recognizer.</b> Therefore, you can use it for your 2D multi-touch pad but it is not suitable for interaction in 3D space, such as when you make gestures in front of a Kinect (except if you draw them into a 2D plane).<br><br>

<i>Unistroke:</i> Second, the recognizer is a 'unistroke' recognizer. What this means is that it can only recognize gestures that are made by <b>one finger</b> in <b>one continous stroke</b>. For instance, consider the 2 gestures shown below designed for the letter "A". The left one is written in one continous stroke and thus can be recognized by the $1 Unistroke recognizer. The right one, however, is written in two separate strokes and thus cannot be recognized. Similarly, you can recognize a single finger swipe left/right gesture, but you cannot recognize a zoom gesture with two fingers since that would require combining two separate strokes into one gesture. The $1 family has also has a multi-stroke recognizer but as mentioned previously it is not available as a library for Processing.<br><br>

<img src="images/pset4/drawing-pattern-requirement-1.png" width="500px"> <br><br>

<i>Stroke Orientation:</i> The gesture recognizer ignores the stroke 'orientation'. Consider the example below, which shows the same gesture but executed in a different orientation on the multi-touch pad. The gesture recognizer automatically compensates for the change in orientation and will detect that both gestures are the same.<br><br>

 <img src="images/pset4/stroke-orientation-example.png" width="450px"> <br><br>

<i>Stroke Direction:</i> While the recognizer ignores stroke orientation, it does recognize stroke direction, i.e. the vector direction of the line along which your finger moves. For example, the two gestures shown below are visually identical on first sight but in the first gesture we start on the top left, while in the second gesture we start on the bottom left. Therefore, although visually the same gesture, the $1 algorithm is able to distinguish them as two different gestures.<br><br>

<img src="images/pset4/stroke-direction-example.png" width="450px"> <br><br>

<!-- <i>$1 Dollar:</i> Finally, the recognizer is called <b>"$1"</b> because in machine learning terms, <span style="color:red">$1 is an instance-based nearest-neighbor classifier with a 2-D Euclidean distance function, i.e., a geometric template matcher.</span> <br><br>
 -->
<b>Try the Gesture Recognizer for yourself on the Demo Webpage</b><br>
We recommend before you move on, you try out the interactive demo of the $1 Unistroke Recognizer that you can <a href="http://depts.washington.edu/acelab/proj/dollar/index.html">find here on this website</a>. Scroll down to the 'Demo' section and you see 16 different gestures you can draw into the gray window on the right side. Draw each of the gestures once to get a feel for how different gestures are designed and how well they can be classified. After you drew a gesture, you will see its classified name at the top of the window. You can also make your own gesture by first drawing your custom gesture into the gray drawing window and then giving it a name in the 'Add as example of custom type:' field, then click 'Add'. If you now draw your custom gesture again, it should be correctly classified.<br><br> 

<a href="http://depts.washington.edu/acelab/proj/dollar/index.html"><img src="images/pset4/one-dollar-demo.png" width="700px"></a> <br><br>

<h4>Processing Library for $1 Unistroke Recognizer</h4> <br>

Now that you know a bit more about what the $1 Unistroke recognizer can and cannot do, you can move on to using the recognizer for your multi-touch pad.<br>
As mentioned previously, the authors of the UIST'07 paper provide the $1 Unistroke Recognizer as a Processing library with all the functionality you saw above.<br>
Start by importing the <b>$1 Unistroke Recognizer</b> library by going to <b>Sketch -> Import Library</b> and finding it through the search bar.<br><br>

<img src="images/pset4/$1 Unistroke Recognizer.png" width="700px"> <br><br>

Next, import the $1 Unistroke Recognizer library into your PSet3 Processing code.<br>
<pre>import de.voidplus.dollar.*; </pre> <br>

Construct a new object of the one dollar recognizer with:
<pre>OneDollar oneDollar = new OneDollar(this);</pre> <br>

Finally, turn on console plotting for the gesture recognizer using the line below.<br>
If this is on then whenever the gesture recognizer detects a gesture it immediately prints it on the console and also gives you the similarity percentage with one of the template gestures, so this is a useful thing to have turned on.<br><br>

<pre>oneDollar.setVerbose(true); \\activates console plotting</pre> <br>

<h4>Add finger gesture </h4> <br>
Next, you will create a new finger gesture.<br>
For starters, we will use a 'V' gesture, which is one of the most reliable ones that can be detected. Other reliable gestures are triangle, zigzag, checkmark, and anything that has some unique features. A circle, for instance, is one of the harder gestures to recognize due to its smooth outline that is easily susceptible to noise.<br><br>

To create a new finger gesture, you can use the following function:<br><br>
<pre> oneDollar.learn(String gesture-name, int[] x-y-coordinates); // {x1, y1, x2, y2, x3, y3, x4, y4, ...} </pre> <br>

<b>Recording x/y coordinates for New Finger Gesture</b><br>
While you could obtain the x-y-coordinates for your gesture template by tracking finger input from your multi-touch pad, we recommend not doing this and instead recommend that you draw the gesture with the mouse on screen and record the x-y coordinates from the mouseDragged() event.<br> The reason for this is that the drawing with the mouse will give you very clear tracking points for creating your template gesture. Later on, when you draw the gesture with your multi-touch pad during actual use, this will be more fault resistant. In contrast, if you record your template gesture on the multi-touch pad, you will already have some noise in the data and it will be harder to match a gesture when there is additional noise during actual use later on.<br><br>

Below you see a recording of all the coordinates for our 'V' gesture that we then feed into the learn() function to add our new gesture. Do the same before moving on.<br><br>

<img src="images/pset4/add-gesture3.png" width="650px"> <br><br>

<h4>Connect Finger Gesture to a Callback Function </h4> <br>

Now that you have added your custom 'V' finger gesture, you next have to define which function should be called when the gesture is being detected. Such a function is called a <b>callback function</b> and it will be triggered every time the algorithm detects the corresponding finger gesture.<br><br>

You can bind the finger gesture to a callback function via the following command: <br><br>

<pre> oneDollar.bind(String gesture-name, String callback-function-name); </pre><br>

Note that the call-back-function-name is only the same of the function without parenthesis and parameters.<br><br>

You can either have each gesture have its own call-back function or you can also bind multiple gestures to the same call-back function via this line:<br>

<pre> oneDollar.bind(String gesture-name1 gesture-name2 gesture-name3..., String callback-function-name); </pre><br>

Next, you need to implement the callback function.<br>
The callback function always has to be in the format as shown below, i.e. contain the same number of parameters and parameter types and return void.<br>
The function name and parameter names, however, are up to you.</span><br><br>

<pre> void foo(String gesture-name, float percentOfSimilarity, int startX, int startY, int centroidX, int centroidY, int endX, int endY){
	// do something when the gesture is detected
 }
</pre> <br>

There are a lot of parameters in the function but for our purposes we can keep it simple.<br>
If you bind one gesture to one function, all you need is to add some action in the function body.<br>
If you bind more than one gesture to one function, you can use the 'String gesture-name' to figure out which gesture triggered the function and then proceed with the corresponding action in the function body.<br><br>

The remaining parameters contain how confident the recognizer is that it actually detected the right gesture (percentOfSimilarity), the start point of the gesture (startX,startY), the end point of the gesture (endX,endY), and the centroid of the gesture. For our purposes, we don't need these but we still have to list them in the function header to match the template requirement of the call-back function.<br><br>

<h4>Tracking the finger gestures </h4> <br>

Now that you have defined how your gesture looks like and what function should be called when it is detected, you still have to tell your code when it should start tracking finger input.<br>

We want to start tracking our gesture when our code detects a 'finger blob'.<br>
Therefore, we need to add the tracking function below in the blob detection method where it determines if a finger was successfully detected.<br><br>

<pre> oneDollar.track(int x-coordinate, int y-coordinate); </pre><br>

The x/y coordinate in the function are the x/y coordinate of your touch point.<br><br>

Note that you don't have to 'stop' tracking. The track() function will only be executed when Processing goes through its infinite loop and detects a finger blob, i.e. knows a finger is present. If no finger has been present for a while, i.e. no new data is fed to the gesture recognizer, it will automatically determine that the gesture has ended and will provide its best guess at what gesture was executed. <br><br>

<h4>Visualize Gesture Detection Result</h4> <br>

Finally, once your gesture is detected, your callback function should visualize the detected gesture.<br>
You can do this by writing the name of the detected gesture into the Processing UI window as shown in the top left corner of the images below.<br>

<img src="images/pset4/UI-recognizer-A.png" width="350px">
<img src="images/pset4/UI-recognizer-C.png" width="350px"> <br><br>

<h4>Create 3 Different Gestures</h4> <br>

Implement 3 different finger gestures that can be recognized by your multi-touch pad. <br>
For example, you can implement finger input for recognizing letters (A, C etc.), 2D shapes (triangle, circle etc.), and symbols (check mark etc.). <br><br>

Note that in good gesture design, the gesture should be related to what function is being called. For instance, you cannot draw a 'star' and associate it with a 'smile'. Instead, it would be better to draw a 'half circle', which more closely resembles a 'smile'. Similarly, gestures cannot be too abstract. For instance, you cannot have your finger draw one straight line on the multi-touch pad and the system recognizes it as 'fish'. Instead, there are better one-stroke gestures that could represent a fish outline. <br><br>

Take a short video of your gesture recognizer showing how you perform all three different gestures and how they are correctly classified and upload this video to your google drive.<br><br>

<h3>(2) Visualize Touch and Pressure in 3D</h3>

So far you have only recognized and visualized touch points.<br>
Next, you will extend your code to also recognize and visualize touch pressure.<br>
To recognize touch pressure, you can look at the brightness of the touch point <span style="color:red">(and why would it be brighter if I pressed harder? because more finger area touches there leading to more cumulate capacitance?)</span><br>
To visualize the touch pressure, you are going to use a 3D visualization.<br>
The height of the touch point represents how much pressure is applied.<br>
Below you can see an example: on the left side only little pressure is applied and the touch bar is small, whereas on the right side a lot of pressure is applied and the bar is much higher.<br><br>

<img src="images/pset4/UI-3d-visualization-1-finger-1.png" width="354px">
<img src="images/pset4/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

For this part of the PSet, you are going to do the following three steps: <br>
<ol>
	<li>draw three axis for x, y, z on screen for the 3D visualization coordinate system</li>
	<li>implement functionality to move the camera around the 3D visualization scene via keyboard input, i.e. the camera will be able to zoom in/out of the scene, rotate around the scene, and move up/down with respect to the scene when the corresponding keys are pressed</li>
	<li>update the 3D visualization with the touch points and touch pressure based on interpolated PImage of your touch points from Pset3</span></li>
</ol>
<br>

<h4>Download Skeleton Code</h4> <br>
Start by downloading <a href="software/pset4_3d_visualization_skeleton_v3.zip">the skeleton code for the PSet4 3D visualization from here</a>. <br><br>

Once you opened the skeleton code, you will see that there are some new variables and functions that are related to the 3D visualization (e.g., the axis and camerapos parameters and the drawAxis(), cameraViewControl() and update3DImage() functions). The remaining functions (readSerial(), setBaseLine(), substractBaseLine(), setColors(), and interpolate()) are from pset3 and you only have to move your code over to fill them out.<br><br>

<img src="images/pset4/pset4-skeleton-code.png" width="450px"> <br><br>

<h4>Drawing the XYZ axes for 3D Visualization</h4> <br>

You will start by drawing the XYZ axes for 3D visualization in the <i>drawAxis()</i> function. <br>
You can draw them as a standard Cartesian coordinate system. Note that we are in <b>3D now, not 2D</b> so you need 3 coordinates for defining the positions of your points/lines.<br><br>

Cartesian 3D systems are often described as "left-handed" or "right-handed." <br>
If you point your index finger in the positive y direction (down) and your thumb in the positive x direction (to the right), the rest of your fingers will point towards the positive z direction. <br>
It's left-handed if you use your left hand and do the same. <br>
In Processing, the system is left-handed, as follows: <br><br>

<img src="images/pset4/processing-3d-system.png" width="450px"> <br><br>

The length of the X and Z axes should be the width and height of your interpolated PImage.<br>
The length of the Y axis should be slightly higher than the maximum pixel value (brightness) of your interpolated PImage so you have some space at the top.<br>
Notice that you should draw the Y axis (and later sensing data on y axis) in negative direction so it is pointing "up".<br><br>

In addition to generating the axis lines, you need to color the axis by drawing a colored dot at their end.<br>
We will use red for the x-axis, green for the y-axis, and blue for the z-axis (since "RGB" corresponding to "XYZ").<br><br>

Once you finish the <i>drawAxis()</i> function, you should have something that looks like this (note that the height of your y-axis depends on the brightness values you get in your PImage and may not be shown to scale here):<br><br>

<img src="images/pset4/UI-3d-visualization-drawAxis.png" width="400px"> <br><br>

<h4>Moving the Camera Around the 3D Visualization via Keyboard Input</h4> <br>

Without camera motion, it is kind of hard to see if you really succeeded in drawing your axis correctly or if it just 'looks' correct from the 2D perspective we have but in reality the lines are all over the place in 3D space.<br>
To facilitate debugging of issues like this, we will implement camera control, i.e. you will be able to move around the camera by pressing keys on the keyboard to zoom in/out and move the camera around the scene.<br><br>

<b>Initialize Camera in Processing</b><br>

Processing provides a Camera Class for implementing camera movement. You can find the <a href="https://processing.org/reference/camera_.html">camera class documentation here</a>.<br>
To get started, initialize a new camera in Processing with the following method:<br><br>

<pre>camera(float eyeX, float eyeY, float eyeZ, float centerX, float centerY, float centerZ, float upX, float upY, float upZ); </pre><br>

Let's look at what all these parameters are:<br>
<i>eye x/y/z:</i> This is the camera's 3D position in space, i.e. where it would be physically located if it was a camera in real-life.<br>
<i>center x/y/z:</i> The center of the scene defines how the camera is angled. For instance, if the center of the scene is lower than the camera, then the camera will look down, resulting in a bird's eye view. Similarly, if the center of the scene is higher than the camera, then the camera will look up, resulting in a frog's eye view.</span><br>
<i>up x/y/z:</i> This defines which axis is facing upwards. You can use values from 0 to 1 to -1. If you use a -1, the camera will be flipped and face up-side-down. In our case, we want to keep it simple and use for x,y,z 0,1,0 since y is pointing upwards.<br><br>

When you initialize your camera, you may want to start by using the default parameter values we provide in the code, which should give you a reasonable start for a good viewport. If you see nothing in your view after initializing the camera, you may be facing the wrong way with you camera, e.g. the scene may be behind you, which causes you to only see a white screen with nothing on it.<br><br>

<b>Zooming in/out</b><br>
Moving the camera position (eye position) closer or further away from the 3D visualization results in a zooming in/out effect (see images below).<br>
Implement a zooming effect so that when you press the key "E" you zoom in and if you press the key "Q" you zoom out (i.e. change the distance between camera position and the 3D coordinate origin).<br><br>

<img src="images/pset4/UI-3d-visualization-cameraViewControl-zoom-out.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-zoom-in.png" width="350px"> <br><br>

<b>Rotating in the X-Z Plane</b><br>
To move the camera in the x-z plane around the 3D visualization, you need to keep it at the same distance from the 3D visualization at all times (see images below).<br>
This is best accomplished by moving the camera on a circle around the scene.<br>
Update the camera position (eye position) to accomplish this (you have to do some math here, check out how radians work again to help you with this).<br>
If you press the key 'D' you should rotate right and if you press the key 'A' you should rotate left.<br>
Note that you only have to update the camera's position.<br>
Since you set the scene's center x/y/z to be the 3D visualization, the camera will automatically rotate while moving around the 3D visualization to always look at it.<br><br>

<img src="images/pset4/UI-3d-visualization-cameraViewControl-rotate-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-rotate-2.png" width="350px"> <br><br>


<b>Changing the Camera's Y-Axis Height</b><br>
To move the camera along the y-axis, you need to change the camera's eye position again, but this time only along the y-axis.<br>
Pressing 'W' on the keyboard should move the camera up and pressing 'S' on the keyboard should move the camera down along the y-axis as shown in the image below.<br><br>

<img src="images/pset4/UI-3d-visualization-cameraViewControl-view-height-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-cameraViewControl-view-height-2.png" width="350px"> <br><br>

Finally, if you are experiencing some lagging in your application, you can also set the frame rate for the camera for better performance using the line below:<br><br>

<pre>frameRate(30); </pre><br>


<h4>Draw the Touch Points and Touch Pressure into the 3D Visualization</h4> <br>

Next, you will draw the touch points and the touch pressure into the 3D visualization based on your interpolated PImage from pset3.<br><br>

<b>Drawing 3D Points in Processing</b><br>
To implement the touch points, you will create a set of 3D points, where the X,Z coordinates are the row & column number of the pixel on the PImage, and the Y-coordinate is the interpolated value of the pixel. <br>
As mentioned previously, the brighter pixels are in an area the more pressure was applied to this area.<br>
As a result, the brighter a pixel, the higher the touch bar will be at this point.<br><br>

<b>Mapping the Y-Height to a Color Gradient</b><br>
You also need to color the touch bars with a color gradient that represents the "height" (i.e. y-axis coordinate) information.<br>
We chose a color gradient of red when only touched with light pressure and yellow when touched with high pressure.<br>
You can choose your own color gradient, just make sure that the colors are very different so you can actually see a difference.<br>
To implement the y-height to color mapping, you may find it helpful to use the <a href="https://processing.org/reference/map_.html">map() function</a> again that you already used in pset3.<br>
The resulting 3D visualization should look like something in the following.<br><br>

One finger touching: <br>
<img src="images/pset4/UI-3d-visualization-1-finger-1.png" width="350px">
<img src="images/pset4/UI-3d-visualization-1-finger-2.png" width="350px"> <br><br>

Two fingers touching:<br>
<img src="images/pset4/UI-3d-visualization-2-finger-2.png" width="350px">
<img src="images/pset4/UI-3d-visualization-2-finger-4.png" width="350px"> <br><br>

Note that all this noise is because we use the PImage that also contains brightness values outside the recognized 'finger blobs'. If you want a cleaner visualization, you can create a new PImage that has all pixels black except those pixels that belong to a blob. Note that this is not mandatory for this pset but if you'd like some clean output you can experiment with this.
<br><br>

<h3>Upload your Processing Code and Pictures of your Visualization</h3>

For grading, please upload the following to your google drive student folder:<br>

<ol>
	<li>Gesture Recognizer
		<ul>
			<li>a drawing on paper that shows your 3 custom finger gestures so we know which gestures you implemented, use the same notation as in the pset to indicate the start point for executing the gesture</li>
			<li>the .pde file of your Processing program</li>
			<li>3 photos showing your Processing UI successfully recognizing the 3 different finger gestures </li>
			<li>a short video showing your Processing UI successfully recognizing the 3 different finger gestures (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>
	</li>
	<li>3D Visualization
		<ul>
		<li>the .pde file of your Processing program </li>
		<li>3 photos showing your Processing UI when no finger, one finger, and two fingers touch the multi-touch pad with different pressure levels</li>
		<li>a short video showing your Processing UI when no finger, one finger, and two fingers touch the multi-touch pad with different pressure levels (take the video so that the multi-touch pad and Processing Window are seen at the same time) </li>
		</ul>
	</li>
</ol>

<h3>Grading</h3>

We will give 25 pts in total:
<ul>
	<li>5pts: you implemented 3 different gestures using the learn(), bind(), and track() functions from the $1 Unistroke Recognizer</li>
	<li>5pts: you submitted a drawing showing all three gestures you implemented and show in the video that all three gestures are correctly recognized</li>
	<li>5pts: the x,y,z axis are correctly drawn for the 3D visualization and the touch / pressure mapping is correctly rendered, which you show by having at least 2 fingers touch simultaneously</li>
	<li>5pts: the camera motion for zooming in/out, rotating the camera in the x/z plane and moving the camera up/down are correctly implemented</li>
	<li>5pts: the remaining 5pts will be given for creating presentation materials for your multi-touch pad, i.e. drawing a rotoscope (1.5pts), taking some high-quality photos (1.5pts) and making a short 2-3 sequences video (2pts). This will be part of the weekly Friday labs and you will work on this over the next weeks in small steps. We will provide more information on this shortly.</li>
</ul>



        <br />
        <br />
      </section>

      <aside class="col-md-4 pull-left">
         <br /> <br /> <br /> <br />
<!-- 				 <h4>Pset Steps</h4><br>
				 <ul>
		 			<li><a href="#pset1">pset1 (due Sept. 21, 11.59pm): laser cut and bend the acrylic base</a><br /></li>
		 			<li><a href="#pset2">pset2 (due Oct. 5, 1pm): insert LEDs, add USB connecting and solder everything</a><br /></li>
		 			<li><a href="#pset3">pset3 (due Oct. 19, 1pm): write touch recognition so that you can determine (x,y) location of each finger</a><br /></li>
		 			<li><a href="#pset4">pset4 (due Oct. 26, 1pm): add an application of your choice</a><br /></li>
				</ul>
				<br /> <br /> <br /> <br />
        <img src="../2018-fall-6810/images/multi-touch-pad/iap1.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap2.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad//iap3.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap4.jpg" width="220px">

		<img src="../2018-fall-6810/images/multi-touch-pad/iap5.jpg" width="220px">
 -->



        <!-- Publication -->

        <br><br><br><br><br>

        <!-- Publication -->

<!-- <h4>Side Bar</h4><br>

    <ul>
      <li>Prof. Stefanie Mueller (Instructor)</li>
      <li>Lotta-Gili Blumberg (TA)</li>
      <li>Xin Wen (UTA)</li>
      <li>Loren Maggiore (LA)</li>
      <li>Mark Chounlakone (LA)</li>
    </ul>
 -->
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

      </aside>

    </div>
  </div>
  </div>
</section>

<div class="container">
	<div class="row">
		<div class="col-md-12 footer" style="text-align: center;">
			<span class="copyright">
			Since 2017 &copy; MIT CSAIL (HCI Engineering group) [redesign by
			<a href="http://punpongsanon.info/" target="_blank" style="text-decoration:none; border-bottom:0px">
			moji
			</a>].
			All Rights Reserved.

			<a href="http://mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/mit.svg" alt="MIT" class="footer-logo" />
			</a>
			<a href="http://csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/csail.svg" alt="CSAIL" class="footer-logo"/>
			</a>
			<a href="http://hci.csail.mit.edu/" target="_blank" style="text-decoration:none; border-bottom:0px">
			<img src="../../images/logo/hci.svg" alt="HCI" class="footer-logo"/>
			</a>
			</span>
		</div>
	</div>
</div>

<!-- Bootstrap -->
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>
<!-- header -->
<script type="text/javascript" src="../../js/headerstrap-for-subpage.js"></script>

</body>
</html>
